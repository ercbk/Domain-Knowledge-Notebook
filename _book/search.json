[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Domain Knowledge Notebook",
    "section": "",
    "text": "Preface\nThese notes were transferred from Evernote to Quarto, so some of notes may be difficult to read as the editing of the note format is still an ongoing process.\nNotes without numbers in sidebar should be a good indicator of a note that I’ve finished formatting.\nIf you see any mistakes or have any questions, please open an issue at the github repository."
  },
  {
    "objectID": "qmd/agriculture.html#sec-ag-misc",
    "href": "qmd/agriculture.html#sec-ag-misc",
    "title": "Agriculture",
    "section": "Misc",
    "text": "Misc\n\nCommodity prices normally forecast using a cobweb model which leads to price risk\nVegetable price series are much more volatile than other commodities\n\nSeasonality regulates the supply and demand\nPerishable nature of the produce adds complications in stabalizing the price"
  },
  {
    "objectID": "qmd/banking-credit.html#sec-bank-cred-misc",
    "href": "qmd/banking-credit.html#sec-bank-cred-misc",
    "title": "Banking/Credit",
    "section": "Misc",
    "text": "Misc\n\nLogistic Regression models typically with 8 to 10 predictors are common\n\n“To adopt a new algorithm, it not only had to outperform regression. The improvement also had to justify the effort of explaining the algorithm.”\n\nUsing SHAP, PDPs, etc. just adds more complexity because then you would have also needed to explain the method used to explain the model\n\n\nDS Use Cases\n\nCredit risk — predict default due to financial distress\nFraud — predict if customers do not intend to repay a loan\nPre-areas — identify customers in financial distress\nChurn — identify customers who intend to leave the bank\nMarketing — identify the best customers to promote a product to"
  },
  {
    "objectID": "qmd/banking-credit.html#sec-bank-cred-fraud",
    "href": "qmd/banking-credit.html#sec-bank-cred-fraud",
    "title": "Banking/Credit",
    "section": "Fraud",
    "text": "Fraud\n\nMisc\n\nFraud Score - Values that indicate how risky a user action is. Scoring determined by fraud rules.\nComputing ROI for a fraud model\n\nAssume the cost of fraud is the cost of the transaction\nCalculate the total cost of all the fraudulent transactions in the test dataset.\n\nCalculate the cost based on the model predictions.\n\nFalse Negatives: Observed frauds that weren’t predicted are assigned a cost equal to the value of the transaction.\nFalse Positives: Legitimate transactions that were marked as fraud are assigned $0 cost.\n\nThis likely isn’t true. There is the cost of having to deal with customers calling because the transaction was declined or the cost sending out texts for suspicious transactions, but this cost is very small relative to the cost of a fraudulent transaction.\nZhang, D. , Bhandari, B. and Black, D. (2020) Credit Card Fraud Detection Using Weighted Support Vector Machine. Applied Mathematics, 11, 1275-1291. doi: 10.4236/am.2020.1112087.\n\nOther costs can include deployment (e.g. DL model vs logistic regression)\nROI of the new model = costs of old model - cost of new model\nExample: article\n\n\nMetrics\nFN: predicting “not fraud” for a transaction that is indeed fraud\n\nA false negative more costly than false positive\n\nRecall (aka sensitivity): Ratio of correct fraud predictions (TP) out of all fraud events (TP + FN)\nFN Rate: Ratio of fraud events  the model failed to predict out of all fraud events\n\ncomplement of Recall, (FN/(TP+FN))\nmost expensive to organizations in terms of direct financial losses, resulting in chargebacks and other stolen funds\n\nFP Rate: rate at which a model predicts fraud for a transaction that is not actually fraudulent\n\nFP / (FP + TN)\nMeasures the incovenience for the customer that the model inflicts\nSeems to be a metric to monitor by group to see if the model is ethically biased\n\nModel Monitoring\n\nFar more false positives in production than the validation baseline\n\nResults in legitimate purchases getting denied at the point of sale and annoyed customers.\n\nA dip in aggregate accuracy\n\nInvestigate prediction accuracy by group\nExample: The fraud model isn’t as good at predicting smaller transactions relative to the big-ticket purchases that predominated in the training data\n\nFeature performance heatmaps can be the difference between patching costly model exploits in hours versus several days\nScenario Examples\n\nPrediction Drift\n\nPossible Drift Correlation: An influx and surge of fraud predictions could mean that your model is under attack! You are classifying a lot more fraud than what you expect to see in production, but (so far) your model is doing a good job of catching this. Let’s hope it stays that way.\nReal-World Scenario: A hack of a health provider leads to a surge of identity theft and credit card numbers sold on the dark web. Luckily, the criminals aren’t novel enough in their exploits to avoid getting caught by existing fraud models.\n\nActuals Drift (No Prediction Drift)\n\nPossible Drift Correlation: An influx of fraud actuals without changes to the distribution of your predictions means that fraudsters found an exploit in your model and that they’re getting away with it. Troubleshoot and fix your model ASAP to avoid any more costly chargebacks.\nReal-World Scenario: A global crime ring sends unemployment fraud to all-time highs using new tactics with prepaid debit cards, causing a dip in performance for fraud models trained on pre-COVID or more conventional credit card data.\n\nFeature Drift\n\nPossible Drift Correlation: An influx of new and/or existing feature values could be an indicator of seasonal changes (tax or holiday season) or in the worst case be correlated with a fraud exploitation; use drift over time stacked on top of your performance metric over time graph to validate whether any correlation exists.\nReal-World Scenario: An earlier holiday shopping season than normal takes hold, with bigger ticket purchases than prior years. It might be a sign of record retail demand and changing consumer behavior or a novel fraud exploitation (or both)."
  },
  {
    "objectID": "qmd/budget-allocation.html#sec-budallo-misc",
    "href": "qmd/budget-allocation.html#sec-budallo-misc",
    "title": "Budget Allocation",
    "section": "Misc",
    "text": "Misc"
  },
  {
    "objectID": "qmd/budget-allocation.html#sec-budallo-perform-cuts",
    "href": "qmd/budget-allocation.html#sec-budallo-perform-cuts",
    "title": "Budget Allocation",
    "section": "Performance-based Budget Cuts",
    "text": "Performance-based Budget Cuts\n\nNotes from: The Politics of Performance Measurement - Scenario: “Criminal Justice Division (CJD) of the Texas Governor’s Office received news all government agencies dread: budgets were to be cut. CJD oversaw a grant program that funded specialty courts throughout the state, however it was now being told that the program’s budget of $10.6m would be reduced 20% to $8.5m by 2018.”\n\nHow should these cuts be distributed among grant holders?\nGoal: Develop a data collection and performance assessment process to allocate budget cuts in a manner widely accepted\n\nOptions\n\nCut across the board. The Advisory Council would employ the same scoring method as the previous year but reduce each grant amount by 20%.\n\nThis option would leave long-running grantees scrambling to make up for this shortfall by reducing services, laying off staff, or spending more of their limited local funds. Worse, it would punish all grantees equally — our most successful programs would be arbitrarily defunded.\n\nFewer grants. Grants were scored based on the quality of their application and all grants that passed a certain threshold got funded. The Advisory Council would employ the same scoring method as the previous year but instead of funding the top $10.6m worth of grants, they would fund the top $8.5m worth.\n\nThis seemed a less bad option than cutting across the board, but we would still run into the problem of arbitrarily defunding successful programs. Grants near the bottom of the Advisory Council’s cutoff that got funded the previous year would be denied renewal only because the goalposts had moved.\n\n\n\n\nTargeted funding. The Advisory Council would incorporate performance data and statewide strategic plan alignment into their scoring method and make cuts accordingly.\n\nEngage stakeholders and define performance\n\nConvene a strategy session with the stakeholders to discuss how to proceed as part of a broader strategic plan\n\nAchieve consensus on high-level goals (e.g. fund strategically, focus on success, build capacity)\nLarger plan agreed upon that would also include: capacity building, training and technical assistance, helping courts obtain non-CJD sources of funding, and steering grantees toward established best practice."
  },
  {
    "objectID": "qmd/economics.html#sec-econ-misc",
    "href": "qmd/economics.html#sec-econ-misc",
    "title": "Economics",
    "section": "Misc",
    "text": "Misc\n\nDynamic stochastic general equilibrium (DSGE) models, which are popular in macroeconomic modeling, are garbage (article)\n\nEven under extremely ideal conditions they don’t retrieve the parameters and using them to forecast is no better than chance."
  },
  {
    "objectID": "qmd/economics.html#sec-econ-terms",
    "href": "qmd/economics.html#sec-econ-terms",
    "title": "Economics",
    "section": "Terms",
    "text": "Terms\n\nAdverse Selection - a market situation where buyers and sellers have different information. The result is that participants with key information might participate selectively in trades at the expense of other parties who do not have the same information\n\ne.g. A person waits until he knows he is sick and in need of health care before applying for a health insurance policy. The buyer has more knowledge (i.e., about their health). To fight adverse selection, insurance companies reduce exposure to large claims by limiting coverage or raising premiums\n\nIntertemporal Price Discrimination - charging a high price initially, then lowering price after time passes.\n\nA method for firms to separate consumer groups based on willingness to pay\ne.g. last minute travel bookings (opposite direction since last minute bookings usually cost more)\n\nPrice Elasticity of Demand (PED)- the percent change in demand given the percent change in price assuming that everything else doesn’t change\n\nHow sensitive the quantity demanded is to its price. When the price rises, quantity demanded falls for almost any good, but it falls more for some than for others.\nThe elasticity of a good or service can vary according to the number of close substitutes available, its relative cost, and the amount of time that has elapsed since the price change occurred.\nWhen the price of a good or service has reached the point of elasticity, sellers and buyers quickly adjust their demand for that good or service.\nAn inelastic product is one that consumers continue to purchase even after a change in price\nProducts or services that are elastic are either unnecessary or can be easily replaced with a substitute.\n\nSecond Degree Price Discrimination - charging a different price for different quantities at the same time"
  },
  {
    "objectID": "qmd/economics.html#sec-econ-pricelas",
    "href": "qmd/economics.html#sec-econ-pricelas",
    "title": "Economics",
    "section": "Price Elasticity",
    "text": "Price Elasticity\n\nBy identifying the price elasticity of demand, you can try to determine the amount of price you can increase without hurting the demand, as well as check at what point an increase in price starts to affect the market.\nPrice is NOT the only variable that influences whether you purchase a product or service. Therefore, looking at quantity purchased at each price to determine price elasticity is not enough.\nGuidelines\n\nIf the PED is greater than one (PED &gt; 1), it is known as “elastic”, meaning changes in price causes a significant change in demand.\nIf the PED is equal to 1 (PED = 1), then this means any change in price causes equivalent changes in demand.\nIf the PED is less than one (PED &lt; 1), it is known as “inelastic”. This means changes in price don’t affect the demand that much.\nIf the PED is equal to 0 (PED = 0), known as “perfectly inelastic”, meaning any change in price doesn’t cause a change in demand.\n\nProcess\n\nFilter on the specific subset of sales data relevant to the dimension which you are estimating elasticity (e.g. if estimating the price elasticity for red wine, filter on only red wine sales)\nPerform a log transformation on the future sales target variable and on the current price feature\nTrain a multivariable linear regression model to accurately predict future sales\nThe price elasticity estimate will be the coefficient of the log transformed, price feature\nRepeat steps 1–4 for each elasticity estimate"
  },
  {
    "objectID": "qmd/environment.html",
    "href": "qmd/environment.html",
    "title": "1  Environment",
    "section": "",
    "text": "TOC\nMisc\nMetrics\n\nExisting Tree Canopy: The amount of urban tree canopy present when viewed from above using aerial or satellite imagery. - ETC % = tree canopy / land area\nPossible Tree Canopy - Vegetated: Grass or shrub area that is theoretically available for the establishment of tree canopy. - e.g. residential areas\nPossible Tree Canopy - Impervious: Asphalt, concrete or bare soil surfaces, excluding roads and buildings, that are theoretically available for the establishment of tree canopy without having to remove paved surfaces - e.g. any areas with no trees, buildings, roads, or bodies of water - Possible-Vegetation category should serve as a guide for further analysis, not a prescription of where to plant trees since other factors, such as land use, social, and financial (e.g. golf courses, agricultural and recreational fields), are involved.\nNot Suitable: Areas where it is highly unlikely that new tree canopy could be established (primarily buildings and roads).\nrelative tree canopy change - change of tree canopy over a period of time - e.g (for 1 hexagon) relative tree canopy change % = (tree_canopy_area_2019 - tree_canopy_area_2012) / tree_canopy_area_2012 - Acre gain per \nCanopy height - proxy for tree age - Steps - Segment tree canopy into polygons approximating individual trees - Attribute each polygon with a height from both the starting date to end date (e.g. 2012 and 2019 ) LiDAR data - Interpretation example - trees in the 0-60 foot height class experienced gain, while there was minimal gain in the other taller height classes. - Therefore, many new trees planted and canopy expanding on existing trees. - Diverse height structure corresponds to a healthy and diverse tree age distribution - Very mature trees in the 130 height class points to the height potential for certain tree species provided the right conditions\n\nNotes from Lousiville Tree Canopy Assessment 2012-2019\n\nTree benefit: reducing stormwater runoff near streets and decreasing the urban heat island effect\nAbove surface factors such as sidewalks to utilities can affect the suitability of a site for tree planting.\nImportant to preserve trees in the 10-50 foot height range, so they can grow into the 60+ foot range while planting a variety of new trees to continue the lifecycle\nLosses are generally easier to detect than gains as losses tend to be due to a large event, such as tree removal, whereas gains are incremental growth or new tree plantings, both of which are smallerin size\nFactors that can affect change in tree canopy - Natural - Invasive species - Natural disasters such as storms - Climate change may cause trees to grow more quickly but could also result in inhospitable conditions for native species - Anthropogenic - Preservation and conservation efforts, the strength of tree ordinances, and the impacts of new development - Tree removal due to homeowner preferences and not being replaced by new trees - Proximity to roads: Regular salting, compaction, limited space, clearance pruning, and plow collisions\nData sources - LiDAR - features distinguished by their spectral (color) properties - trees and shrubs can appear spectrally similar or obscured by shadow, LiDAR, which consists of 3D height information enhances the accuracy of the mapping - resolution of 30-meters - “LiDAR datasets were acquired under leaf-off conditions and thus tend to underestimate tree canopy slightly” (i.e. Fall or Winter?) - LiDAR and imagery datasets are not directly comparable due to differences in the sensor, time of acquisition, and processing techniques employed. - Resources: - Paper summarizing their tree canapy mapping approach - Getting to Land Use/Land Cover - Threshold Classification in eCognition - Object-based approach to LiDAR\n%  using 500-acre hexagons\n\nUse LiDAR Hill shade map with % canopy change to highlight local areas\n\nLand Use Categories - Overall: residential, commercial, and recreational - Metric change by category | acres lost (orange)/gained (green) by category\n\n - Change per Council District (by metric)"
  },
  {
    "objectID": "qmd/epidemiology.html",
    "href": "qmd/epidemiology.html",
    "title": "2  Epidemiology",
    "section": "",
    "text": "TOC\nMisc\nDisease Mapping\n\nGoals - Provide accurate estimates of mortality/incidence risks or rates in space and time - Unveil underlying spatial and spatio-temporal patterns - Detect high-risk areas or hotspots\nRisk estimation using metrics such as Standardized Mortality Ratio (SMR) when analyzing rare diseases or low-populated areas are highly variable over time, so it’s diffficult to spot patterns and form hypotheses - SMR = Observed number of cases / Expected number of cases - SMR &gt; 1: risk is greater than the whole region under study - Guessing “Expected number of cases” is the average number of cases for the whole study region\nStatistical models smooth risks by borrowing information from spatio-temporal neighbors - The smoothed gradient over the entire study region makes it easier to detect patterns and form hypotheses than highly variable, local area metric estimates (e.g. SMR in a low populated county)\nTraditional Models - Types - Mixed Poisson with conditional autoregressive (CAR) priors for “space” and random walk priors for “time” that include space ⨯ time interactions (Knorr-Held, 2000, Bayesian modeling of inseperable space-time variation in disease risk) - Reduced rank multidimensional P-splines (Ugarte et al, 2017, One-dimensional, two-dimensional, and three dimensional B-splines to specify space-time interactions in Bayesian disease mapping) - Issues - Estimating the cov-var matrix becomes intractable with big data and many areas since the covariance must be estimated between each pair of areas - CAR models assume the same level of spatial dependence between all areas which isn’t likely.\n{bigDM} - Scalable non-stationary Bayesian models for high-dim, count data - Dependencies - Uses {future} for distributed computing - Integrated, nested laplace approximation (INLA) method through {R-INLA} - k-order neighborhood model - Breaks up local spatial or spatio-temporal domains so that estimations can distributed and local area dependencies (neighborhoods) can be accounted for. - “areas” are usually districts, counties, provinces, etc. - package does provide a method to create a “random” area grid - Might be useful to compare a random grid model with the e.g. county model to see how much county boundaries influence the estimates - Each local area model includes k adjacent areas which creates a partition - The local area estimate is smoothed by taking information from the adjacent areas - Adjacent areas also have estimate posteriors computed - Each area will have multiple posterior estimates from local area models where the area is the local area or where it is the adjacent area - Merge or don’t merge estimate posteriors for each area - Merge: use weights proportional to the conditional predictive ordinates (CPO) ??? - Don’t Merge: Use the posterior marginal risk estimates of an area corresponding to the original submodel. - i.e. use the posterior where the area is the “local area” in that local area model and not an adjacent area. - Primary functions - CAR.INLA() fits several spatial CAR models for high dim count data - STCAR.INLA() fits several spatio-temporal CAR models for high dim count data"
  },
  {
    "objectID": "qmd/finance.html#misc",
    "href": "qmd/finance.html#misc",
    "title": "3  Finance",
    "section": "3.1 Misc",
    "text": "3.1 Misc\n\n435 choices for start and end dates of each monthly investment cycle - i.e. easy to have a selective endpoints fallacy for an investment strategy.\nStrategies - Hedgefund - factor rotations (longer term) - something about futures - factor tilts (shorter term)\nResources - Advances in Financial Machine Learning - de Prado (R &gt;&gt; Documents &gt;&gt; Financial)\nMisleading chart (Thread)\n\n- No transaction costs - No fees - No taxes - US “exceptionalism” - Overlapping time period analysis - Example: if I look at 1920-1950 and then 1921-1951, twenty eight of the thirty data points are overlapping. So there’s not a lot of “unique” data in this set. - Nominal vs real returns - Time weighted vs dollar weighted returns\n\nTerms\n\nBasis Point - 1/100th of a percentage point\nBasis Risk - the risk that an asset and a hedge will not move in opposite directions as expected; “basis” refers to the discrepancy.\nCapital expenditure (CapEx) - money that is spent to acquire, repair, update, or improve a fixed company asset, such as a building, business, or equipment. For assets to fall under the CapEx destination, the investments must have a useful life of one year or more. A CapEx is amortized, or its value is deducted a little each year based on the total cost and its expected useful life. - Useful life refers to the estimated and generally agreed upon shelf life of a specific business asset. - According the IRS, Car’s useful life is 5yrs and new building’s is 39yrs - Also see What Is a Capital Expenditure (CapEx)? Definition and Guide (example, calculations, relation to operating expenditure (OpEx)\nCost of Carry or Carrying Charge - the cost of holding a security or a physical commodity over a period of time. The carrying charge includes insurance, storage and interest on the invested funds as well as other incidental costs - For a stock, it’s is the opportunity cost of the capital that goes into it plus the risk you take on for holding it (this is what the idea of risk neutral valuation is based on).\nDerivatives are securities that move in correspondence to one or more underlying assets. They include options, swaps, futures and forward contracts. The underlying assets can be stocks, bonds, commodities, currencies, indices or interest rates. Derivatives can be effective hedges against their underlying assets, since the relationship between the two is more or less clearly defined (if they’re negatively correlated? Or maybe if the underlying asset goes down, there’s a lag between the asset going down and the derivative going down. Therefore, you can sell the derivative before it goes down. Thus, hedging your risk). Knowing the value of an underlying asset helps traders determine the appropriate action (buy, sell, or hold) with their derivative.\nExchange Traded Fund (ETF) - A mutual fund that may be traded daily like a stock or bond.\nFalse Strategy Theorem - Gives the threshold for which a Sharpe Ratio would be significant. - Given a sample of estimated performance statistics (e.g. sharpe ratios), {Sk} for k = 1, …, K, where each S ∈ N(0, 1) \\[\n        \\mathbb{E}[\\max_{k} {S_k}] \\approx (1-\\gamma) Z^{-1} [1-\\frac{1}{K}] + \\gamma Z^{-1} {1 - \\frac{1}{Ke}}\n        \\] - Z-1 is the inverse of the standard Gaussian cdf - e is the exponential constant (i.e. 2.71…) - γ is the Euler-Mascheroni constant (approx. 0.5772156649…) - Useful for backtesting multiple strategies and deciding whether the strategy with the maximum sharpe ratio is significant (mitigates multiple testing bias)\nFutures are an obligation to the buyer and a seller. The seller of the future agrees to provide the underlying asset at expiry, and the buyer of the contract agrees to buy the underlying at expiry. The price they receive and pay, respectively, is the price they entered the futures contract at. Most futures traders close out their positions prior to expiration since retail traders and hedge funds have little need to take physical possession of barrels of oil, for example. But, they can buy or sell the contract at one price, and if it moves favorably they can exit the trade and make a profit that way. Futures are a derivative because the price of an oil futures contract is based on the price movement of oil, for example.\nHedge - an investment that is made with the intention of reducing the risk of adverse price movements in an asset. Normally, a hedge consists of taking an offsetting or opposite position in a related security. An example could be investing in both cyclical and counter-cyclical stocks.\nHedge Ratio (delta) - The effectiveness of a derivative hedge, delta, is the amount the price of a derivative moves per $1 movement in the price of the underlying asset.\nLeg - one part or one side of a multistep trade.  Legs should be exercised at the same time in order to avoid any risks associated with fluctuations in the price of the related security. So a purchase and sale should be made around the same time to avoid any price risk. Strategy often associated with derivatives trading.\nLimited Liability Corporation (LLC) - A corporation is a business organization that issues stock to its shareholders. A limited liability company is a business organization composed of members with membership interests. a type of legal entity that can be used when forming a business that offers protection to the owner(s) from personal liability for debts and other obligations that a business might incur. In other words, the personal assets of the owner cannot be used for legal claims against the business.The differences don’t really matter much at the taxation or day-to-day corporate level except in scale: LLCs tend to be smaller than corporations (more or less; a lot of people form small business corporations for good reasons and ignorant ones). (See Differences between a LLC and S Corp)\nMarket Capitalization - the total dollar market value of a company’s outstanding shares. Calculated by multiplying the total number of a company’s shares by the current market price of one share.\nMarket Momentum - measure of overall market sentiment that can support buying and selling with and against market trends. In general, Momentum = Today’s price - Price from X days ago. Positive: bullish, Negative: bearish. More sophisticated indicators can be calculated, see https://www.investopedia.com/terms/m/marketmomentum.asp#:~\nOptions - an option on stock XYZ gives the holder the right to buy or sell XYZ at the strike price up until expiration. The underlying asset for the option is the stock of XYZ. The writer must either buy or sell the underlying asset to the buyer on the specified date at the agreed-upon price. The buyer is not obligated to purchase the underlying asset, but they can exercise their right if they choose to do so. If the option is about to expire, and the underlying asset has not moved favorably enough to make exercising the option worthwhile, the buyer can let the expire and they will lose the amount they paid for the option. - Put option- if Morty buys 100 shares of Stock plc (STOCK) at $10 per share, he might hedge his investment by buying an American put option with a strike price of $8 expiring in one year. This option gives Morty the right to sell 100 shares of STOCK for $8 any time in the next year. Let’s assume he pays $1 for the option, or $100 in premium. If one year later STOCK is trading at $12, Morty will not exercise the option and will be out $100. He’s unlikely to fret, though, since his unrealized gain is $100 ($100 including the price of the put). If STOCK is trading at $0, on the other hand, Morty will exercise the option and sell his shares for $8, for a loss of $300 ($300 including the price of the put). Without the option, he stood to lose his entire investment. - Call options - contract giving the owner the right, but not the obligation, to buy a specified amount of an underlying security at a specified price within a specified time. The specified price is known as the strike price and the specified time during which a sale is made is its expiration (expiry) or time to maturity. As the price of the stock goes up, the value of the call option contract goes up. The contract can be sold at any time or you can purchase the stock at the guaranteed price on the expiration date. The price of the call option is called the premium. - if Apple is trading at $110 at expiry (aka expiration date), the strike price is $100, and the options cost the buyer $2, the profit is $110 - ($100 +$2) = $8. If the buyer bought one contract that equates to $800 ($8 x 100 shares), or $1,600 if they bought two contracts ($8 x 200). If at expiry Apple is below $100, then the option buyer loses $200 ($2 x 100 shares) for each contract they bought. - Suppose that Microsoft shares are trading at $108 per share. You own 100 shares of the stock and want to generate an income above and beyond the stock’s dividend. You also believe that shares are unlikely to rise above $115.00 per share over the next month. You take a look at the call options for the following month and see that there’s a 115.00 call trading at $0.37 per contract. So, you sell one call option and collect the $37 premium ($0.37 x 100 shares), representing a roughly four percent annualized income. If the stock rises above $115.00, the option buyer will exercise the option and you will have to deliver the 100 shares of stock at $115.00 per share. You still generated a profit of $7.00 per share, but you will have missed out on any upside above $115.00. If the stock doesn’t rise above $115.00, you keep the shares and the $37 in premium income.\nOutstanding Shares - the number of stocks that a company has issued. This number represents all the shares that can be bought and sold by the public, as well as all the restricted shares that require special permission before being transacted.\nRelative Performance - the price ratio of two stocks\nSharpe Ratio (annualized) - measures the performance of an asset relative to violitility (i.e. riskiness) - (expected excess returns relative to a risk free asset (e.g. treasury bond) / sd of those expected excess returns) * √number_of_observations_in_a_year - Sharpe ratios above 1.0 are generally considered “good,” as this would suggest that the portfolio is offering excess returns relative to its volatility - Even if your sharpe ratio is above 1 it may not be good if it is below the average sharpe ratio of peer group portfolios. - Multiplying by √number_of_observations_in_a_year makes “annualizes” the sharpe ratio and makes sharpe ratios comparable - ** Should NOT be thought of as t-stats for testing significance of the sample mean (i.e. p-values for estimates) since it doesn’t account for the number of observations ** - See Sharpe Ratio (deflated) - Investment professionals often use a rule of thumb of dividing the sharpe ratio by 2 when backtesting to avoid overfitting, there is no statistical basis for this.\nSharpe Ratio (probabilistic) - allows you test the significance of the Sharpe Ratio under assumptions of ergodicity and stationarity (Paper)\nSharpe Ratio (deflated) - the probability that an observed Sharpe Ratio was drawn from a distribution with positive mean after controlling for sample size (aka backtest length), skewness, kurtosis, and number of strategy variations explored. - Combines probabilistic sharpe ratio and false strategy theorem - Paper: The Deflated Sharpe Ratio: Correcting for selection bias, backtest overfitting and non-normality  - Shows that if a strategy has a maximum sharpe ratio of 1 but had 3 variations backtested, it’s deflated sharpe ratio drops below the 95% CI for a sharpe ratio = 1. - Returns from investment strategies often exhibit autocorrelation, fat tails, and negative skewness which further “deflates” the deflated sharpe ratio\nSpread (Bid-Ask): the difference between two prices, rates or yields. - the gap between the bid and the ask prices of a security or asset, like a stock, bond or commodity. - the gap between a short position (that is, selling) in one futures contract or currency and a long position (that is, buying) in another. This is officially known as a spread trade\nVolatility: - Parkinson Range (PR) = ln(closing_price) - ln(opening_price)\nYield a return measure for an investment over a set period of time, expressed as a percentage. - Includes price increases as well as any dividends paid, calculated as the net realized return divided by the principal amount (i.e. amount invested). - Higher yields are perceived to be an indicator of lower risk and higher income, but a high yield may not always be a positive, such as the case of a rising dividend yield due to a falling stock price.\n\n60/40 Portfolio Strategy\n\nMisc - {{QSTrader}} - Notes from - The 60/40 Benchmark Portfolio\nDescription - 60/40 US Equities/Bonds strategy is a simple long-term investment approach that is widely utilised in the investment industry. It seeks to ensure that at any point during the lifetime of the investment that 60% of account equity is invested in one or more assets representing a broad selection of US equities (such as an S&P500 ETF), while 40% of account equity is invested in one or more assets representing a broad selection of US treasury bonds (such as a treasury bond ETF). - Since the actual percentage allocations of each asset class can deviate over time due to relative growth of the respective assets a ‘rebalance’ approach is often carried out. This means that trades are issued on a relatively infrequent basis to buy/sell amounts of each asset class to periodically bring the account equity allocations back into the 60/40 split. For the particular strategy implemented here we are using an end of month rebalance frequency.\n\nMomentum Tactical Asset Allocation Strategy\n\nMisc - {{QSTrader}} - Notes from - Systematic Tactical Asset Allocation: An Introduction - Tutorial\nDescription - The US sector momentum strategy is a long-only dynamic tactical asset allocation strategy that attempts to exceed the performance of simply going long the S&P500. - At the end of every month the strategy calculates the holding period return (HPR) based momentum of all of the SPDR sector ETFs (the ticker symbols of which begin with the prefix XL) and selects the top N to invest in for the forthcoming month, where N is usually between 3 and 6. - In the implementation given here the HPR momentum is calculated over the previous 126 days (approximately six months of business days) and the top three sector ETFs are chosen for the portfolio. The portfolio allocation is equally weighted between each of these three sector ETFs. Irrespective of changes in signal the portfolio is rebalanced once per month to weight the assets equally.\n\nMean Reversion Strategy or Pairs Trading\n\nMatch two trading vehicles that are highly correlated, trading one long and the other short when the pair’s price ratio diverges “x” number of standard deviations - “x” is optimized using historical data. If the pair reverts to its mean trend, a profit is made on one or both of the positions.\nChart relative performance (price ratio) - The center white line represents the mean price ratio over the past two years. The yellow and red lines represent one and two standard deviations from the mean ratio, respectively. - The potential for profit can be identified when the price ratio hits its first or second deviation. When these profitable divergences occur it is time to take a long position in the underperformer and a short position in the overachiever. (i.e. shorting the stock that’s rising in relation to the other, and buying the other stock that hasn’t risen yet) - The revenue from the short sale can help cover the cost of the long position, making the pairs trade inexpensive to put on. - Position size of the pair should be matched by dollar value rather than number of shares; this way a 5% move in one equals a 5% move in the other. - As with all investments, there is a risk that the trades could move into the red, so it is important to determine optimized stop-loss points before implementing the pairs trade.\nChart spreads between returns\nLook for stocks that move closely together and their: - Spreads zig-zag around zero - Prices separately (marginally) are normally distributed and joint-normally distributed (assumes linear correlation) - If both, then they move up AND down together (symmetric) - If only marginally, then they probably only move in one direction together (asymmetric)\nIf your two assets returns (differenced prices are stationary) do not correlate within a [0.7..0.8] correlation coefficient range at least 70% of the times (70% of all observations) then you’re probably dealing with a very bad hedging instrument - Look to cover the remaining data points and research why correlations broke down during those periods. If you can derive a mathematical relationship you could possibly formulate an approach in which you make adjustments to the hedge ratio through an adjustment in your beta/correlation coefficient. - Figure out the optimal time interval to retrain model due to data drift - Rolling 20-day correlation is a common way to monitor stock correlation\ncopulas (see correlation note) - Measure non-linear association\n\n  For Value-at-Risk (VAR) calculations, Gaussian copula is overly optimistic and Gumbel is too pessimistic\n\nCopulas with upper tail dependence: Gumbel, Joe, N13, N14, Student-t.\nCopulas with lower tail dependence: Clayton, N14 (weaker than upper tail), Student-t.\nCopulas with no tail dependence: Gaussian, Frank.\n\n\nCointegration allows us to construct a stationary time series from two asset price series, if only we can find the magic weight, or more formally, the cointegration coefficient β. Then we can apply a mean-reversion strategy to trade both assets at the same time weighted by β. There is no guarantee that such β always exists, and you should look for other asset pairs if no such β can be found. - cointegrated assets share common nonstationary components, which may include trend, seasonal, and stochastic parts - might have low correlation, and highly correlated series might not be cointegrated at all. - describes a long-term relationship between the prices (correlation describes a short-term relationship between the returns). The resulting stationary series is the spread between the prices of both assets - should have similar risk exposure so that their prices move together - good candidates for cointegrated pairs could be: - Stocks that belong to the same sector. - WTI crude oil and Brent crude oil. - AUD/USD and NZD/USD. - Yield curves and futures calendar spreads"
  },
  {
    "objectID": "qmd/healthcare.html",
    "href": "qmd/healthcare.html",
    "title": "4  Healthcare",
    "section": "",
    "text": "TOC\n\nMisc\nTerms\nProcessing\nFairness and Privacy\n\nMisc\n\nNotes from - Paper: Scalable and accurate deep learning with electronic health records by Google AI’s team - How to Encode Medical Records for Deep Learning - Makes data suitable for a RNN (see Processing for details)\nAlso see - Geospatial, Analysis &gt;&gt; Disease Mapping\nBias in Healthcare Claims Data (article, (mini) paper) - Imbalance of patients or members represented in large healthcare datasets can make your results non-generalizable or (in some cases) flat out invalid.\n- Goals should be: investigate the biases, mitigate as well as possible, and decide which insights are still valuable or meaningful despite these nuances\n- Characteristics of data that lead to biases\n- Only includes any members/patients who actually had an incident/event for which the insurer processed a subsequent claim\n- Tends to over/underrepresent certain groups who are more likely to have chronic health problems, adverse social determinants of health, seek care,\n- Groups are reflective of the type of demographics your organization tends to serve or that you have a larger book of business in\n- Can cause over-and-under-representation of certain diseases - Even companies within the same insurance sector may not have similar populations. Company business practices affect the types of populations within their data.\n- Our [patients / members / employees / residents / etc.] may not act similarly or even represent [all patients / other groups / other types of employer s / other regions / etc.]\n- Data is years old. Effects sizes in data that’s 5 or 6yrs old may not be valid now.\n- Types\n- Undercoverage bias - occurs when a part of the population is excluded from your sample.\n- Historical bias - occurs when socio-cultural prejudices and beliefs are mirrored into systematic processes [that are then reflected in the data].\n- Systemic biases - result from institutions operating in ways that disadvantage certain groups.\n- Solutions\n- Augment or compare with outside data:  data sharing/collaboration or tapping into additional feeds such as health information exchange (HIE)\n- Social Determinants of Health (SDOH) - (see definition below) compare socio-economics and demographics of your dataset with  Census estimates and the SDOH variability seen for said zip codes.\n- Will determine the amount of reweighting/normalizing the data requires\nPopulations within Healthcare Claims Data\n- Lines of business (LOB) such as patients with coverage from a government payer (Medicaid, Medicare); commercial lines (employer groups, retail or purchased via the exchange); self-insured (self-funded groups, usually large employers paying for their own healthcare claims of employees), etc.\n- Sub-lines of business, or groups. For instance, Medicaid may consist of multiple sub-groups representing different levels of eligibility, coverage, benefits, or types of people/why they qualified for the program (Temporary Assistance for Needy Families (TANF) vs. Medicaid expansion for adults)\n- Demographics (certain groups, males or females, certain regions, etc.)\n- Conditions (examining certain chronic conditions, top disease states of interest or those driving the highest costs to the system, those that Medicare focuses on, etc.)\n- Sub-groups of some combination of the above, for instance: Medicaid, TANF, looking at mothers vs. newborns\n- Cross-group comparison of some combination of the above, for instance: Medicaid, TANF, new mothers and their cost/utilization/outcomes trends vs. Commercial, self-insured, new mothers and their cost/utilization/outcomes\n\nICU’s data is usually the most complete and available for research compared to other healthcare data\nRecognized cutpoints - 25 kg/m2 to define “overweight” based on body mass index.\nDS Use Cases: - Clinical Outcomes - mortality (death) events - Early warning score (EWS) - likelihood of death - Features: respiratory rate, oxygen saturation, temperature, blood pressure, heart rate, and consciousness rating - Each variable has a normal range as established by common medical knowledge. A score is computed based on a lookup table to characterize how far away the variable is from its normal range. If the sum of all scores surpasses a threshold, it means a high likelihood of death - Early warning system for patient deterioration (CHARTwatch at Toronto hospital system, video) - Deployed to General Internal Medicine ward (GIM) - Slides with links to papers that discuss the model on the youtube website - Predicts risk score of a patient “at risk” - “at risk” - transfer to ICU or transfer to Palliative Care unit or Death (composite endpoint, see Terms) - Probability score then thresholded into “high,” “medium,” and “low” risk (color coded) labels - Features: laboratory values, vital measurements, and demographichs - Prediction delivery - Email to nurses, palliative care unit, etc. - Table with name, bed #,… , Status (aka prediction) for each patient - “front-end tool” - software for people that sign out patients, has general patient information, includes column for Status - Phone alerts for patients with High risk labels get sent to clinicians - Resource Utilization - Planning for hospital bed capacity - Score for long length of stay (i.e. larger than 7 days) - commonly computed 24 hours after admission - logistic regression model (with proper regularization techniques) - Features: age, gender, condition category, diagnosis code, hospital service received, and lab tests of vital signs to produce a probability number for long length of stay. - Assignment/Scheduling - Forecast daily Emergency Department arrivals - Quality of Care - 30 days readmission after discharge - Hospital score for readmission - typically calculated at discharge time - Features: hemoglobin level, sodium level, type of the admission, number of previous admissions, length of stay, whether the hospital stay is cancer related, and whether medical procedures were performed during the stay - Based on established medical knowledge, the values of each factor is translated to a risk score, and the sum of which depicts the overall risk of readmission - Medical Imaging AI tools - Dashboard to compute dosage (e.g. blood thinner) based on patient health factors\n\nTerms\n\nBiomedical Informatics - involves carrying out analysis on large-scale biological datasets in order to understand and profer solutions to health-related problems. Focuses on the optimal use of biomedical information, data, and knowledge for problem-solving and decision-making by employing computational and traditional approaches\nCalibration - the agreement between the estimated and the “true” risk of an outcome. A well-calibrated model is one that minimizes residuals, which is equivalent to saying that the model fits the test data well. (This is just GoF. How well the model generalizes.)\nClinical Data Science - focuses on applying data science to healthcare with the goal of improving the overall well-being of patients and the healthcare system\nDiscrimination - the ability of a model to rank patients according to risk (Often measured by AUROC)\nElectronic Health Record (EHR) -  comprehensive collection of all information by the individuals involved in patient care. This includes records from clinicians, laboratories, radiology imaging, health insurance, socio-demographics, genetic sequencing data, etc.\nElectronic Medical Record (EMR) - patient medical and treatment history within a single practice. EMR is the electronic version of the traditional paper records found in clinicians’ offices.\nEndpoint - Outcome variable measured in a medical study. e.g. Death, stroke, or quality of life are good endpoints. Blood tests and images on scans are not good endpoints. - A composite endpoint is one that consists of two or more events - Example: death due to cardiovascular causes or hospitalization due to heart failure - So the binary outcome would be a 1 if either of those events took place or a 0 if they did not. Or in a survival model, time until either of those events. - Issues - See The All-Important Endpoint of a Medical Study for details - See Harrell in the comments for the solution\nFHIR - Fast Healthcare Interoperability Resources - open healthcare data standard (also sets standards for other things) - one JSON data schema per healthcare concept — e.g. Patient, Observation, Condition, MedicationRequest, etc.\nHealthcare Analytics - analytics activities that can be undertaken as a result of data generated from core areas of healthcare including claims and cost data, pharmaceutical and research & development data, clinical data, patient behavior & sentiment data (narrower in scope compared to clinical data science)\nLife Expectancy - a snapshot of the current mortality (“expenctancy” comes from “expected value”) - Assumes that assumes that the observed age-specific death rates at the time of birth for a cohort stay unchanged for their entire lifetimes.\nLine of Business (LOB) - a statutory set of heath insurance policies\nProgression-Free Survival (PFS) - The length of time during and after the treatment of a disease, such as cancer, that a patient lives with the disease but it does not get worse. In a clinical trial, measuring the PFS is one way to see how well a new treatment works.\nProgression-Free Survival Rate - The percentage of people who did not have new tumor growth or cancer spread during or after treatment. The disease may have responded to treatment completely or partially, or the disease may be stable. This means the cancer is still there but not growing or spreading.\nSocial Determinants of Health (SDOH) - conditions in the environments where people are born, live, learn, work, play, worship, and age that affect a wide range of health, functioning, and quality-of-life outcomes and risks. (US Health and Human Services article) - 5 Categories: Economic Stability, Education Access and Quality, Health Care Access and Quality, Neighborhood and Built Environment, Social and Community Context\n\nProcessing\n\nCategoricals and Codes\n - Reconciling codes from various sources of data can be challenging because some coding systems are proprietary - e.g. “Heart failure” may be “123” in one coding system and “1a2b3c” in another - Solution: Tokenize data and create embeddings - Works as long as each healthcare dataset uses a consistent set of coding systems for itself - Tokenization - Text - Just split it by whitespaces. For - Example: “high glucose” becomes [“high”, “glucose”] - Codes - Embedding - For every field in every healthcare concept, we build a vocabulary of a predefined size - Creating a global vocabulary doesn’t work because different fields carries distinct healthcare semantics - Train an embedding for each token - Embeddings are learnt jointly with the prediction tasks - Choose the same embedding dimension for all fields within a given healthcare concept (easier for aggregating) - Aggregate to a common time-step - e.g. 1 hour or a few hours or can be tuned as a hyperparameter - Take the average of all field embeddings in an instance (embeddings for a time step) to form the aggregated embedding for that instance - Can also use median, etc. instead of mean - If there are multiple instances of a concept, we can further average the instance embeddings to form the embedding for that concept - Since the healthcare concepts are a predefined enumerated list, we can concatenate the concept embeddings together to form a fixed sized example. - If a concept does not appear in the time-step, we just set its embedding to all 0s. - Append timestamp feature to embedding - The timestamp of the instances / training examples are not evenly spaced. When a particular event occurs and has significant clinical meaning, it’s not captured by the embeddings. - Take the average of the timestamps of all instances in the time-stamp, and append it to the end of the fixed size embedding we obtained via fields -&gt; instance, and instances -&gt; concept aggregation - This part isn’t completely clear to me. Need to check the Google AI paper - I think it means average all the timestamps within each aggregated embedding and append it to the embedding - Seems like the averaged timestame would need to be transformed into a numerical before being appended.\n\nFairness and Privacy\n\nMisc - Notes from - Algorithmic Bias in Healthcare and Some Strategies for Mitigating It - Deidentified clinical data sets are collections of observational patient data that have been stripped of all direct Patient Health Information (PHI) components. IRB permission is not necessary for access to deidentified clinical data sets. - Clinical data sets with HIPAA restrictions include observational patient information such as dates of admission, discharge, service, and birth and death as well as city, state, zip codes with five digits or more, and ages expressed in years, months, days, or hours. Without a patient’s consent or a HIPAA waiver, HIPAA-restricted clinical data sets may be used or shared for research, public health, or healthcare operations.\nBiases to fairness - Historical bias - when the data collected to train an AI system no longer reflects the current reality - e.g. even though the gender pay gap is still an issue, it was worse in the past. - Representation bias - depends on how the training data is defined and sampled from the population. - e.g. the data used for training the first facial recognition system, mostly relying on white faces, which lead the model to have a hard time recognizing black faces and other dark-skinned faces. - Measurement bias - occurs when training data features or measurements differ from real-world data - e.g. where the data for image recognition is mainly collected from one type of camera while real-world data is from multiple types of cameras. - Coding/human bias - this happens mostly when scientists dive deep into a project with their subjective thoughts about their study - e.g. “non-white patients receive fewer cardiovascular interventions and fewer renal transplants”, and “Black women are more likely to die after being diagnosed with breast cancer”. Source\nStrategies to mitigate bias - Collecting and using diverse training data: - Data on-representative of the real-world population is a common cause of bias. - Collect and use diverse training data that accurately reflects the demographics, backgrounds, and characteristics of the population the algorithm will be used on. - Test the algorithm for bias: - Can be done using a variety of methods, including conducting bias audits and using fairness metrics to measure the algorithm’s performance. - Use algorithmic fairness techniques: - Pre-processing algorithms that adjust the data to reduce bias - In-processing algorithms that make adjustments during the training process - Post-processing algorithms that adjust the output of the algorithm to make it fairer - Ensure transparency and accountability: - Provide clear explanations of how the algorithm works, - Regularly review and update the algorithm to remove any biases that may have been introduced - Provide mechanisms for individuals to challenge the decisions made by the algorithm. - Engaging with diverse stakeholders - e.g. individuals and communities that may be affected by the algorithm - Understand their perspectives and incorporate their feedback into the design and implementation of the algorithm. - Can help ensure the algorith accurately reflects the needs and concerns of the population it will be used on.\nExamples of Algorithmic Bias - UnitedHealth Group - Developed a commercial algorithm in order to determine which patients would require extra medical care (patients with the greatest medical need). - A bias in the algorithm reduced the number of black patients identified for extra care by more than half, and falsely concluded that black patients are healthier than equally sick White patients. - race correlated with other factors such as historical healthcare expenditures to evaluate future healthcare needs, which made it reflect economic inequality rather than the true medical needs of patients. - Drug discovery for Covid-19 - An AI system was developed to triage patients and expedite the discovery of a new vaccine - The AI system was able to predict with 70 to 80% accuracy which patients are likely to develop severe lung disease - The triage process was solely based on the symptoms and preexisting conditions of patients, which can be biased because of the disparities based on race and social economic status."
  },
  {
    "objectID": "qmd/insurance.html",
    "href": "qmd/insurance.html",
    "title": "5  Insurance",
    "section": "",
    "text": "TOC\n\nMisc\nRisk\nAnalysis\nMarket Basket Analysis\n\nMisc\n\nDS Use Cases - Predict insurance claims frequency (see bkmk)\n\nRisk\n\nLimiting exposure - From http://ronaldrichman.co.za/2021/02/24/x-is-not-fx-insurance-edition/ - Severity - Capping the payout of a policy - e.g. only paying a maximum amount if tragedy strikes - Frequency - Setting a threshold to which the policy only pays out after the threshold has been passed - Keeps the insurance company from being needled to death by administrative costs of frequent policy payouts - e.g. minor doctor appointments - Reinsurance - policies that produce an option-like exposure, where one can pass risk above a fixed level of losses to the counterparty for a fixed premium (excess of loss). Other options are to share risks in more or less equal proportions. - allows insurers take on risky (and potentially more profitable) policies by taking on an insurance policy themselves for the excess risk - airplanes, volatile manufacturing, etc.\nAnalysis - Fit one distribution to the smaller and more frequent attritional losses, and another disruption to the extreme losses, with the latter distribution often motivated by extreme value theory - This approach ignores the fact the each loss has an upper bound determined by the limits on the policy generating the loss. Also, since these extreme losses follow a very heavy tailed distribution, naïve estimators of the statistical properties of these losses are likely to be biased - Shadow Moments - transform the data to a new domain that is unbounded, parameterizing EVT distributions in this domain, and then translating the implications of these models back to the original bounded domain - Cirillo, P., & Taleb, N. N. (2016). On the statistical properties and tail risk of violent conflicts. Physica A: Statistical Mechanics and Its Applications, 452, 29–45. https://doi.org/10.1016/j.physa.2016.01.050 - Cirillo, P., & Taleb, N. N. (2020, June 1). Tail risk of contagious diseases. Nature Physics, Vol. 16, pp. 606–613. https://doi.org/10.1038/s41567-020-0921-x\n\nMarket Basket Analysis\n\nsupport: What percent of patients have disease 1 and disease 2?\nconfidence: Of the people w/disease1, what percent also have disease 2?\nlift: How much more likely are you to have disease 2 if you already had disease 1 (and vice versa)"
  },
  {
    "objectID": "qmd/kpis.html",
    "href": "qmd/kpis.html",
    "title": "6  KPIs",
    "section": "",
    "text": "TOC\n\nMisc\nTerms\nGeneral\nProduct Metrics\n\nMisc\n\nAlso see - Job, Organizational and Team Development &gt;&gt; Developing a data strategy &gt;&gt; Objectives and Key Results (OKRs) - Product Development &gt;&gt; Metrics\nLeaders should focus on 3 to 6 KPIs that will drive growth.\nWhen a KPI’s growth begins to stagnate, break up the KPI monolith into smaller, more meaningful (and hopefully, easier to optimize) segments\n\nTerms\n\nOperationalization - a process of defining the measurement of a phenomenon that is not directly measurable (e.g. happiness), though its existence is indicated by other phenomena. It is the process of defining a fuzzy concept so as to make the theoretical concept clearly distinguishable or measurable, and to understand it in terms of empirical observations.\n\nGeneral\n\nKPI characteristics - Monotonic: you always want to drive them higher or lower - Typically a summary statistic or rate\nQuestion that you should ask when determining which metrics should be your KPIs What do you want to know?\nHow much do you need to know?\n\nWhy do you want to know this?\n\nWhat is the value or impact between not knowing and knowing?\n“Input –&gt; Output –&gt; Outcome” framework - Input Metrics - Provide information about the resources used to create a system or process - Useful for evaluating the efficiency of a system or process - What you control: amount of time you spend on a task, the quantity of materials used to produce something, etc. - Outputs should be highly responsive to your inputs - Output Metrics - Provide information about the immediate results of a system or process - Useful for evaluating the performance of a system or process - Answers whether the system or process is producing the desired results. - It should be clear how much your output would change for one additional unit of input. - Very actionable but not fully under your control - There is a causal link between your output and your outcome, so difficult to discover - May require experimentation to determine causality - Typically tracked daily - Outcome Metrics - North Star metrics (See Product Metrics &gt;&gt; Types) - What you are aiming to move with all your activities. - Typically requires multiple outputs to move - Typically tracked monthly or quarterly - Example: Educational Program - The # of teachers, their average seniority, the funding of the school (inputs) help drive the grades of the students and their consistency over time higher (output) which ultimately lead to more students graduating high school (outcomes).\nIndustry examples - Financial — Revenue growth, cash flow, burn rate, gross profit - Customer — Engagement rates, net promoter scores, acquisition costs, conversion rates - Support & Service — Turnaround time, mean time to resolve, SLA compliance, quality - Employee — Attrition and retention, satisfaction, engagement - Governance, Risk, & Compliance — Percent compliance to process, audit compliance, non-security incidents Issues - Avoid incentivizing the wrong behavior - As soon as someone’s performance is linked to a metric — it is fair game to expect them to try to move the metric, so it is up to the metric designer to make sure the ‘rules of the game’ are clearly articulated. - Example: Reducing the number of support tickets opened via email - Potential Solution: Engineer makes it as hard as possible to contact the email support - Unintended Consequence: Increasing the number of ‘negative’ social media interactions - Example: You don’t want to push your salespeople to sell without caring about the retention rate. - e.g Car salesmen selling a lemon to a customer. Customer unlikely to return to buy another car. - Solutions - Pair metric with a guardrail metric (see Product Metrics &gt;&gt; Types) - Examples - quantity (sales) + quality (retention rate) - short term metric (inventory level) + long term metric (# of shortages) - Design a compound metric - By taking into account multiple measurement, it cannot be easily gamed - Can also include metrics you don’t want move negatively (e.g. cost metrics, guardrail metrics, health metrics) - For experiments that use this metric, you then create a binary decision rule to decide if your experiment is successful or not. - Beware of Deceptive Metrics - Correlation does not equal causation - Example: Someone discovers that the # of transactions is correlated w/higher revenue - Company decides to make # of transactions a metric and starts trying to increase it. - e.g. re-targeting former customers, offering discounts, etc. - Revenue doesn’t substantially increase after considerable effort and resources - Analysis reveals that the revenue is being mostly driven by a few whales buying high-ticket items while the team’s increase in transactions was mostly low-ticket items - So, the strategy should’ve been to target more whales and not just trying to increase the # of transactions\nChecks - Metric is precise (i.e. measurement isn’t noisy) - Metric is accurate (i.e. it properly depicts the phenomenon it is supposed to depict).\nNext Steps - Figure out the best way to calculate these metrics - Create dashboards to monitor them over time - Set up alerts when thresholds are crossed - anomaly detection\n\nProduct Metrics\n\nWhy are they important? - Metrics help companies have clarity, alignment and prioritization in what to build. - Metrics help a company decide how to build the product once they’ve prioritized what to build. - Metrics help a company determine how successful they are and hold them accountable to an outcome.\nTypes - North Star Metric (NSM) - Long-term metric focused on the desired outcome for the entire company - Examples of potential NSMs - Instagram: Monthly Active Users - Spotify: Time Spent Listening - Airbnb: Booked Nights - Lyft: Rides per week - Slack: Daily Active Users - Customer happiness (e.g. revenue, Net Promoter Score (NPS), and customer satisfaction) - Quora: Questions Answered - Primary - Ask - “what is the desired outcome for this product?” (keeping the company’s mission in mind) - Example: number of high quality sellers that join the platform as a result of the email outreach - “What do our customers care about, and how do we solve it as fast as possible?” - Example - Engineering, product, and marketing agree that onboarding is a pain point, you decide to build goals and KPIs around making it easier for new customers to get started. - Align the company around the shared goal of reducing new tool onboarding from five days to three days - Data team gathers metrics on usage and helps build A/B tests - Engineering team modifies the product - Marketing team creates nurture campaigns - Map this outcome to a metric that is meaningful, measurable, and movable - Meaningful - Should reflect the way a company intends to drive value for its customers (based on the company’s mission)\n        -   Avoid _vanity_ metrics.\n                -   Example: Quora uses push notifications to alert users when they would be best suited to answer a question. However, the number of times users click on the notification and open the app is a vanity metric. It may make Quora feel good to see open rates going up. But Quora didn’t release push notifications to drive open rates. The outcome they were aiming for was to get high quality answers to questions asked on their platform. So an outcome oriented metric would focus on answer rates, not open rates.\n        -   Measurable\n        -   Each component of that metric's formula should be a datapoint that you can collect with high confidence and precision. And remember to add a time frame to your metric.\n        -   Some aspects of a company's or product's mission/goal aren't measurable directly but may be measured through latent variables, proxy, or highly associated variable or group of variables\n        -   Moveable\n        -   Basically means the metric shouldn't be so noisy as to inhibit being able to measure changes when you take actions to change it.\n        -   The metric should measure something that as directly under control of the company as possible.\n                -   Example: Page Load Time - time difference between a user clicking on a link or typing the URL in the browser and the page being rendered to the user.\n                -   This is affected substantially by the user's computer, ISP, browser, etc. along with the app/website code. So asis, it wouldn't be a good metric to track.\n        -   Other Desirables\n        -   Quick Feedback\n                -   Actions taken to influence the metric should be (almost) immediately observable\n                -   Example: Measuring (subscription) retention would require you to wait until the end of the month in order to be able to judge the effect of your action. Daily Active Users might be a better alternative\n        -   Easily Interpretted\n                -   The role of a metric is to align teams around a specific goal so that they can take the right steps towards achieving that goal. If people can’t understand the metric, they can’t take the right steps to optimize for it.\n                -   Examples:\n                -   Complicated: the median of a weighted combination of viewing time, comments, likes, shares per user (with each action having different weights based on its importance)\n                -   Simpler: average watch time per user.\n        -   Not Gameable\n                -   I.e. picking an easily moveable metric that has no real value leads to bad incentives.\n-   **Supporting/Tracking/Input Metrics**\n-   Leading indicators that the NSM or the primary metric is moving in the right direction\n-   Inputs into the NSM and are directly correlated to its value\n-   Also tells you where your efforts to move your NSM may be falling short.\n-   Examples\n        -   Number of emails sent\n        -   The number of people that opened the email\n        -   Number of businesses that signed up to sell on the platform\n-   **Counter Metrics/Guardrails**\n-   Other outcomes that the business cares about, which may be negatively affected by a positive change in the primary metric (or NSM)\n-   They exist to make sure that in the pursuit of your primary metric, you are not doing harm to another aspect of the business\n-   Example (email marketing to obtain quality sellers for Amazon)\n        -   if your primary metric focuses on product _quantity_, your guardrail metric might be around product _quality_\n        -   The guardrail, average number of purchases a user makes in a day, ensures that the influx of sellers that the primary metric optimizes for doesn’t result in consumers becoming so overwhelmed by choice, that they end up not buying anything at all\nUser journey: AARRR (or Pirate Metric) Framework - Awareness: How many people are aware your brand exists? - metric examples: number of website visits, social media metrics (number of likes, shares, impressions, reach), time spent on a website, email open rate - Acquisition: How many people are interacting with your product? - A lead is any potential user who’s information you’ve been able to capture in some shape or form. - e.g. people who give you their email addresses when they sign up for your mailing list are considered to be leads. - A qualified lead when they show additional interest in your product beyond giving you their information. - e.g. in addition to signing up for your mailing list they also watch a webinar or sign up for a demo - metric examples: number of leads, number of qualified leads, sign ups, downloads, install, chatbot interactions - Activation: How many people are realizing the value of your product? - typically in the form of an action taken x times with in a period of y days - When the activation hurdle is crossed, an individual goes from unknown entity to actual user. - Example (Dropbox): Their activation metric is the number of users that have stored at least one file in one dropbox folder on one device - metric examples: number of connections made, number of times an action is performed, number of steps completed - Engagement: What is the breadth and frequency of user engagement? - Depth of their usage: How often are they using your product? Is it above or below the average users frequency? - Breadth of their usage: Are they performing every action that’s possible with your product? Are they favoring some more than others? What if you have multiple products? Are they using all of them? - metric examples: daily, weekly and monthly active users, time spent in a session, session frequency, actions taken in the product - Revenue: How many people are paying for your product? - ** Remember, a business should optimize for the value they bring to their customers, not the revenue they generate. And, if their customers are deriving a lot of value from the business, willingness to pay will be a natural byproduct - metric examples: % of paid customers; average revenue per customer; conversion rate of trial to paid customers; number of transactions completed; shopping cart abandonment rates; ad-metrics like click-through-rate and conversion rate (crucial for ads based businesses) - Retention/Renewal: How often are your people coming back? - metric examples: % of users coming back to your platform each day, month, year (product dependent); churn rates; customer lifetime value - Referral: How many customers are becoming advocates? - metric examples: Net Promoter Score, viral coefficient i.e. the average number of people that your users refer you to - Also see - Survey, Design &gt;&gt; Response Scales &gt;&gt; Net Promoter Score - Algorithms, Product &gt;&gt; Net Promoter Score"
  },
  {
    "objectID": "qmd/location-selection.html",
    "href": "qmd/location-selection.html",
    "title": "7  Location Selection",
    "section": "",
    "text": "TOC\n\nMisc\nFactors\nLocation Profiles\n\nMisc\n\nCustomer research, market expertise & experience, and competitor location analysis can all help inform the important criteria for your business\nTool to calculate population density within a certain radius of a location\nMight be more useful to aggregate smaller geographies into overlapping circular areas to compare candidates - Would have to decide how to handle geographies that are only partially enclosed in a circlular area - Use a percentage? - Include the whole thing?\n\nFactors\n\nUnderstanding of the demographic or economic factors that must be in place to be successful\nExamples of questions - Do you need a large population? - High income population? - High presence of certain age brackets? - Do you rely on office worker foot traffic? - Is the presence of certain business types important (restaurants, healthcare facilities)?\nNon-data factors - appropriate accessibility (car traffic/foot traffic, street frontage) - signage - availability and size of space - cost/affordability\n\nLocation Profiles\n\nThese are created for existing stores and locations or potential new stores\nExample: Workforce and Demographic\n\n\n\nOther potential variables - Customer median driving distance - may also inform on the correct census geography to use - Distance_to highways, business district, etc.\n\nAnalysis\n\nUse thresholds for any profile variables to help narrow the group of potential candidate locations to a managable number - Might be useful to fit a decision tree to develop rules to use as thresholds - Example - Zip Code Population of 25,000+ - May want to use census geographies other than zip code - City Population of 150,000+ - Growing Population - Household Income of $75,000+ - High percentage of the population in the workforce - High economic activity - Primary industry of employment in White Collar - Percentage millennial population - Restaurant density\nScore candidate locations - Create weights for important profile variables and then calculate scores for each candidate location - Methods for creating weights - Wing it with domain knowledge - Coefficients from a regularized regression of KPI ~ standardized_profile_vars could be used as weights - Or feature importance, shapely values, etc. from tree model - Correlation or association statistics as weights - Order scores highest to lowest - If more than one location is considered, then group_by a suitably-sized geography\nCluster candidate location profiles with current successful stores - Candidate locations that are in the same cluster as your stores are the ones that should be considered - Prominent features of the cluster(s) may indicate which profile variables are more important than others\nTake top-n candidates and dig deeper: - Competitor analysis - Example questions - How many competitors exist is location? - Where are they located? - How satisfied are consumers with the options that exist today? - Which competitors are most popular, suggesting we may want to look in other areas? - e.g. Google Map, Yelp, etc. reviews of competitors at this location - Mapping may illuminate other considerations - e.g. one location has large swaths of uninhabitable land — is there enough population density for us to be successful? - How close are these locations to your other stores? - Could one leach customers from the other? - Examine profiles of final candidates - What are the primary differences? - What are the best features?"
  },
  {
    "objectID": "qmd/logistics.html",
    "href": "qmd/logistics.html",
    "title": "8  Logistics",
    "section": "",
    "text": "TOC\n\nMisc\nDemand Forecasting\nSafety Stock\nEconomic Order Quantity (EOQ)\nOrder-Up-To Level Policy\nReorder Point\nDecision Impact Metrics\nProfit functions for perishable products\nDoorDash\nSupply Chain\nWarehouse Management\n\nMisc\nGoal: develop a replenishment policy that will minimize your ordering, holding and shortage costs.\n    -   Ordering Costs ($ per order): fixed cost to place an order due to administrative costs, system maintenance or manufacturing costs\n    -   Holding Costs ($ per unit per unit of time): all the costs required to hold your inventory (storage, insurance, and capital costs)\n    -   Shortage/Stock-out Costs ($ per unit): the costs of not having enough inventory to meet the customer demand (Lost Sales, Penalty)\n\nForecast PI widths serve as a proxy for inventory holding costs and provides valuable input for setting a target service level\nPackages - {planr} - uses opening inventory, sales forecasts and supply variables to calculate projected inventory and projected coverage calculations (article)\nQuestion: “Due to the complexity/cost of maintaining inventory management system. Would it be sufficient to just set a safety stock level and replenish once the SKU dips below that level?” - The more products you handle, the more an inventory management system matters. If you have 1000s or 10s of 1000s of different products, it makes a large difference whether you do demand forecasting along with implementing the Order-Up-To Level Policy, etc.\nVariables of interest that need to be forecasted for various decisions - Stock/No Stock - should we continue to stock a product or discontinue the product and just let the current stock dwindle to zero. - Mean of Demand - Replenishment - how much product should we restock - Mean and Variance of Demand - Forecasts in conjunction with a hypothesized demand distribution (parametric) vs build-up of the empircal distribution via bootstrapping (non-parametric) - Returns - customer returning products - Net Demand which is equal to Demand - Returns - Can be forecasted itself or by forecasting Demand and Returns separately. - Last Time Buy (LTB) - the supplier’s “last call” for a part or component. The final chance an enterprise will have to buy the part before the supplier stops producing it. - Rate of Demand Decline\n\nTerms\n\nActive References - a product may have multiple SKUs. All SKUs would be needed in order to calculate stats for that product. The active reference for a product is one SKU that encompasses all other SKUs. - e.g. Last season’s dress “C” has been replaced by the new dress “D”. Even though both dresses are identical, they have different SKUs. Dress D’s SKU will be the active reference. Therefore, if the retailer sold two units of C in the past and three units of D this week, Nextail will show D having sold five units. - Also seen this term used when referring to all unique SKUs on an order sheet as the total active references.\nLinked Lines (aka Silent Switches) - refer to when several products in a retailer’s inventory are commercially equivalent. In other words, when identical products are identified by multiple SKUs. - Example: continuity products are ordered over multiple seasons or years or when large orders are split among different suppliers\nOrder Components - Order: The “shopping basket” full of items you’ve just purchased. - Lines: The different products within your order, recognized by warehouses as each individual Stock Keeping Unit (SKU) or Universal Product Code (UPC) number. - Units: The quantity of each line.\nReference - id for a product (e.g. SKU)\nRotations - speed at which products enter and exit the warehouse - High rotation: Units enter and exit continuously. These items are in high demand. - Medium rotation: Units enter and exit in smaller volumes than those in High Rotation. - Low rotation: These are the items that spend the most time in the warehouse, and are in low demand.\nSKU - Stock Keeping Unit - Usually a bar code that has all the information to distinguish it from another product. These attributes can include manufacturer, description, material, size, color, packaging, and warranty terms. When a business takes inventory of its stock, it counts the quantity it has of each SKU.\nSupply Chain - a network of processes and stock locations built to deliver services and goods to customers.\nWastage - where supply greatly outstrips demand, and the product expires\nS&OP - Sales & Operations Planning\n\nDemand Forecasting\n\nMisc - Also see DoorDash &gt;&gt; Forecast Supply and Demand - Reinforcement Learning for Inventory Optimization Series III: Sim-to-Real Transfer for the RL Model | by Guangrui Xie | Jan, 2023 | Towards Data Science\nMetrics for judging demand forecasts - change in the number of Stock-Out days - percent change in Service Level - change in Profit - change in wastage - change in inventory costs\nNotes from Are Your Demand Forecasts Hurting Profits and Service Levels - Blue Dot Thinking’s WasteNot API service ($) WasteNot (logistics analytics firm) uses prophet for demand forecasting which consistently under-predicts, so they use an adjustment for better forecasts\n-   Prediction adjustment - (e.g. for perishables) calculate an optimal “Buffer Multiplier” value — each predicted value from the statistical forecast is multiplied by the multiplier, resulting in a higher number of units replenished each day\n-   Variables used:\n        -   unit\\_sale\\_price/unit\\_cost\n        -   historical variability\n        -   shelf-life (seconds)\n        -   stock levels\n    They didn't show the formula, but I'm guessing their modeling residuals with these variables.\nNotes from Case Study: Applying a Data Science Process Model to a Real-World Scenario A VERY detailed article that goes through a scenario of step-by-step planning and execution of changing a manual stock replenishment process to an automated one\nAs a guide, they use [DASC-PM](https://medium.com/towards-data-science/dasc-pm-a-novel-process-model-for-data-science-projects-9f872f2534b1) (DAta SCience - Process Model) - a structured and scientific process for project management\n\n-   Project manager tries to examine whether the project can fundamentally be classified as feasible and whether the requirements can be carried out with the available resources.\n-   Expert Interviews: Is the problem in general is very well suited for the deployment of data science and are there corresponding projects that have already been undertaken externally and also published?\n-   Data science team: Are there a sufficient number of potentially suitable methods for this project and are the required data sources are available?\n-   IT department: check the available infrastructure and the expertise of the involved employees.\nDemand forecasting model\n-   Requirements:\n        -   Accuracy of 75%. This means that the forecasts for quantities of each product should deviate from actual requirements by no more than 25%.\n        -   Produce monthly planning cycles and quantify the need for short-term and long-term materials\n-   Data sources: Order histories, inventory and sales figures for customers, and internal advertising plans\n-   Features: seasonality, trends, and market developments\n-   Forecasts regenerated every month\n-   Forecasts loaded into internal planning software\n-   Projections will be analyzed and, if need be, supplemented or corrected.\n        -   Planners can make their corrections during the first four working days of the month.\n        -   The final planning quantity will ultimately be used by the factories for production planning.\n-   Example: IBM Planning Analytics\n        -   Allows for the creation of flexible views where the users can personally choose their context (time reference, product groups, etc.)and adjust calculations in real-time.\n        -   Sounds like expensive optimization software with a snazzy UI\n-   Final plans are loaded into the Data Lake after processing by the planning teams so they can be referenced in the future.\n-   User Integration\n-   Users are included in the development from the beginning to ensure technical correctness and relevance and to ensure familiarity with the solution before the end of the development phase.\n-   Simple line and bar charts for processes and benchmarks are used, along with tables reduced to what is most important.\n-   Planners get training sessions to help them interpret the forecasts and classify their quality.\n-   Complete documentation is drafted\n        -   Technical part:  data structures and connections\n        -   Content part: jointly prepared with the users to describe the usage of the data product\n-   Post-Development Phase (i.e. maintenance of the project)\n-   Constant automated adjustment of the prediction model to new data\n-   Various parameters such as the forecast horizon or threshold values for the accuracy of the prediction can be made by the planners\n-   Problems occurring after the release of the first version are entered via the IT ticket system and assigned to the data science area\n-   At regular intervals, it is also checked whether the model still satisfies the expectations of the company or whether changes are necessary.\nForecasting shocks is difficult for an algorithm - Notes from Why Good Forecasts Treat Human Input as Part of the Model - Preprocessing - It can be better to smooth out (expected) shocks in the training data and then add an adjustment to the predictions during the dates of the shocks. - The smoothed out data will help the algorithm produce more accurate predictions for days when there isn’t an expected shock. - e.g. kalman filter with parameters for seasonality, trends from {{tsmoothie}} - Nothing special about this smoother. Probably just the method in tsmoothie that performed best for them (which is a good reason to use it). - Approaches for manually replacing oulier values - Replace the outlier week with the most recent week prior to the dip as a proxy for what should have happened. - Use domain knowledge - e.g. Marketing departiment is expecting 10% growth the week after it’s new promotion - Replace with one of the previous methods, then apply a smoothing algorithm - Shows the Kalman filter by itself fails to replace the outlier dip with the normal, expected peak, but the manual adjustment + Kalman gives us what we want - Examples of shocks that may need training data to have manual adjustments and not be smoothed by an algorithm - one-time spikes due to abnormal weather conditions - one-off promotions - replace the outlier week with the most recent week prior to the dip as a proxy for what should have happened. - a sustained marketing campaign that is indistinguishable from organic growth. - Prediction Adjustment - Extreme events (i.e. unprecedented promotions or weather), where the expected impact is well outside of any historical data, may prove impossible for forecasting methods to produce an adequate forecast - Since it’s outside the range of historical data, building a more complex model or just including weather or promotional features won’t help. - Requires using domain expertise of the event to use to adjust the predictions - Make sure to build-out the code infrastructure so that each manual adjustment should just feel like adding another input to the model.\nIntermittent Demand - Misc - Also see ADAM ebook chapter, post, real world use case on adding an occurrence variable to an ETS model to handle intermittent data - The use case paper also includes a GAMLSS with truncated Normal distribution model with code that performed well - Uses {probcast} which has functions around gams, gamlss, and boosted gamlss models from {mgcv}, {mboost}, {gamlss}, etc. - Notes from - CMAF FFT: Intermittent Demand Forecasting (Video) - From authors of “Intermittent Demand Forecasting. Context, Methods and Applications” (see your book shelf) - Preferrable to avoid intermittence by aggregating data to a category that’s higher in the product hierarchy or lengthen the frequency. - Not always possible. For example, different SKUs have different lead times, so aggregating products into categories with the same protection intervals can be complicated. - Protection Interval is your horizon, which equals the length of the review period + length of the lead time - See Order-Up-To (OUT) Level Policy for details - Compound Distributions - Target - Incidence/occurrence (of Demand): Poisson and Bernoulli - Discrete positive Demand interspersed with zeros: Neg. Binomial - Sounds like zero-inflated, censored distribution - Options - Discrete Compound Poisson (aka Stuttering Poisson): Poisson + Geometric - Negative Binomial: Poisson + Logarithmic - Lumpy data - Data that’s intermittent and has extreme variability in demand sizes - Negative Binomial built to handle this type of data - Forecasting Mean Demand - Parametric - Exponential Smoothing - Popular method but bad for intermittent forecasting - ADAM extends ETS by adding an occurence variable to the model, so this might not be the case. - Biased on “issue points” (see video for more details) - Croston Method - Inversion Bias: we believe the mean demand is higher than predicted - SBA (Syntetos-Boylan Approximation) - *Recommended* - Corrects Croston bias - Supported by empirical evidence - Temporal Aggregation: Overlapping vs Non-Overlapping - self-improving mechanism - See video for details (although it wasn’t discussed in great detail) - Non-Parametric (aka Empirical) - Bootstrapping - resample in blocks or resample independently - independently is how you normally see bootstrapping - With blocks, bin your sequential data into non-overlapping partitions and resample independently within each partition. - Forecasting Demand Variance - Using the variance of the forecast error of the protection interval to estimate the variance of demand over the protection interval - Classical method is to calculate the variance of the errors over each review period and aggregate to the get the variance over the protection interval, but the above method is better empirically and theoretically - Diagnostics - Do NOT use metrics based on absolute errors (e.g MASE, MAPE) by themselves - Minimization of these metrics, by themselves, can result (i.e. over half of your values are zeros) in always recommending 0 value forecasts - Can be used in conjunction with bias correction measures - Scaled Mean Squared Error - Relative Root Mean Squared Error - Look at the predictive distributions instead of just the point forecasts - Accuracy Implication Metrics - measure the effect of the forecast on inventory - Y Axis: 1 - Fill_Rate; X Axis: Average on-hand inventory - Each method is a forecasting model - Think the Fill Rate is estimated from a simulation using the method’s forecast and a range of average SOH values as parameters\n\nSafety Stock (aka buffer stock)\n\nMisc - Notes from Article - Calculation of Safety Stock can be more useful than trying to improve forecast accuracy for intermittent(or sporadic) product time series (lotsa zeros).\nSafety stock - extra inventory held by a retailer or a manufacturer in case demand increases unexpectedly. This means it’s additional stock above the desired inventory level that you would usually hold for day-to-day operations.\nReplenishment cycle - cycle between replenishment (i.e. restocking) orders\nFill rate (aka Fulfillment Rate)- the percentage of orders that you can ship from your available stock without any lost sales, backorders, or stockouts. - Fill Rate = (Total Orders Shipped / Total Orders Placed) x 100 - On average, companies typically maintain a fill rate of about 85%-95%. But ideally, you should strive for a fill rate between 97% and 99%.\nReasons for safety stock: - Demand uncertainty - Every retailer and manufacturer will have products that sell well all year round and products that fluctuate in demand. - Lead time uncetrainty - deliveries arriving earlier or later than expected, a safety stock formula will help you to cover unexpected delays and demand fluctuation to maintain a consistent output. - lead time (aka performance cycle)- time required between the creation of a replenishment order and the effective store replenishment - Usually a distribution and not a constant, so it needs to be recorded in order to get a sample standard deviation thats used the safety stock formula - Factors: - deciding what to order or produce - approval time - submitting a purchase requisition - emailing vendors - manufacturing and processing of the product - delivery time from vendor - incoming inspection time - time it takes to put on the shelf - any additional time required to return to the start of the next cycle Stockouts - out of stock events - Safety stock determinations are not intended to eliminate all stockouts—just the majority of them Usually caused by: - Changes in consumer demand - Incorrect stock forecasts - Variability in lead times for raw materials - Costs due to stockouts - Loss of revenue - Loss of gross profit - Loss of customers - Reduced market share - Poor efficiency - Strained supplier and retailer relationships\n(Cycle) Service level (Z) - the probability that the amount of inventory on hand during the lead time is sufficient to meet expected demand – that is, the probability that a stockout will not occur. - Higher service level –&gt; more safety stock - Independently choose a service level for groups of products based things such as strategic importance, profit margin, or dollar volume. - The retail industry aims to maintain a typical service level of between 90% and 95% depending on the product - Example:  - At 95 percent service level, expect: - (D1) for 50 percent of replenishment cycles, not all cycle stock will be depleted and safety stock will not be needed - (D2) for 45 percent of replenishment cycles, the safety stock will suffice. - (D3) and for 5 percent of replenishment cycles, expect a stockout. - Kind of a confusing diagram but I think the y-axis is total stock and x-axis represents time (kinda sorta) - Stock dwindles as product is sold as the cycle ends, then stock is replenished after an order and begins to dwindle again.\nSafety Stock equations - When the demand interval doesn’t equal lead time interval - used to mitigate demand variability and lead time variability (σLT see next formula) is very small or zero - Safety stock = Z × √[PC / T1] × σD - PC = performance cycle, another term for total lead time - T1 = time increment used for calculating standard deviation of demand - σD = standard deviation of demand. - (Partial) Example: if the standard deviation of demand is calculated from weekly demand data and the total lead time including review period is three weeks. - PC = 21 days (3 weeks) - T1 = 7 days (weekly data) - Safety Stock = Z * √3 * σD - Example: Desired service level = 95%; seven-day manufacturing time and the one day needed to arrive at the warehouse; Standard deviation of weekly demand = 10 rolls - Safety stock = 1.65 * √[8/7] * 10 rolls - Safety stock = 18 rolls - When the lead time varies and demand variability (σD see previous formula) is very small or zero - Safety Stock = Z × σLT × Davg - Z is the z-score of the service level (1-sided, upper-tail) - e.g. 95% service level –&gt; Z = 1.64; 90% service level –&gt; Z = 1.28 - qnorm(0.95) = 1.644854, qnorm(0.90) = 1.281552 - σLT is the standard deviation of the lead time - Davg is the demand forecast - Depends what the frequency of the series is, but you want an estimate of the total demand between orders - Example: If orders are made monthly and you forecast weekly sales, then your horizon is likely monthly and you sum around 4 weeks of point estimates to get D. - When both demand variability and lead time variability are present - demand and lead time variability are independent - Safety stock = Z × √[(PC/T1  ×  σD2 ) + (σLT × Davg)2] - demand and lead time variability are not independent - Safety stock = (Z × √[PC/T1] × σD) + ( Z × σLT × Davg) - Demand variability is the dominant influence on safety stock requirements. - With the recognition of what factors dominate an equation, it becomes easier to focus improvement efforts\nIssues - Sometimes recommended safety stock volumes are larger than business leaders are comfortable having - alternative/supplement: order expediting - Reduce safety stock volume by keeping small amounts of expensive products and rely on air freight to cover peaks in demand. The cost of shipping a small percentage of total demand via air can be minimal compared to the cost of carrying large amounts of safety stock of the valuable material on an ongoing basis. - alternative/supplement: make-to-order (MTO) or finish-to-order (FTO) production environment - If lead times allow, MTO eliminates the need for most safety stock. Meanwhile, FTO allows for less differentiation in safety stock than finished-product inventory, which lowers demand variability and reduces safety stock requirements. - FTO and MTO also are well suited for situations where customers are willing to accept longer lead times for highly sporadic purchases.\n\nEconomic Order Quantity (EOQ) (aka Wilson Formula)\n\nthe ideal order quantity a company should purchase to minimize inventory costs such as holding costs, shortage costs, and order costs - Usually used for purchase ordering (not production) - Assumes demand, ordering, and holding costs remain constant over time\nGoal: minimize the cost of ordering and holding stock, while still meeting demand and service level requirements\nCosts of ordering: - placing your order - delivery - transportation - receiving the order\nCosts of holding stock: - paying for stock in advance - warehousing - storage - depreciation\n\n\nOrder-Up-To (OUT) Level Policy \n\nTime intervals (i.e. Review Interval) trigger a replenishment, not reorder points - Length depends on the industry - Examples - Manufacturing: 1 month - Retail: 1 week\nMisc - Notes from - CMAF FFT: Intermittent Demand Forecasting (Video) - From authors of “Intermittent Demand Forecasting. Context, Methods and Applications” (see your book shelf) - Should have more details on OUT replenishment model - UT-Dallas “Basestock Model CH. 13” Slides - Based on Cachon & Terwiesch book, “Matching Supply with Demand” (link)\nInventory Position (IP) = Stock-on-Hand - backorders + On-Order-Inventory\nAfter every Review Interval, the OUT gets optimized according to Replenishment variables - Mean and Variance of Demand are estimated - Forecasts in conjunction with a hypothesized demand distribution (parametric) vs build-up of the empircal distribution via bootstrapping (non-parametric) - See Demand Forecasting &gt;&gt; Intermittent Demand &gt;&gt; Forecasting Mean Demand - Given updated variables, place order that raises IP to OUT level (S)\nProtection Interval is period that you should have enough inventory to cover. - The forecast horizon which equals the Review Interval + Lead Time\nEvaluate service at each level of an order: orders, lines, units. - Lines (SKU Level) - Whether to use Cycle Service Level (CSL) or Fill Rate (ReadyRate (?) is also a possibility) - CSL - probability of not going out of stock -  not realistic but easy to calculate - Fill Rate measures true service offered to customers, but more involved in its application - Also see Safety Stock for more details on CSL and Fill Rate\n\nReorder Point\n\nThe reorder point is the threshold amount of inventory at which you need place an replenishment order - once an item’s stock falls below PAR level, an optimised order quantity is generated\nComponents used to determine a reorder point - Safety Stock - Reorder Point Formula - Periodic Automatic Replacement (PAR)\nReorder Point = Safety Stock + (Davg × Lead time)\n\nDecision Impact Metrics\n\nOther methods for determining forecast performance have serious flaws - Reasons industry benchmarks for forecast accuracy shouldn’t be used - diversity of business strategies (size of portfolio, product & brand positioning) - the level at which the forecast accuracy is measured and even the metric’s definition itself (especially with value-weighted formulas) may differ - Forecast Value Added (FVA) - FVA: The change in a forecasting accuracy metric that can be attributed to a particular process or participant in the forecasting process. - Example - Statistical model has 5% FVA vs Naive forecast - Adjusted Statistical Forecast has 2% FVA vs Naive Forecast - Forecast gets manually adjusted by management - Reasons why FVA shouldn’t be used - The difference in Forecast Accuracy (FA) metrics (e.g. MAPE) of the production forecasting algorithm and the naive forecasting algorithm is usually weighted by portfolio revenue, volume or number of items - A FA metric is not a key business performance indicator (KPI) - FA has little correlation with business performance. - Improving FA does not mean you are generating value. - Costs may increase, decrease or remain the same as accuracy changes. - FA metrics can contradict each other - Switching from one FA metric to another could profoundly alter your FVA results.\nConstraints and business rules have to be considered and not just a forecast metric - e.g. allowed pack sizes, the minimum order quantity, the storage finite capacity, holding/excess costs, shortage costs, fixed costs, etc.\nForecast Accuracy (FA) metrics are difficult to understand for normals - Using Decision Impact (DI) metrics can help eliminate: - “The forecast is always wrong!”, “The forecast is too this”, “The forecast is not enough that”, “What does 70% FA mean?”, “Is this good or bad?”\nProbabilistic Forecasts + DI metrics - Identify parameters (i.e. whatever value you’re forecasting) with the highest economic risk - Economic risk is related to the size of the PI of the forecast - Investigate potential causes for the risk and find solutions - Example - Calculate - DIa min - the costs associated with the forecast at the 5% percentile - DIa max - the costs associated with the forecast at 95% percentile. - Calculate economic risk\nDecision-based Components - components that takes into account the decision being made using the forecast - Example: One weather forecast predicts 4 inches of rain but another forecast predicts 0 inches of rain. The next day it rains 1 inch. - If the decision being made was whether to take an umbrella, the first forecast is the best forecast even though its error is worse than the second forecast. - Components - Decision Function - for any forecast input, simulate the decision process and evaluate the quality of the final decision - Decision Impact (DI) - metric that defines how decision quality is measured - usually in terms of financial cost - A “North Star” type metric\nDecision Cost function: The cost function is used to score each stock replenishment decision based on its true business impact usually in terms of financial cost - Example: Walmart retail data (M5) - The Decision Cost is the sum of following factors: - Ordering, shipping and handling cost (Fixed Costs) - Fulfilling an order generates costs for the ordering, preparation, expedition, transportation, etc. - Let’s assume these costs represent $40 per range of $1000 of purchase value. - Holding cost (Excess Costs) - Holding costs are associated with the storage of unsold inventories. - Let’s assume the annual holding cost is 10% of the inventory value (valued at purchase price), i.e. 0.19% per week. - Shortage cost - When demand exceeds the available inventory, both the demand and customer goodwill may be lost. - As retailers propose a wide range of similar products, a part of the demand is carried to other products. - Let’s assume that only half of the sales will effectively be lost. The shortage cost could then be measured as 50% of each lost sale gross margin. - Other Considerations - Replenishment Strategy - Leadtime - Order Cycle - Replenishment Policy - Safety Stock - Additional Product Information - Gross Margin - Pack Sizes - Initial Inventory - example: using the safety stock value - Computational Costs - Algorithm training time - Compute size - Data pipeline\nTypes of forecasts that are required - “actual” forecast: forecast from a candidate model that will potentially go into production - “naive” forecast: forecast from a simple method (e.g. seasonal-naive, simple moving average, etc.) or a previous used method\nSteps\n\nDecide how best to measure the Decision Impact as related to the forecast\n\ne.g. financial cost\n\nFormulate a decision cost function\n\ne.g. fixed costs + excess costs + shortage costs (see above)\n\nGenerate forecasts on a test/assessment set.\nCalculate decision impact costs:\n\nActual cost (DIa): cost after applying the decision cost function to the “actual” forecast (see above)\nNaive cost (DIn): cost after applying the decision cost function to the “naive” forecast (see above)\nOracle cost (DIo): cost after applying the decision cost function to the observed values - This would be the costs of a forecast in which we had perfect knowledge. (So probably just fixed costs)\n\nCalculate Decision Impact metrics:\n\nEarned Value (aka Forecast Value Added) (DIna) - DIna = DIn - DIa\nUnearned Value (aka yet-to-be earned value)(DIao) - The value that could still be gained by improving forecasts - DIao = DIa -DIo\nTotal Earnable Value (DIno) - The range of earnable value - DIno = DIn - DIo\nProportion of the Earned Value - DIna/DIno\nProportion of yet-to-be earned value - DIao/DIno\n\n\nDI metrics for all generated forecasts can be used to calculate a quarterly, semestrial, or annual ROI for the product planning department - ROI = sum(DIna)- (cost of people, tools, etc. used by department to generate forecasts)\n\nProfit functions for perishable products\n\nThe newsvendor problem is a class of problems, where the product can only be sold one day, after which it goes to waste. So this is appropriate, for example, for perishable products in retail\nNotes from An Integrated Method for Estimation and Optimisation; associated paper\nIf we order more than needed, we will have holding costs. In the opposite case, we will have shortage costs.\n\nBased on these costs and the price of product, we can find the optimal amount of product to order, that will give the maximum profit.\n\nInstead of a two-stage problem (see DoorDash section): optimising the forecast model via MSE or any other conventional loss and then solving the optimisation problem, we could estimate the model via maximisation of the specific profit function, thus obtaining the required number of product orders directly.\nCalculate profit as a linear\n\nπ(_q__t_,_y__t_)={_p__y_t−_v__q_t−_c__h_(_q__t_−_y__t_),_p__q_t−_v__q_t−_c__s_(_y__t_−_q__t_),for ​_q__t_≥_y__t_for ​_q__t_&lt;_y__t_,\n\nTerms - yt is the actual sales - p is the price of the product - qt ​is the order quantity - v is the cost of production - ch is the holding cost - cs is the shortage cost\n\nlibrary(greybox)\n# Generate artificial data\nx1 &lt;- rnorm(100,100,10)\nx2 &lt;- rbinom(100,2,0.05)\ny &lt;- 10 + 1.5*x1 + 5*x2 + rnorm(100,0,10)\nourData &lt;- cbind(y=y,x1=x1,x2=x2)\n# Define price and costs\nprice &lt;- 50\ncostBasic &lt;- 5\ncostShort &lt;- 15\ncostHold &lt;- 1\n# Define profit function for the linear case\nlossProfit &lt;- function(actual, fitted, B, xreg){\n    # Minus sign is needed here, because we need to minimise the loss\n    profit &lt;- -ifelse(actual &gt;= fitted,\n                    (price - costBasic) * fitted - costShort * (actual - fitted),\n                    price * actual - costBasic * fitted - costHold * (fitted - actual));\n    return(sum(profit));\n}\n# Estimate the model\nmodel1 &lt;- alm(y~x1+x2, ourData, loss=lossProfit)\n# Print summary of the model\nsummary(model1, bootstrap=TRUE)\nCoefficients:\n            Estimate Std. Error Lower 2.5% Upper 97.5% \n(Intercept)  36.5177    14.2840    2.7783    51.4844 *\nx1            1.3622    0.1622    1.1909      1.7528 *\nx2            3.3423    2.7810    -6.5997      5.9101\n\nInterpretation: with the increase of the variable x1, the orders should change on average by 1.36\nPlot\n\nplot(model1, 7)\n\n\nFigure above corresponds to the orders (purple) and would cover roughly 90.91% of cases (black), so that we would run out of product in approximately 10% of cases, which would still be more profitable than any other option.\nNonlinear case - See link to associated paper above - The only thing that would change is the loss function, where the prices and costs would depend non-linearly on the order quantity and sales.\n\nDoorDash Notes from https://towardsdatascience.com/managing-supply-and-demand-balance-through-machine-learning-70d4f0808617\n\n2-stage solution - forcasting supply and demand - optimizing supply of workers with demand of food orders\nDefine the problem - Supply and demand imbalance  (i.e. drivers and food orders) - Effects - For consumers, a lack of driver availability during peak demand is more likely to lead to order lateness, longer delivery times, or inability to request a delivery and having to opt for pick up. - For Dashers, a lack of orders leads to lower earnings and longer and more frequent shifts in order to hit personal goals. - For merchants, undersupply leads to delayed deliveries, which typically results in cold food and a decreased reorder rate.\nOptimization Strategies - Balancing at the delivery level means every order has a Dasher available at the most optimal time - consumer preferences and other changing conditions in the environment, such as traffic and weather, make it difficult to balance supply and demand at the delivery level - Balancing at the market level means there are relatively equal numbers of Dashers and orders in a market but there are not necessarily optimal conditions for each of these groups at the delivery level.\nMetric - Number of hours required to make deliveries during a time period - Optimize keeping delivery durations low and Dasher (drivers) busyness high - Able to account for regional variation driven by traffic conditions, batching rates, and food preparation times. - Units: hourly or day-parts (e.g. breakfast, lunch, dinner) - There’s too much variation during a day in order to aggregate to a higher level metric. Demand and supply would be artifically smoothed. - Example: - Sunday at dinner time in New York City, and we estimate that 1,000 driver hours are needed to fulfill the expected demand. We might also estimate that unless we provide extra incentives, only 800 hours will likely be provided organically. Without mobilization actions we would be undersupplied by about 200 hours.\nOptimization - Adjust supply of drivers by incentivizing with pay bonues during high demand hours\nForecast Supply and Demand - LightGBM - Predictors - Information about population size, general traffic conditions, number of available merchants, climate, and geography - character variables were replaced with embedding vectors - Supply - Counterfactual to understand how to make tradeoffs between supply and costs - How will supply levels change if we changed incentive levels so that we can ? - I assume “incentive_level” is a variable in the supply forecast model, so this could just be adjusted in “newdata” and a  prediction (or maybe during a training session?) made to see the effects on supply.\nDecision Making - consumes supply and demand predictions and attempts to generate a set of optimal actions - Mixed-Integer Programming (MIP) - linear optimization (see bkmks, ompr pkg) - easy to formalize, implement, and explain to stakeholders - Custom objective function for minimizing undersupply with several constraints. - Can be configured to favor either profitability (profit per customer?) or growth (increase in orders per customer?) - or maybe its about drivers — profitability (just enough incentive to get just enough drivers?) or growth (not sure what the “driver” angle is here) - Constraints - Never allocate more than one incentive in a particular region-time unit. - Never exceed the maximum allowable budget set by our finance and operations partners. - regional - different budgets, custom penalties, exclusion criteria for which units should not be included in the optimization, or incentive constraints that are guided by variability of the inputs. - Optimizer must account for uncertainty - City B’s forecast distribution has substantial uncertainty, and it’s mean says it will have enough drivers (i.e. oversupply) - City A’s forecast distribution is more certain, and it’s more likely to be undersupplied. - W/o taking uncertainty into account, the optimizer will not take into account that there’s a sizeable chance B will be undersupplied - Taking uncertainty into account sometimes causes an over-allocation of resources to these uncertain regions (small areas, fewer orders, fewer drivers, larger variance) - They do something with resampling to solve this but I didn’t quite understand it.\n\nSupply Chain\n\nMisc - Notes from: 4 Smart Visualizations for Supply Chain Descriptive Analytics\nFlow Distribution of units between production areas and markets - Data  - Source: the production facility name (left-side) - Target: the market supplied (right-side) - Units: the number of items flowing (width of bars) - Interpretation - India is the biggest country for production output - Japan market demand is mainly supplied locally - USA and Germany do not have local production facilities\nNetwork Optimization\n-   x-axis: each column represents a demand scenario (i.e. there are 50 demand scenarios in this example)\n-   y-axis: are the production/supply locations\n-   A blue box means that that location is included in the optimal configuration of locations for that scenario\n-   e.g. In scenario 1, having a low capacity facility in India and a high capacity facility in India is optimal for this scenario.\n-   I think this viz can be done with {[waffle](https://github.com/hrbrmstr/waffle)} using `geom_waffle`_without_ `theme_enhance_waffle`\n\n-   Simulate how the variability of demand in various markets (e.g. 50 scenarios) affects the optimal distribution of production/supply locations\n-   Hopefully a configuration of locations will be optimal for a preponderance of scenarios. Assuming each scenario is equally important, that configuration of locations is the optimal choice.\n        -   Or I guess you could weight each scenario by frequency or something. Maybe you have a distribution of scenarios from which you drawing from.\n-   Linear programming\n-   Also see [Optimization, general](Optimization, general)\n-   Set decision variable, objective function\n-   list the constraints according to the demand for each market\n-   Solutions are indicator variables for production/supply locations and whether they are 1 or 0.\n        -   There should be a boolean variable for a high capacity location and low capacity location in each country\n        -   For each variable, 1 indicates that location should be built or that it should be in operation at that particular capacity\nPareto Plot - Data - “BOX” is the number of box/packs picked of that product (“SKU”) for that order (“ORDER_NUMBER”) on that date (“DATE_FORMAT”) - Preprocessing - Sum the number of boxes picked per SKU - Sort your data frame by descending order on BOX quantity - Calculate the cumulative sum of BOX - Calculate the cumulative number of SKU\n\nWarehouse Management\n\nMisc - Picking operations account for the largest proportion of the total warehouse costs (up to more than 60%). This is why the design of these areas is of such importance. - The closer the high demand or large goods are to the loading and unloading docks, the lower the handling costs.\nMaterial Flow Types - Simple flows: To understand how these movements work, we can examine the simplest possible flow, which takes place when units sent by the supplier are used, without dividing these up. - Medium flows: Movements start to become more complex with this type of flow. It is normally found in warehouses with single or combined picking operations, generally with the supply of full pallets. - Complex flows: There are warehouses with different working areas, depending on the types of product and their consumption. They normally have intermediate handling areas and can require various operations that in turn need flows of a certain (and at times great) complexity. This diagram shows an example of this type of facility and the loading movements that occur there.\nWarehouse Optimization - A: High Rotation, B: Medium Rotation, C: Low Rotation - (Left) Pareto Plot shows how High Rotation products are classified as those accounting for 20% of total products but also 80% of sales (point on the curve) - (Right) Shows how the “A” products have been positioned closest to the loading and unloading area."
  },
  {
    "objectID": "qmd/manufacturing.html",
    "href": "qmd/manufacturing.html",
    "title": "9  Manufacturing",
    "section": "",
    "text": "TOC\n\nMisc\nDistributed Manufacturing\n\nMisc\nDistributed Manufacturing (DM) Notes from Online Scheduling Approach for Distributed Additive Manufacturing\n\nGlobal Mass Production vs Global Distributed Production\nDynamica Allocation of Production Orders (POs) - Production order inputs delivery location, spare part to be manufactured, quantity needed - The only thing being locally modeled seems to be production time - Travel Time from PC to Delivery Location is a request from an API - Seems like there should be a cost element here. Freight charges if delivery is outside the company. If manufacturing and delivery is local though, then maybe the only real cost is gas and that would indirectly included in the Travel Time calculation - Queue time should be something that’s monitored and can be looked up in a table - Set-up time should be something that’s considered a constant depending on the spare part\nImplementation - FASTEN is a manufacturing software company - Not sure if this tool is used to feed simulated POs to the pipeline or if it’s used to optimize the objective function or both - Components in this project make sense to me but these arrows don’t all make sense to me\nBenchmark - The null model was one where the PC with shortest distance from PC to delivery location was chosen every time - The metric was Average Wait Time to receive the spare part after being ordered"
  },
  {
    "objectID": "qmd/marketing.html",
    "href": "qmd/marketing.html",
    "title": "10  Marketing",
    "section": "",
    "text": "TOC\n\nSources\nMisc\nTerms\nUse Cases\nMetrics\nConversion Lift Tests\nGeo-experiments\nChannel Attribution Search Engine Marketing (SEM)\nCustomer Segmentation\nCustomer Journey\nWorkflow\n\nSources\n\nhttps://hbr.org/2021/07/why-you-arent-getting-more-from-your-marketing-ai?ab=seriesnav-spotlight\nhttps://hbr.org/2021/07/how-to-design-an-ai-marketing-strategy?utm_source=Data_Elixir&utm_medium=social\n\nMisc\n\nMost marketing AI addresses segmentation, targeting, and budget allocation.\nTools to engage customers: Salesforce, Marketo, Braze, Facebook Ads, or Google Ads - Google and Facebook are perennially at the top of charts for highest return on ad spend - www.singular.net may be useful resource to check from year to year - Example: run a Facebook Ad campaign for these at-risk customers - Manual: Create a SQL query, pull a list from the data warehouse, download it to CSV, and upload it into Facebook Ads or build an integration - Automate: - Reverse ETL - I think this process is described in the “Manual” part (above) - Platforms: Flywheel\nFacebook enables smart mobile user acquisition for mobile brands with 2 methods: - App Event Optimization (AEO) - looks for new-to-you people who are similar to customers you already have in your apps at defined stages - Value Optimization (VO) - looks for people who will spend a certain amount of money in your mobile app\n\nTerms\n\nAction-Based - clicks, impressions, and GRP data\nAdstock - an average decaying function for the carryover effect (see Channel Attribution section)\nAdvanced Mobile Measurement (AMM) - Facebook’s tracker. Allowed advertisers to access click-through conversion data through their MMP\nAffiliate marketing - a type of performance-based marketing in which a business rewards one or more affiliates for each visitor or customer brought by the affiliate’s own marketing efforts.\nCarryover Effect - the lag between the time a consumer is touched by an ad and the time a consumer converts because of the ad\nClick-in - when a user click on something to reach your website/app\nClick-out - the last click a user makes that take them to another website/app\nClearing Price: The final price paid for an impression.\nCPC - Cost Per Click - refers to the cost an advertiser pays each time an online user clicks on his or her digital ad\nCPM or Cost Per Thousand Impressions is the amount advertisers are willing to pay for every one thousand impressions they receive - CPM = (campaign_budget * 1000) / #_of_desired_impressions\neCPM or effective CPM is the predicted revenue earned by a publisher for every one thousand impressions; metric for ad testing - eCPM = (estimated_earnings * 1000) / total_impressions - factors that affect eCPM (for all, more is better) - monthly website traffic - # of ad networks - find ad networks offering better deals for different geographical locations - viewability score - multiple links in this article\nFloor prices are traditionally used by publishers to increase the closing price of their auctions. Think these are implemented by publishers with second-price auctions when they feel reductions are too large.\nGRP -  Gross Rating Point. A standard measure in advertising, it measures advertising impact. You calculate it as a - One GRP is one percent of all potential adult television viewers (or in radio, listeners) in a market Calculatations: - percent of the target market reached multiplied by the exposure frequency. - GRPs (%) = 100 * Impressions (#) ÷ Defined population (#) - GRPs (%) = 100 * Reach (%) × Average frequency (#) - Examples - if you get advertise to 30% of the target market and give them 4 exposures, you would have 120 GRP. - If an average of 12% of the people view each episode of a television program, and an ad is placed on 5 episodes, then the campaign has 12 × 5 = 60 GRPs. - If 50% view three episodes, that’s 150 GRPs\nHeader Bidding: Just the name for technical auctioning process behind bidding on ad space on a website. - Used to be done by waterfall archetecture where ads would pass from one publisher’s website to another until it reached one with a price floor that was below the advertiser’s bidding price\nIdeal customer profile (ICP) - customers who bring in the most long-term value for a company\nIdentifier for Advertisers (IDFA) - a random device identifier assigned by Apple to a user’s device. Advertisers use them to precisely target and track users within apps on iOS devices Impression: An instance of each time your ad is shown on a search result page or other site on the Google Network. (i.e. number of people your ad reaches) - Each time your ad appears on Google or the Google Network, it’s counted as one impression. - In some cases, only a section of your ad may be shown. For example, in Google Maps, we may show only your business name and location or only your business name and the first line of your ad text. - You’ll sometimes see the abbreviation “Impr” in your account showing the number of impressions for your ad.\nInventory is the amount of ad space (or the number of advertisements) that a publisher has available to sell. While the term originated from print, it has grown to encompass ad space on the web and on apps and mobile ads\nPrice Floor: The minimum price a publisher will accept for its inventory-  ignoring all bids below that price. Reduction: Money saved in a second-price auction; difference between the bid price and the clearing price\nRPM is similar to eCPM; it’s the amount earned by publishers per thousand pageviews\nTouchpoint - any time a potential customer or customer comes in contact with your brand–before, during, or after they purchase something from you. Interactions with marketing campaigns and the home page provide rich information about who they are and what they like - Examples: - Before Purchase: Social media, Ratings and reviews, Testimonials, Word of mouth, Community involvement, Advertising, Marketing/PR, visits your website - During Purchase: Store or office, Website, Catalog, Promotions, Staff or sales team, Phone system, Point of sale - Post Purchase: Billing, Transactional emails, Marketing emails, Service and support teams, Online help center, Follow ups, Thank you cards\nUTM - Urchin Traffic Monitor - used to identify marketing channels or results from ad campaigns\n-   Name comes from Urchin Tracker, a web analytics software that served as the base for Google Analytics.\n\n-   e.g. http://yourwebsite.com/your-post-title/?utm\\_source=google\n-   utm code = string after \"?\"\n        -   separate each UTM parameter with the '&' sign.\n-   This person clicked a google ad to get to your site\n-   Google URL builder [tool](https://ga-dev-tools.appspot.com/campaign-url-builder/)\n-   See [article](https://agencyanalytics.com/blog/utm-tracking) for me details, best practices, etc.\n-   Parameter types\n-   utm\\_source - traffic source (e.g. google, facebook, twitter, etc.)\n-   utm\\_medium - type of traffic source (e.g. CPC, email, social, referral, display, etc.)\n-   utm\\_campaign - campaign name, track the performance of a specific campaign\n-   utm_\\__content - In case you have multiple links pointing to the same URL (such as an email with two CTA buttons), this code will help you track which link was clicked (e.g `utm_content=navlink` )\n-   utm\\_term - track which keyword term a website visitor came from. This parameter is specifically used for paid search ads. (e.g. `utm_term=growth+hacking+tactics`)\n\nUse Cases\n\nChatbots for lead development, customer support, and cross-selling or upselling\nInbound call analysis and routing, and customer comment and email analysis, classification, and response\nMarketing campaign automation (including emails, landing page generation, and customer segmentation)\nChannel Attribution - Marketing mix modeling (MMM) - Multi-touch Attribution Modeling (MTA) - Unified Approach Online product merchandising\nPricing Product or service recommendations and highly personalized offers\nProgrammatic digital ad buying ( digital ads are served up almost instantaneously to users)\nSales lead scoring\nSocial-media planning, buying, and execution\nSocial-media sentiment analysis\nTelevision ad placement (partial)\nWeb analytics narrative generation\nWebsite operation and optimization (including testing)\nSales propensity models in customer relationship management (CRM) systems\nAutomated Tasks - send a welcome email to each new customer - chatbots on social media platforms - start with a stand-alone non-customer-facing task-automation app, such as one that guides human service agents who engage with customers. - less-capable bots can irritate customers. It may be better to have such bots assist human agents or advisers rather than interact with customers. - Once companies acquire basic AI skills and an abundance of customer and market data, they can start moving from task automation to machine learning\nApps - Stand-alone applications continue to have their place where integration is difficult or impossible, though there are limits to their benefits. - Example - Using IBM Watson’s natural language processing and Tone Analyzer capabilities (which detect emotions in text), the application delivers several personalized Behr paint-color recommendations that are based on the mood consumers desire for their space. Customers use the app to short-list two or three colors for the room they intend to paint.\nIntegrated ML/DL - Netflix - recommendations are on a separate standalone app where the customer needs to request recommendations - ML that coaches a customer service rep through a call\n\nMetrics\n\nconversion rates, bounce rates, and average basket sizes\nReturn on Ad Spend (ROAS) - ROAS = Sales Revenue / Advertising Budget Spend\n\nConversion Lift Tests\n\nAlso see - Data Science &gt;&gt; Algorithms, Marketing - Data Science &gt;&gt; Business Plots\nRCTs on a customer sample that tries to understand the impact of an ad campaign by randomly showing it to one group of users, with-holding it from another, and looking for a difference in behaviour between the two groups over some predetermined period of time. - Example: - A typical ecommerce brand might want to understand whether their Facebook ads are driving sales that wouldn’t have happened anyway, and so they’d want to track purchases as a KPI in their lift test.\nGold standard of incrementality measurement - Incrementality Tests also help with re-engagement strategies to highlight the optimal day, post-install, to re-engage users and to ensure the highest incremental lift from your marketing efforts\nAlternatives - A/B tests - No way to measure incrementality - The difference seems to be semantics IMO. - With A/B, the control group gets a “different treatment” and with conversion lift tests, the control group gets no treatment - Brand Lift Tests - seek to measure a campaign’s impact on brand metrics, not conversions - Geo-experiments (see below)) - Capable of measuring incrementality. Viable alternative to lift tests\nHow do you not serve ads to an audience (i.e the control group),  yet still “own” the ad real-estate? - Methodologies: - Intent-to-treat (ITT) – this method calculates the experiment results based on the initial treatment assignment and not on the treatment that was eventually received (meaning you mark each user for test/control in advance and do not rely on attribution data. You have the “intent” to treat them with ads / prevent them from seeing ads, but there’s no guarantee it will happen). - Ghost ads/bids – this is another example of a randomly split audience, but this time it is done just before the ad is served. The ad is then withheld from the control group, simulating the process of showing the ad to the user, known as ad serving, without paying for placebo ads. This is a tactic mostly used by advertising networks carrying out their own incrementality tests. - Public service announcements (PSAs) – these are in place to show ads to both the test and control group however, the control group is shown a general PSA while the test group is shown the variant. The behaviors of users in both groups are then compared to calculate incremental lift.\nCost per Incremental Conversion (CPiP) - Metric used to determine whether the treatment’s effect (e.g incremental conversion) is enough to warrant implementing the treatment in production - Higher is worse - Example: - ecommerce brand spent $100k on their campaign. - They measured - 7,500 sales from their campaign’s treatment group (both during and for some fixed time after the campaign) - 5,000 sales from their campaign’s control group - 7500 - 5000 = incremental sales - CPiP = $100,000 / (7500 - 5000) = $40 - If the CPiP &lt; Customer Lifetime Value (CTV)(see Algorithms, Business) and this margin is acceptable at which to acquire customers, then the treatment (e.g. ad campaign) can move to production\nIncremental Return on Advertising Spend (iROAS) = (Treatment Group Revenue - Control Group Revenue) / Treatment Spend - less than 100% you can redistribute budgets to better-performing campaigns and channels - equal to or higher than 100% you know you are not cannibalizing organic traffic and that your ads are effective\n\nGeo-experiments\n\nA quasi-experimental methodology where non-overlapping geographic regions (geos) are randomly assigned to a control or treatment group - Example: ads are served only in the geos of the treatment group while users in geos of the control group won’t be exposed to the advert\nAll the major ad networks/tracking platforms allow for targeting ads to the relevant level of location (neighbourhood, city, state, etc.) - Conversions are measured at a geo-level.\nSteps\n\nDecide geo-level based on market geography\n\ncountry –&gt; states\nstate –&gt; zip codes, Designated Market Area (DMA)\nDon’t select geos that are too small  as people may travel across geo-boundaries and the volumes of conversions may be too low\n\nPerform a preliminary analysis of the geos\n\nDetermine factors that differ between geos that may influence the experiment\nFind geos most similar to each other so the experiment isn’t biased even after randomization\n\nAssignment\n\nRandomly assign treatment and contol to geos\n\nAnalysis\n\nUse difference in differences or synthetic controls to measure incremental conversions\nExample DiD\n\n\n\nChannel Attribution\n\nMisc - Also see UTM parameters in Terms on how to use URLs to track different ad accounts. - When you start tracking the UTM parameters and tying it to the account creation, you can now measure the conversion rate from ad click to purchase. - The main KPI is now end-to-end customer acquisition cost, rather than cost per click. - Model should reflect these characteristics - The concave shape of diminishing returns - As the market becomes saturated by your ads, additional ad spend generates less return - sales ~ ln(spend) and you’d have make sure you change the 0s to a really small number or add 1 , e.g y = ln(x+1). - By adding 1, zeroes will be zeros on the log-scale since log 1 = 0 - Carryover Effect - the lag between the time a consumer is touched by an ad and the time a consumer converts because of the ad. - Adstock is a decaying function that describes the average carryover effect - The advertising effect of this example channel was about 40% in the initial week, then decrease to 20% in the following week, and so on - Not sure how this “effect” is measured. Maybe it means a 40% increase in conversions during the initial week of the ad buy. Maybe this is measured experimentally. Maybe it’s something like a ratio of conversions after clicks on digital ads/paid search to total clicks on the ad. - Calculate a “spend_with_adstock” variable - decay rate is specific to an ad channel and business type - online ads have a more immedicate effect since users are usually ready to buy - offline ads have a longer delay since it takes time to respond - Typically dealing with sparse data - There can be a big spike in spending due to a new launch but no consistent spending afterward. - For certain channels, the budget could be turned off for some time and back on due to market dynamics or business strategy changes. - In some cases, we can pool together smaller locations that are believed to behave similarly to each other and estimate them using a hierarchical Bayesian model (the same logic can be applied to similar channels) - Issues - Self-selection/endogeneity - Example: paid search - a user already has the brand she wants to purchase in mind and searches the brand name online. Then she clicked the paid search ad and made a purchase. This would incur ad spend, however, the purchase would not be incremental sequence of steps (i.e customer journey) because she would have purchased anyway. So the ad didn’t induce the purchase since the user was already ready to purchase before encountering the ad. - Upper funnel channels can get less credit than downstream channels - e.g. a user saw an ad on TV first and wanted to make a purchase online. He then searched for the product on Google and bought it from the paid search ad. The model could attribute more credit to the search if not treated well. - Solutions - Informative priors from reliable experimentation on channels with high selection bias to guide the model and prevent the model from being biased towards endogenous channels. - Instrumental variables to better control for the bias (although instrumental variables may be hard to find or construct) - Selection Bias Correction approach developed by Google (Paper). - Google team used DAGs of the search ad environment and derived a statistically principled method for bias correction - Adjusted for search queries to close backdoor paths. - After the adjustment, the coefficient for the search channel is much less than that from the naive model and is aligned with the result from the experimentations. - Multi-Collinearity - e.g. two channels might be launched together to reinforce each other or to support a product launch - Solutions: variable reduction techniques: PCA, ElasticNet, etc.\nUnified Approach - Use experimentation results as priors for MMM and use MTA results to validate and calibrate MMM - Experiment examples: Conversion Lift Tests, Geo-Experiments, A/B Tests, RCTS, etc. - Use trustworthy priors obtained from experiments or external sources to inform a bayesian model - A bayesian model with time-varying coefficients can handle violatility of marketing effects (particularly for new companies)\nMarketing mix modeling (MMM) - Sales (or ) ~ Channel_Spend (+ Trend + Seasonality(s) + Campaigns + Events + Holidays + Weather +… etc.) - Other outcome variables - KPI or google search volume or website visits or account sign-ups - Other explanatory variables: - media cost for each channel - product pricing - economic indicators - synergistic effect between different media (i.e. interaction) - competitor activities - Better for measuring performance of offline traditional Out-of-Home (OOH) media like TV, radio and print which is unlike online advertising where we have access to metrics like clicks, clickthrough rate, impressions, etc. - Requires relatively large budgets and longer data history to have reliable reads. Sparse data and short data history could make the results biased and unstable - Typically requires at least 2 years of weekly data (longer is better) and a good volume of media spend. - For small data situations, reduce the number of features by combining smaller channels and prioritizing bigger channels - Tends to underestimate upper funnel channels and overestimate lower funnel channels. - Less likely to get more granular level insights, and will not show the nuances of the user journey\nMulti-touch Attribution Modeling (MTA) - Also see - Multi-touch attribution: The fundamental to optimizing customer acquisition (only skimmed, haven’t taken notes yet) - Unlike MMM, it acknowledges that the customer journey is complex and customers may encounter more than one (or even all) of our marketing channel activities in their journey - Requires metrics like clicks, clickthrough rates, impressions, etc. - Therefore, only suited for digital media - MTA data aims to model the entire user ad journey. However, in reality, the real-world data can be partial and only includes part of the touch points due to tracking difficulty. - ** MTA is also subjective to user data privacy initiatives, such as Apple no IDFA and Facebook no AMM. In the foreseeable future, Google will also join the force. Without user-level tracking data, MTA models cannot be built. ** - Most MTA methods utilize click data, not impressions, which tends to give more credit to more clicky channels, like search. This can also make the analysis biased. - Weighting is assigned to these channels based on their touchpoints to determine each channel’s contribution / involvement towards the sale - Weighting methods: - Last click attribution and First click attribution - assigns 100% weighting to the last/first channel that was clicked on - Naive default model used on many analytics platforms - Google switched from this model to “data driven attribution” (see below) in Google Analytics 4 (GA4) - Time Decay - weighting is distributed based on the recency of each channel prior to conversion. The most weighting will be given to the last channel prior to conversion (PPC). The least weighting is assigned to the first channel. - Position-based - the first and last channels each receive 40% of the weighting, with the remainder 20% distributed equally to the middle channels. - See https://support.google.com/analytics/answer/10596866?hl=en#zippy=%2Cin-this-article  for explanations of other rules based models in GA4 - Data-driven attribution (DDA) - weighting is assigned objectively by an algorithm based on the probability of conversion, given the touchpoints. Methods like - Google Search Ads 360 - Markov Chain, game-theory approaches using Shapley values - GA4 - Analyze the available path data to develop conversion rate models for each of your conversion events - Use the conversion rate model predictions as input to an algorithm that attributes conversion credit to ad interactions\nMobile Channel Attribution - Advertising Identifiers - helps mobile marketers attribute ad spend. - Google’s advertising identifier (GAID) - Example: - When a company like Lyft or Kabam runs user acquisition campaigns to gain new mobile customers, a mobile measurement partner like Adjust, Singular, Kochava, or AppsFlyer can help them connect a click on an ad with an eventual app install on a specific device. That helps Lyft know that an ad worked, and that whatever ad network they used for it succeeded. - if the person who installed that app eventually signs up for an account and takes a ride share, Lyft knows where and how to attribute the results of that marketing effort, and connect it to the ad spend that initiated it. Even better, from Lyft’s perspective, it can use the IDFA to tell mobile ad networks essentially: I like users like this; go find me more. - Apple’s SKAdNetwork[](./_resources/Marketing.resources/https-3A-2F-2Fspecials-images.jpg%3Ffit%3Dscale]] - privacy-safe framework for mobile attribution - allows advertisers to know which ads resulted in desired actions without revealing which specific devices — or which specific people — took those desired actions. - You go to an ad network like Vungle or AdColony or Chartboost — or Facebook or Google, for that matter — and kick off an ad campaign. They show your ads to potential new mobile customers, and when one clicks on it and downloads your app from the App Store, Apple itself will handle sending a cryptographically signed notification — a postback — to the ad network. That postback will not include any user or device-specific information, so while it will validate the conversion for marketing purposes, it won’t reveal any personal information of your new app user. - Can’t be used in Conversion Lift Tests - postbacks require a ad campaign ID for the conversion to be attributed to, so it’s not possible to measure the conversion volume from a lift test’s control group (since the control group isn’t exposed to the ad)\n\nSearch Engine Marketing (SEM)\n\nmarketing on search engines where you pay to place ads at the top so that the user sees your ads before the organic search results (e.g. Google, Bing, etc.)\nMost SEM ads work on an auction system, where you have to place a bid on each of the search terms (can be millions of terms) relevant to your business and if you win a position in the auction(there are usually 2–3 SEM ads per search, or more based on your region and the search engine you are using), then you would pay the cost equal to your bid or lower depending on which auction system the search engine follows.\nAuctions - Types - First price auction: A model wherein the buyer pays exactly the price they’ve bid on any given advertising impression. (greater transparency) - Second-price auctions: A model wherein the buyer pays $0.01 more than the second highest bid for an ad impression. - Example (3 bidders) - Bids - Bidder A $2.20 - Bidder B $2.80 - Bidder C $2.50 first-price auction: B wins\n        clearing price will be the same as the bid- $2.80\n\n    second price auction:\n        B wins\n\n        clearing price = $0.01 + second-highest bid ($2.50) = $2.51\n\n        reduction = $2.80 (winning bid) - $2.51 (clearing price) = $0.29\nbid shading technique buyers use in first-price auctions in an attempt to avoid overpaying\ntakes a maximum possible bid and tries to forecast the market value for a given impression, in order to determine the actual bid price to submit.\n    outcome: bid price; model features: site, ad size, exchange and competitive dynamics\n\n    if win rates decrease, the algorithm raises the price they pay\n\nSome publishers (google, bing, etc.) use intelligent price floors to counteract bid shading\nOptimize bidding for value vs traffic volume characteristics of key words - sparsity issues (i.e. zero-inflation) - Due to the long tail nature of the search terms (some search terms are more popular than some other obscure ones) - Due to the conversion ratio itself — not every click from these SEM ads converts to a purchase - What should the outcome variable be? (binary: conversion/no_conversion or numeric: bid value) - Other considerations - How does the effect of a competitor’s bid change based on the type of auctioning method? - Seasonality? - Pre-corona vs post-corona? - Is there an optimal level of investment?\n\nCustomer Segmentation\n\nAlso see - Customer Journey (below) - Algorithms, Product &gt;&gt; Customer Journey\nSeeks to answer: - Who are the most valuable customers? - Where do they come from? - What do they look like? - What and how do they like to buy?\nThe company can achieve a higher return on ad spend with a smaller marketing budget, yielding a higher profit margin and more room for market expansion.\nIncreases marketing efficiency by helping to indicate which campaigns are more likely to succeed with certain groups of customers - Form marketing hypotheses based on cluster characteristics and test these hypotheses by varying campaigns based on the customer’s cluster membership. - recommend a product they’re likely to purchase, a multi-buy discount, or on-boarding them on a loyalty scheme (e.g. rewards program) - Once you know who your customers are and what their value is to your business, you can: - Personalize your products and services to better suit your customers’ needs - Create Communication Strategies tailored to each segment - Focus Customer Acquisition to more profitable customers with messages and offers more likely to resonate with them - Apply Price Optimization to match customer individual price sensitivity - Increase Customer Retention by offering discounts to customers that haven’t purchased in a long time - Enhance Customer Engagement by informing them about new products that are more relevant to them - Improve your chance to Cross-sell and Up-sell other products and services by reaching out for the right segment when they’re more likely to respond - Test which type of incentive a certain segment is more likely to respond to (e.g. pricing discounts, loyalty programs, product recommendation, etc.)\nRFM Analysis (Recency, Frequency, and Monetary Value) - Just need a transactional database with client’s orders over time (at least 2.5 to 3 yrs of data to capture enough behavior variation). - time frame could be monthly or yearly or any other time frame required by the business - Explicitly creates sub-groups based on how much each customer is contributing. - Recency – How recently did the customer purchase? - Invoice date can be used to calculate the recency - number of days since the last purchase - Frequency – How often do they purchase? - Invoice numbers can be used to calculate the frequency - number of invoices per month - Monetary Value – How much do they spend? - Total price can be used to calculate the monetary value. - total monetary value of all purchases in a month - Modelling - Also see - Algorithms, Marketing &gt;&gt; Customer Lifetime Value &gt;&gt; BG/NBD Gamma-Gamma model - Video: Bizsci - lab-49-feature-engineering-customer-segmentation - lab-58-customer-lifetime-value-rfm-calc - python customer lifetime value, rfm + xgboost (youtube) - Clustering (k-means is popular) - Cluster Customer IDs based on Recency, Frequency, and Monetary Value variables - Add additional variables if relevant to helping you choose a market strategy - If you end up with too many variables for the amount of data you have, then perform PCA before clustering - Interpret clusters and create strategy for each cluster - Example - Charts - Both charts have the same info, right shows the patterns better but you can’t read the values - RFM variables were standardized between 0 and 100 before clustering - Cluster 0 - Interpretation — Customers who have not made purchases recently - Strategy — Make offers to bring them back to purchasing Cluster 1 - Interpretation — Customers who have a high monitory value - Strategy — Create a loyalty program so that they can continue spending more Cluster 2 - Interpretation — Customers who have not made purchases recently - Strategy — Make offers to bring them back to purchasing Cluster 3 - Interpretation — Customers who are likely to churn - Strategy — Retain them with exciting offers Cluster 4 - Interpretation — Regular customers - Strategy — Create a loyalty program to keep them purchasing on a regular basis\nBehavioral Segmentation - Tries to understand how customers interact with your product or website - Misc - Also see - Customer Journey - Product Development &gt;&gt; Behavioral Data - Data - Demographics: location, age, gender, and income - Touchpoints (see Terms): view-through, click-through, the time between each action, etc - If a user sees an ad on Instagram and quickly swipes over it, that probably indicates they are not very interested in it. - Often impacted by advertisers’ programmatic push notifications - Company Website Navigation: visiting behaviors provide enlightening ideas about what a visitor is truly looking for and how they like to shop (See Customer Journey) - Purchasing: what a user buys, their purchasing frequency and timing, purchasing price and discounts, etc. - Social media network: Is there an influencer within a customer’s network? (see bkmks &gt;&gt; Network Analysis) - Example: Telecommunications - Cluster the data and interpret the clusters - Example using the telecommunications data above The green cluster (aka segment) is Digitally engaged customers. A market strategy could be to create a digital loyalty card and reward them based on the use of digital services. This will in turn also increase revenue for the company.\n        The blue cluster is moderately engaged with low tenure.\n            A market strategy could be to offer discounts and convert them into long-term contracts.\n\n        The red cluster is basic customers with only phone service.\n            A market strategy could be to educate them about the advantages of digital services and then upsell digital products.\n\nCustomer Journey \n\nTop Figure: Typical SaaS User Journey\nBottom Figure: General User Journey - Area of the rectangle represents the amount of customers that haven’t dropped off and remain the funnel - The red arrow represents customers that drop off and don’t convert - The start of the arrow show the section of the rectangle that represents the amount customer that drop off from on point to the next.\nMisc - Also see Algorithms, Product &gt;&gt; Customer Journey\nPre Sale - When potential customers are in the “consideration” phase and researching a product, AI will target ads at them and can help guide their search - determine which customers are most likely to be persuadable and, on the basis of their browsing histories, choose products to show them - Use extremely detailed data on individuals, including real-time geolocation data, to create highly personalized product or service offers\nDuring Sale - upselling and cross-selling and can reduce the likelihood that customers will abandon their digital shopping carts - after a customer fills a cart, AI bots can provide a motivating testimonial to help close the sale—such as “Great purchase! James from Vermont bought the same mattress.”\nPost sale - AI-enabled service agents (chatbots) triage customers’ requests—and are able to deal with fluctuating volumes of service requests better than human agents are - can handle simple queries about, say, delivery time or scheduling an appointment and can escalate more-complex issues to a human agent\n\nWorkflow\n\nFind the Ideal Customer Profile (ICP) and target them (article) - Customer’s long-term value = Revenue from the customer’s repeated purchase — customer’s acquisition cost - Segment in Negative, Low-Value, Medium-Value, and High Value cohorts - See Customer Segmentation &gt;&gt; RFM Analysis - Alternatively, based on its sales and marketing attribution systems, the brand can calculate one-year revenue and acquisition costs for each customer (or visitor that doesn’t convert). With that, the brand quickly gets each customer’s value and decides who is worth its marketing budget. - The split between medium-value segment and high-value segment typically follows the 80/20 rule - The High-Value segment can be further segmented into three smaller cohorts using pca or clustering algorithms. - See Customer Segmentation &gt;&gt; Behaviorial Segmentation - The customer data used to cluster can include: interactions with marketing campaigns, browsing history, purchase history, records of using coupons, etc. - Domain knowledge is usually required to meaningfully label these cohorts and develop marketing plans based on the cluster attributes - Examples - Cohort 1 likes to buy the latest model, cohort 2 prefers to buy with discounts, and cohort 3 often makes purchases as gifts - Cohort 1 reacts positively to campaigns about cool geeky features. On the other hand, cohort 2 engages more with campaigns spotlighting the practical benefits of using a new gadget. - Cohort 1 trusts YouTubers who talk about the latest and greatest tech products and make purchases after watching unboxing videos. Cohort 2 likes to shop at Costco and buy gadgets at a discount - Partnering with tech influencers helps the company attract cohort 1 while working with wholesale stores speeds up selling to cohort 2. - Use experimentation on High-Value cohorts to test which acquisition/marketing campaigns have high ROAS. - For cohort 3, campaign A seems to have more success than campaign B - Future campaigns can now target high value cohorts with greater conversion and less expenditure.\nPotential Pitfalls - Make sure the metric fits the business question. - Example: Churn - Also see Algorithms, Marketing &gt;&gt; Churn - Don’t model who was most likely to leave, they should have asked who could best be persuaded to stay—in other words, which customers considering jumping ship would be most likely to respond to a promotion. - i.e. swing customers - like politicians looking for swing voters because they are persuadable. - If you model the wrong objective, you can squander money on swaths of customers who were going to defect anyway and underinvest in customers they should have doubled down on. - Think this has to be 2 stages - Filter data before promotions over some window –&gt; model traditional churn –&gt; filter data after promotion –&gt; label which likely churns left and which ones didn’t –&gt; model churn with new churn labels using data after promotion because you want probability of churn given promotion - So you’d have 2 models: 1 to identify churners and 1 to identify swing customers from churners - Does a lift model help here? - Example: Increasing user spend for a video game - Finding features that increase time spent playing doesn’t efficiently raise revenue or maybe not at all if most of your players play for free. - e.g. Increasing my time spent playing FOE doesn’t increase FOE’s revenue since I don’t pay for shit on that game. - Guessing there’s a decent correlation between weekly total_time_spent_playing and overall revenue and they thought they could increase revenue by increasing total time_spent_playing. They should have been looking at correlations between features and spend_per_customer. - Maybe just filter top spenders and see which features they prefer (i.e. correlated to) - Make sure when you increase the accuracy of a model, it doesn’t hurt the bottom line due to a bad tradeoff - Consider the consumer goods company whose data scientists proudly announced that they’d increased the accuracy of a new sales-volume forecasting system, reducing the error rate from 25% to 17%. Unfortunately, in improving the system’s overall accuracy, they increased its precision with low-margin products while reducing its accuracy with high-margin products. Because the cost of underestimating demand for the high-margin offerings substantially outweighed the value of correctly forecasting demand for the low-margin ones, profits fell when the company implemented the new, “more accurate” system. - Determine the optimal granularity of predictions - Consider a marketing team deciding how to allocate its ad dollars on keyword searches on Google and Amazon. The data science team’s current AI can predict the lifetime value of customers acquired through those channels. However, the marketers might get a higher return on ad dollars by using more-granular predictions about customer lifetime value per keyword per channel.\nRefine the Business Question - When defining the problem, managers should get down to what we call the atomic level—the most granular level at which it’s possible to make a decision or undertake an intervention. - A good business question captures the full impact of the decision on the Profit and Loss (P&L), recognizes any trade-offs, and spells out what a meaningful improvement might look like. - Gradation from an extremely vague question to a more finely grained question 1. “How do we reduce churn?” 2. “How can we best allocate our budget for retention promotions to reduce churn?” - Has the retention budget been set, or is that something we need to decide? - What do we mean by “allocate”? - Are we allocating across different retention campaigns? 3. “Given a budget of $x million, which customers should we target with a retention campaign?”\nDefine Missed Opportunity metrics - Identify sources of waste and missed opportunities - waste example: targeting a likely-to-churn customer with a promotion that is not persuadable - missed opportunity example: not targeting a likely-to-churn customer with a promotion that is persuadable - Compare the distribution of success versus failure to quantify waste and missed opportunities. - Measure - For difficult to quantify situations use aggregate data - Example of an approach for churn model example using aggregated data - customers who received promotion: what’s the cost of the promotion incentive relative to the incremental lifetime value - difference or ratio? - customers who didn’t receive promotion: what’s the lost profit associated with the nonrenewal of their contracts\nDetermine the cause of waste and missed opportunities - In an ideal world, what knowledge would you have that would fully eliminate waste and missed opportunities? Is your current prediction a good proxy for that? - In churn example, rather than the basic churn/no churn model, focusing on persuadability would have led to great improvements. - Does the output of your AI fully align with the business objective? - Churn example: A persuadable user with low expected profitability should have a lower priority than a persuadable user with high expected profitability. - additional factor that further optimizes the solution to the refined business question - How much are we deviating from the business results we want, given that the AI’s output isn’t completely accurate? - Quantifying the errors of your model - Churn example: the cost of sending a retention promotion to a nonpersuadable customer (waste) is lower than the cost of losing a high-value customer who could have been persuaded by the offer (missed opportunity). Therefore, the company will be more profitable if its AI system focuses on not missing persuadable customers, even if that increases the risk of falsely identifying some customers as being receptive to the retention offer."
  },
  {
    "objectID": "qmd/meteorology.html",
    "href": "qmd/meteorology.html",
    "title": "11  Meteorology",
    "section": "",
    "text": "TOC\nMisc\nExample: Extreme hot weather events in Western Europe-Atlantic region (video, around ~1:07:03)\n\nJonathan Koh: pre-print available in June or July 2023\nData: “ERA5 reanalysis data” - Monthly - Training: 1979 - 2020 - Validation: 2020 - 2022 (simple train-test split?)\nGoals: - predict probability of an event (i.e. binary classification), intensity of event (ie regression), spatial dependence of events (i.e. spatial correlation) - identify weather patterns driving extremes (i.e. predictors) - global thermodynamic (i.e.. climate change) - local land surface: soil moisture or snow cover determin surface energy budget (i.e. wet soil buffers heat) - regional dynamic conditions: diabatic (clear skies), adiabatic warming (subsidence) or advection of warm air from anti-cyclonic circulation (atmospheric blocking)\nModel - Response: - Extreme 2m daily temp anomalies over land - Predictors - atmospheric blocking measured by variable, “Z500” (5-day avg) - soil moisture, “SM” (15-day avg) - Resolution - There’s a predictor for each 5.6 x 5.6 degree (lat-lon) grid points - Shows the Western Europe-Atlantic region (disregard the horizontal black line) - There’s a response variable for each 1.3 x 0.3 degree (lat-lon) grid points - Study region, larger box, is a smaller subregion within the Western Europe-Atlantic region, and the Risk area is the smaller box - The response variable measurements are for the grid points in the study region - Model - Model has 3 components - Exceedence Probability is fit first. Then, its probabilities are used in the Marginal Extermal Intensity model. Then, both results are used to model the Spatial Dependence - Exceedence Probability: indicator response for whether temp has exceeded the threshold. - fits a boosted tree with mean log loss - Marginal Extremal Intensity: uses a univariate loss function discussed at the beginning of the talk - ksi (shape?) is a hyperparameter along number of trees, m. - Spatial Dependence - Paper doesn’t come out until June or July, so will have to wait to see if there’s a package and to get further details - Feature importances will be the Z500 and SM for each of the geographical location variables. These feature importances can used to figure out weather patterns in those areas that were important to occurance of the high temperature event in the risk area."
  },
  {
    "objectID": "qmd/nonprofits.html",
    "href": "qmd/nonprofits.html",
    "title": "12  Nonprofits",
    "section": "",
    "text": "Notes from:\n\nData Strategy for Non-Profits: Why?\n\nWhy they need a data strategy\n\nidentify opportunities, inform decision makers on how to allocate scarce resources, and to measure and communicate impact\nProviding greater transparency and accountability to funders. - The availability of data has driven donors and grantmakers to be more conscious on the measurement of the impact that their dollars have. Non-profits which can better demonstrate their impact receive more financial support.\nGrowing impact of programs and services. - Data is utilized internally to improve services and expand impact which results in being able to generate greater impact.\n\nDeveloping a data strategy\n\nAssessing the following will help lay a foundation for a data strategy: - Mission and Theory of Change - a concrete outline which states - the impact that will be generated - the conditions needed to generate the impact - the programs in place to create those conditions. - Each piece of the Theory of Change can then be stated in terms of a quantifiable measure of success which will serve as the starting point for developing a data strategy. - Example - Stakeholders - Answer these questions: - Who are your stakeholders? - Identify subgroups and individuals who fall into these groups - Donors and Volunteers. - Management and Employees. - Beneficiaries. - What questions do stakeholders have that can be answered through data? - donors and volunteers - example: a data-driven Impact Report which provides a holistic view of how the nonprofit utilizes their resources to achieve its mission. - Contains anectdotal stories with data that demonstrate how effectively their resources are being used - Management and employees - example: more granular views on how individual programs and initiatives are performing on metrics related to their Theory of Change - Beneficiaries - example: data around how projects in different sectors are performing - How will the data affect stakeholder decision making? - donors and volunteers - can influence decisions around donating time and money - management and employees - provides visibility into how resources are allocated internally and empowers internal decision makers to evaluate how to get the most impact out of the limited resources they have - beneficiaries - can be used to garner buy in and allow the nonprofit access to communities that they would otherwise not have - Data Gap Analysis - Identify gaps between current data capabilities and those needed to answer all stakeholder questions - Contents - Outline all data needs in the form of questions derived from your Theory of Change and stakeholder analysis. - Deep dive into the required data to answer the questions and an estimate of how much that data would cost to obtain. Don’t forget that the same data could answer multiple questions. - Identify data that has already been collected and any existing efforts to collect additional data. - Connect existing data and data efforts to questions and determine gaps between questions and data. - Propose strategies to bridge data gaps and sustain data assets. Evaluate both the benefits of answering the question and costs of acquiring the data. - Prioritize data gaps to close. - Communicate findings to relevant stakeholders.\n\nIdentify key questions\n\nShould be designed to be measurable, clear, and actionable (Also see nonprofit example in  Project Management &gt;&gt; General Steps to Starting a Project)\nMake sure that you are addressing the social outcome and not program output of your organization. - just answering the question of how many people go through your program, how many volunteers you recruit / retain, are not sufficient because they focus only on outputs — but not social impact. - You would still need to assess whether those outputs are truly feeding into the social outcomes you want to accomplish through your organization.\nThe answers to key data questions should be resolved by a “North Star metric” or the single, defining outcome metric that best captures the core value that your nonprofit delivers. - Chasing non-critical metrics can strain resource constraints of the budget and time constraints of staff.\nExamples - Doctors Without Borders: Are we delivering adequate medical aid to people affected by crises (e.g., conflicts, epidemics, disasters) or exclusion from healthcare? - American Red Cross: How many lives have been saved from the blood donated by our donors? - Feeding America: How many people have we been able to provide food for this year? - BUILD: Are we improving the academic and professional outcomes of our students?\n\nOutputs vs Outcomes\n\nLogic model - It focuses on how inputs and activities translate to outputs and eventually\nExample"
  },
  {
    "objectID": "qmd/politics.html",
    "href": "qmd/politics.html",
    "title": "13  Politics",
    "section": "",
    "text": "Ethan Epping talk\n\nMost common modeling are for “support” (who they vote for) and “turn-out” (whether they vote at all) responses\nmessages around civic and community engagement is better at encouraging low propensity voters than pushing your candidate’s policy agenda\nturnout model - features: previous vote history of an individual; feature that influence turnout in general - ncoa data (national change of address) - voter file info can be outdated - when people move, nobody calls to cancel their voter registration - geocoding - commercially available - mobile phone data - Experion - govt - gun and boat registration - college student lists at public universities - data collected through contact by the campaign\nsupport model - features: survey data + other data\nDNC uses BigQuery/SQL for a db and analytics, python for engineering; datascience does some R but more python\nvoter file - list of registered voters in every state - publicly available - states have various restrictions - OH and NC publish online and is free - some cost thousands and only available to campaigns - not available for commercial use though - includes everything that’s on voter registration - addresses. phone numbers, etc. - vote history (participation) - If you don’t vote in 2 straight federal elections you are usually eligible to be removed from the registry\n“arbor” (sp?) tool - for registration drives - at census block or precinct level, take census estimate of eligible voters and subtract the number of registered voters - helps to figure out where to prioritize efforts (i.e. where the biggest pool of unregistered, eligible voters are)\n“election base” tool - de-aggregate results down to the census block level\nresolution is usually at the census block level because precinct boundaries often change\nEarly voting data - Want to find voters that have requested a ballot but have yet to mail it in - best time to talk to them - rejection data: talk to voters who incorrectly filled out a ballot - voter needs to talk to county clerk - or show up on voting day\nJobs - https://www.progressivedatajobs.org/ - guide to application process: https://guide.progressivedatajobs.org/ - https://www.digidems.com/ - takes tech people and gives some political training then deploys to campaigns"
  },
  {
    "objectID": "qmd/product-development.html",
    "href": "qmd/product-development.html",
    "title": "14  Product Development",
    "section": "",
    "text": "15 Why do people leave and stay?\nFeedback\nMetrics"
  },
  {
    "objectID": "qmd/product-development.html#toc",
    "href": "qmd/product-development.html#toc",
    "title": "14  Product Development",
    "section": "14.1 TOC",
    "text": "14.1 TOC\n\nMisc\nTerms\nBehavioral Data\nConversion Funnels\nStages of Product Development\nWhy do people leave and stay?\nFeedback\nMetrics - Success - Guardrail - Funnel - Input-Output - Growth - Platform"
  },
  {
    "objectID": "qmd/product-development.html#misc",
    "href": "qmd/product-development.html#misc",
    "title": "14  Product Development",
    "section": "14.2 Misc",
    "text": "14.2 Misc\n\nAlso see Decison Theory &gt;&gt; Conjoint Analysis for surveying consumers about product attributes to calculate attribute utilities and probability of purchase\nFlaws of Purchase-Funnel Based Attribution Metrics - Notes from The Causal Analysis of Cannibalization in Online Products - Users usually take more complicated journeys than a heuristically-defined purchase-funnel can capture. - Examples - If the recommendations make users stay longer on Etsy, and users click listings on other pages and modules to make purchases, then the recommendation-attributed metrics fail to capture the contribution of the recommendations to these conversions.  The purchase-funnel is based on “click”, and there is no way to incorporate “dwell time” to the purchase-funnel. - Suppose the true user journey is “click A in recommendation → search A → click A in search results → click A in many other places → purchase A”.  Shall the conversion be attributed to recommendation or search? Shall all the visited pages and modules share the credit of this conversion? Any answer would be too heuristic to be convincing. - Violation of causal experiment assumptions (ignorability assumption) - segments of users who follow the pattern of purchase-funnel may not be comparable between treatment and control groups, because the segmentation criterion (i.e., user journey) happens after random assignment and thus the segments of users are not randomized between the two groups. - Solution involves causal mediation analysis (also see Causal Inference &gt;&gt; Mediation Analysis)\nDistribution of effect sizes for webpage variations using Optimizely (platform for A/B testing) data - Median (and average) webpage variations have roughly zero effect on webpage Engagement - Thread summarizing paper - 70% of effects will not show any impact on Engagement compared to a baseline - alpha=0.05 yield an FDR of 18%-25% (i.e. much higher than alpha) - Recommends only testing sizeable changes as these are the type of changes that yield the largest effects and fewer false discoveries (i.e. the tails of the effect size distribution) - See paper for other tests and recommendations\nLoyalty measure by app category\nTrust Thermocline - Point at which the number of customers that use your product suddenly drops off a cliff (3 nested Threads) - Breaching this point typically occurs when a company continually changes the service, raises prices year after year but doesn’t notice a major change in customer subscriptions/engagement. Then customers start abruptly dropping the product. - Most of the time the company cannot regain the customers after this point and even if they do they will never reach heights they were at previously. - If you’re a relatively large company you should have customer retention department/team/process in place - Signals to watch out for - Watch for grumbling and LISTEN to it. - Don’t assume that because people have swallowed a price or service change that they’ll swallow another one. - Treat user trust as a finite asset. Because it is. - Ground-level customer-facing people will be the ones receiving these signals - Maintaining your customer base - Get your customer retention people in a room with a white board and list all the issues they CONSTANTLY hear from customers but have stopped bothering to report up the chain. - However painful it is to your bottom line. However politically tough it is. However complex the problem. These are the things that need to be fixed.\n\nTerms\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. Low bounce rate can indicate the landing page needs improvement\nProduct Listing Page (PLP) -  webpage that lists all your products or a category of your products\nProduct Details Page (PDP) - webpage for a specific product that shows in-depth information\n\nBehavioral Data\n\nBehavioral data serves two main purposes for teams — understanding how the product is being used or not used (user behavior) and building personalized customer experiences across various touchpoints to influence user behavior. - Launching new features without instrumenting them (e.g. tracking) beforehand takes away the opportunity to analyze how those features are used (if at all) and to trigger in-app experiences or messages when relevant events take place (or don’t).\nMisc - Notes from How to Collect Behavioral Data - Also see Marketing &gt;&gt; Customer Segmentation &gt;&gt; Behaviorial Segmentation\nSources - Primary: web app, mobile apps, a smart device, or a combination — powered by proprietary code - Secondary: all external or third-party tools that your customers interact with directly or indirectly - e.g. tools used for authentication, payments, in-app experiences, support, feedback, engagement, and advertising - Auth0 for authentication, Stripe for payments, and AppCues for in-app experiences - Opening a support ticket via Zendesk, leaving feedback via Typeform, opening an email sent via Intercom, or engaging with an ad on Facebook - To collect data from secondary sources, you can either use source integrations offered by data collection tools or write your own code\nExtract Data - Using vendors are the best option. Maintenance and troubleshooting are not trivial for homemade solutions even for experienced engineers. - CDI or ELT? - CDI is best-in-class to collect behavioral data from primary or first-party data sources — web and mobile apps, and IoT devices - ELT is best-in-class to collect all types of data including behavioral data from secondary data sources — third-party tools that power various customer experiences. - Customer Data Infrastructure (CDI) - Characteristics - Purpose-built to collect behavioral data from primary or first-party data sources but some solutions also support a handful of secondary data sources (third-party tools). - Data is typically synced to a cloud data warehouse like Snowflake, BigQuery, or Redshift, but most CDI solutions have the ability to sync data to third-party tools as well. - All CDI vendors offer a variety of data collection SDKs and APIs - Some CDI solutions store a copy of the data, some make it optional, and some don’t. - CDI Vendors - Segment Connections - supports data warehouses and a host of third-party tools as destinations, as well as store a copy of your data that can be accessed later if needed. - mParticle - offers CDI capabilities along with identity resolution in its Standard edition whereas audience building is available on the Premium plan - Also supports data warehouses and a host of third-party tools as destinations, as well as store a copy of your data that can be accessed later if needed. - RudderStack Event Stream and Jitsu - Open Source; support warehouses and third-party tools but RudderStack offers a more extensive catalog of destinations - Snowplow - open-source and unlike the others, Snowplow doesn’t support third-party tools as it is focused on warehouses and a few open source projects as destinations. - Freshpaint that offers codeless or implicit tracking - MetaRouter which is a server-side CDI that only runs in a private cloud instance - ELT tools - These provide more comprehensive source integrations than CDIs can - Airbyte - Open-source - Offers source connectors with 150+ tools like Zendesk, Intercom, Stripe, Typeform, and Facebook Ads, many of which generate event data - Offers a Connector Development Kit (CDK) that you can use to build integrations that are maintained by Airbyte’s community members - Other (all open source): Fivetran, Stitch, and Meltano\nProcess Data - Notes from The Modern Customer Data Stack - Identity Resolution: Identifying the same users in different data sources - Identify match keys: Determine which fields or columns you’ll be using to determine which individuals are the same individual within and across sources. - Example: email address and last name. - Aggregate Customer Records: Create a source lookup table that has all the customer records from your source tables. - Match & Assign a Customer ID: Take records that have the same (or in some cases, similar) match keys and generate a unique customer identifier for that matching group of customer records. Every customer id that is generated can be used to link the customer sources together going forward. - Roll in more sources: As you get more sources you can start rolling them into the same process by setting the correct rules and precedence for the source. - Master Data Models: Creating a final/clean view of your customers and associated facts and dimensions. - Start with a “Customer → Transaction → Event” framework (more details) - Customers: Create a table of your customers with the ability to quickly add new fields - Transactions: Join key from customers table to their transaction history - Events: Any events you track for each customer - Other business types may have other tables - double-sided marketplace - tables for both sellers and buyers as different entities. - B2B business - separate accounts and contacts entities\nAnalysis Tools - General characteristics - Offer SDKs and APIs to collect data from your primary (first-party) data sources - using purpose-built data collection tools (CDI and ELT) is more efficient and prevents vendor lock-in - Store a copy of your data and allow you to export the data (usually for an additional fee) - Amplitude, Mixpanel - Can be integrated with Airbyte - Indicative, Heap - PostHog (open-source) - Can be integrated with Airbyte\n\nConversion Funnels \n\nFigure: Typical eCommerce Funnel - Funnel moves down and users drop off until only 3% of users (“% users”) reach Transaction (aka conversion) - “CR” is the conversion rate at each point in the funnel\nProvide you with quantitative data about the number of customers that churn at each stage of the funnel. While this information is already valuable, paired up with qualitative data, it gives you all the insights you need to retain more customers.\nMisc - Also see - Marketing &gt;&gt; Customer Journey - Algorithms, Product &gt;&gt; Customer Journey, Conversion Funnel - Typical e-commerce user conversion rate can be benchmarked at around 2.5–3% in a regular business as usual time.\nBenefits - Product Managers: - Useful for new feature launches. By grouping your sessions by user or device properties, you can compare the conversions between different user cohorts. - Is the new feature sticking or not? Are your users struggling with it? Are they simply not interested? Looking at the numbers is one thing, but try getting deeper by watching session replays. Now you see what went wrong exactly. - Marketers: - Break down conversion numbers according to the different acquisition channels and figure out where your most valuable users are coming from. This way, you can focus your efforts on the more relevant channels.\nSignals - The more rare an event is that’s high in an e-commerce funnel (i.e more towards Home Page), the more weight it carries in terms of purchase signaling. - If a user has entered the bottom of the funnel (i.e. more towards Begin Checkout) and simply dropped off, it strong reason to reach out in attempts to facilitate or promote movement down the funnel.\nData - Funnel data at the user-level over a time period - The columns are in sequence according the conversion funnel (i.e. “Home” is the beginning of the funnel and “Purchase” is the end.) - Values are counts of viewing events for webpages in the conversion funnel - Values for Home must be an indicator of whether they have visited or started on the Home Page. - Interpretation - User A - Looks similar to a window shopper, that is engaged enough — 50% of their PLP views turn to PDP views. - They have not added anything to a cart but they may have had something there from previous sessions — which is indicated by 1 cart view. - User B - Likely a customer who is actively trying to make a choice. They may be preparing their cart for a transaction but have not started checking out yet. - User C - Went way deeper into PLP browsing. - Shows signs of being ready to commit to a purchase and even started checking out once. However, they did not complete a transaction. - Possibly, they dropped off in search for coupon codes or better deals elsewhere. - Maybe, based on the high PLP view count, they were deep into search but did not manage to find the products of interest. - User D - Probably knew what they wanted, which is - Indicated by a relatively high ratio from PLP to PDP views and high PDP views to Add to Cart ratio. - They viewed their cart multiple times, reviewing it. But somehow, they have not started the checkout. - This could be a perfect candidate for the abandoned cart campaign. - User E - Probably a returning customer who come back shortly after another session. - Signal Scoring Steps - Choose a timeframe - Depends on the business model and the action you’re expecting to take with it. - Examples - Users are taking up to around 1 month to consider a purchase, then update segments on a 30-day rolling basis. - You want to communicate with your customers daily, then daily morning updates could be something to consider. - Individual Signals: events that are positively associated with the conversion (e.g. the number of daily PLP views, PDP views, add to carts, etc.) - Understanding how a user scores in each one of these signals can help identify which part of the conversion funnel was not covered by a user. - These scores can inform how the business should interact with that customer - For each signal in terms of activity (i.e. counts of events), segment customers into 3 quantiles (&lt; Q33, Q33 &lt; Q66, &gt;Q66) with labels below average, average, above average customers - Can also label customers who did not have any events for a signal at all (0-score users)"
  },
  {
    "objectID": "qmd/product-development.html#stages-of-product-development",
    "href": "qmd/product-development.html#stages-of-product-development",
    "title": "14  Product Development",
    "section": "14.3 Stages of Product Development",
    "text": "14.3 Stages of Product Development\nStage 1: Coming up with initial product ideas.\n\nStage 2: Selecting ideas:\n    -   Quantitative analysis to select a subset of ideas to which to devote resources, often referred to as opportunity sizing.\nStage 3: Experiment design:\n    -   Involved with selecting success and guardrail metrics, running sanity checks, choosing randomization units, etc.\n    -   Candidates will need to consider alternatives when it is not possible to run A/B tests.\nStage 4: Making a launch decision:\n    -   Making scientific decisions based on experimentation results.\n    -   Diagnosing problems and evaluating tradeoffs.\nThroughout the product development lifecycle, working with the appropriate **metrics** is of paramount importance"
  },
  {
    "objectID": "qmd/real-estate.html",
    "href": "qmd/real-estate.html",
    "title": "15  Real Estate",
    "section": "",
    "text": "TOC\n\nMisc\nBusiness Questions\nUse Cases\nAppraisal Methods\nFeatures\n\nMisc\n\nListing Price - Affects the final selling price, how long the home spends on the market, the volume of interest in the house, and anchors price negotiations with buyers - Algorithmic estimates have better performances when they take the list price into account\nA home has a value distribution as different potential buyers place different values on the various home features. The eventual selling price is a function of this value distribution and the specific individuals who consider the home.\nPrice/Income ratio - Example: Median house price in middle class suburbia in a very affordable region is $323,000. To maintain an price/income ratio of 3, that requires a household income of $107,000. In 2019, regional median HH income was $62,502. ~30% of HH make over $100K.\n\nTerms\n\nPrice Anchoring - a pricing strategy that plays on buyers’ inherent tendency to rely heavily on a piece of initial information to guide subsequent decisions. In the context of pricing, many businesses will set a visible initial price for a product but make a point of showing that it’s now being sold at a discount.\nPrice Index - a normalized average (typically a weighted average) of price relatives for a given class of goods or services in a given region, during a given interval of time - Case-Shiller U.S. National Home Price Index (also {fredr}) - Wiki has formulas - See Appraisal Methods &gt;&gt; Prediction Adjustments &gt;&gt; Repeat Sales Model - If a price index rises 10%, it means the average level of prices has risen 10%\n\nBusiness Questions\n\nAgent - Which house should I buy or build to maximize my return? - Where or when should I do so? - What is its optimum rent or sale price?\nBuyer - Match them with agent - find them a home - appropriate mortgage\n\nUse Cases\n\nUsing computer vision and NLP to enhance searches - Incorporating image search capabilities: extract information from pictures of the property utilizing object detection and image classification techniques, to be used in search matching. - Recommendation-engine-powered rankings: search results could be ranked according to the likelihood that the specific user will interact with the results, based on previous searches, profile characteristics, and contextual information. - Search intent matching: enhance the user experience by adding the ability to write (or dictate) their home preference(s) instead of manually filtering the results. It may be very wise to incorporate such a feature given the rise of home assistants. - Visual search: perform a search based purely on images of homes. This would enhance the search experience or complement the keyword search and produce more accurate and useful results.\nIdentify homeowners who are more likely to sell and estimate their selling price using publicly available data\nFind correlations between past sellers and current homeowners, in order to help predict the likelihood that a given owner is willing to sell.\nEstimate appropriate selling price point and the interest of possible future owners so as to increase the chances of successful outreaches.\nLead classification: - Based on web-based actions executed by a user, understand where they are at in the customer journey and gather accurate information to move them to the next stage in the funnel.\nRisk assessment: - Multiple risks should be considered when assessing a real estate transaction. Forecasting models powered by machine learning could complement a traditional risk analysis approach well, especially given their multiple data source analysis capabilities.\nHome valuation: - A classic, yet constantly evolving, machine learning task is to set the price of a house based on MLS and alternative data (e.g. satellite imagery). The big iBuyers are all betting heavily on this since it’s key to their business model. An example of the level of detail obtainable is that of how Opendoor analyzed the impact of busy roads in their property valuation model.\n\nAppraisal Methods (Price Estimation)\n\nTraditional - Comparative Market Analysis (CMA) - a collection of recently sold “comparable” homes (“comps”) that, taken in aggregate, can provide a view on the value distribution of the appraised home (the “subject” home) - selecting similar properties (in terms of attributes and location) and inferring the target value from the comparables - Cost Approach - The “replacement cost” is determined by totaling values such as the value of raw land (again, using comparison), the cost of rebuilding a new building that could perform the function of the existing property, and then making necessary adjustments (e.g., deprecation of the existing building). - Profits Method (or income approach) - estimates the value of the property based on the income it generates. The value is linked to the business carried out within that property. For example, the market value of a hotel depends on the potential cash flow derived from ownership. - Repeat sales method - analysis is restricted to just comparing price changes on properties sold more than once\nML Approaches - Misc - Automated Valuation Models (also known as AVMs) were found to have an absolute error below 4% for homes and below 6% for commercial properties, which is much less than the error rates of traditional appraisals. - Features - Different predictors may be more valuable for each of the price models. - Sale price: Zillow and Redfin use their own algorithms for real estate price estimation. - Rental price: HomeUnion developed a tool called RENTestimate for this. - Temporary rental price: Airbnb’s pricing tips use a mathematical model that learns how likely a guest is to book a specific listing, on specific dates, at a range of different prices. - Adding nontraditional data sources can improve estimates - Hedonic Pricing Method: a typical regression that accounts for property features such as size, number of rooms, property age, home quality characteristics (granite countertops, air conditioning, pool, etc.), and location. {hpiR} (vignette)\n    It's just multivariable regression where log sale price (or ppsf, etc.) is the outcome and includes house characteristics and has dummy variables for the years (or days, weeks, etc.) when the house was sold\n\n    The exponential of the coefficients of the year dummy variables are the index and they represent the average change in the outcome variable from the reference year to the year represented by the dummy variable\n        1 is the index of the reference year\n\n-   CMA with Comp Price Adjustments\n    Process\n        Data Scientist role\n            Suggests good comparable properties for a CMA\n\n        -   Highlights the important differences between the comparable and the subject property (the fewer, the better)\n        -   Suggests pricing adjustments for the differences\n        Real Estate Agent\n            Selects which properties to use as comps\n\n            Selects which adjustments are relevant and appropriate for their CMA report\n\n    Misc\n        Notes from\n        -   [Pricing Homes like Agents Do: AI for Real Estate CMA Adjustments](https://medium.com/compass-true-north/pricing-homes-like-agents-do-ai-for-real-estate-cma-adjustments-c3de27a7ef)\n        -   All models of adjustments seem to use Price per Square Foot (PPSF) for a local area as the outcome variable\n        -   Probably want to go as small an area as you can depending on the amount of training data at that resolution\n    Market Price\n        Using the repeat sales model, we estimate how the price-per-square-foot evolved for each locality. Then we can use the model to estimate the difference in average price per square foot (PPSF) in the locality across any two-time points.\n\n        -   If a price index rises 10%, it means the average level of prices has risen 10%\n            Example:\n                CMA model produces a comp that sold for $1,100,000 almost a year ago in April 2019\n\n                Repeated Sales model's indexes say that property prices have decreased by 4.6% in area since the sale of this comp.\n\n                Thus, to make this comp comparable now, we would need to subtract $51K (1,100,000 \\* 0.046 = 50,600) from the price it sold for back in April 2019.\n\n        In this description and the examples, I'm using sale price, but the same methods can be applied to PPSF.\n\n        The data is house id, year sold, sale price\n            There will be repeated measures because each house will need to have been sold multiple times.\n                Maybe can this be fudged a bit by using really good comps for the repeated measures\n\n            Only the data from the 1st and last sales are used\n\n            The first year in the sample data is NOT included as a variable in the regression\n                This is the reference or base year. This year automatically gets an index of 1.\n\n        Assumes homeoskedacity\n\n        Specification\n            log(last\\_sale\\_price / first\\_sale\\_price) = β1 \\* I(year\\_1) + β2 \\* I(year\\_2)  + ...\n                where I(year\\_\\*) is an indicator variable with values of\n                    1 for year of last sale\n\n                    \\-1 for year of first sale\n\n                    0 for years where the house wasn't sold\n\n                    \"\\*\" is the year of the sale of the house.\n\n                Note that there is no intercept so will need `0 +` in the formula\n\n            exp(β\\*) gives the index for that year\n                Index for the first year of the sample data is automatically set to 1.\n\n        `Example`\n\ndat &lt;- tibble(\n  y = rep(c(0.182, 0.041, 0.039), 3),\n  year = rep(c(\"2013\", \"2014\", \"2015\"), each = 3),\n  value = c(-1, -1, 0, 0, 1, -1, 1, 0, 1)\n) %&gt;% \n  tidyr::pivot_wider(names_from = year, values_from = value, names_prefix = \"Y\")\n      y Y2013 Y2014 Y2015\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.182    -1    0    1\n2 0.041    -1    1    0\n3 0.039    0    -1    1\n\nmod &lt;- lm(y ~ 0 + Y2014 + Y2015, data = dat)\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)\nY2014  0.07500    0.04808  1.560    0.363\nY2015  0.14800    0.04808  3.078    0.200\n\nexp(coef(mod))\n  Y2014    Y2015 \n1.077884 1.159513\n\n“y” is the log of the sales price ratio\nIndexes: - 2013: 1 - 2014: 1.08 - 2015: 1.16\nCase-Shiller Method Handles heteroskedasticity by Weighted Least Squares\nTypically follows the same trend (highly correlated) as the regular repeated sales model but on a lower level\n\nPackages\n    {hpiR} ([vignette](https://cran.r-project.org/web/packages/hpiR/vignettes/introduction.html))\n        Multiple modeling options, including weighted regression, robust regression, random forest\n\n        Also splits up houses with more than 2 sales into combinations of binary periods\n\n        \\*Doesn't sqrt the residuals in the weighted regression like I think the original paper recommends\\*\n\nProcess\n    First fit the repeated sales model:\n    log(last\\_sale\\_price / first\\_sale\\_price) = β1 \\* I(year\\_1) + β2 \\* I(year\\_2)  + ...\n\n    Regress the squared residuals on the holding period\n    ε2 = β0 + β1 \\* holding\\_period\n        Holding Period: the period between the first sale and last sale\n\n    Calculate weights using fitted values\n    weights = 1 / sqrt(^ε2)\n\n-   Fit weighted repeated sales model\n-   Model\n\nmod &lt;- lm(LogP ~ 0 + ., data = dat %&gt;% select(-HoldingPeriod))\nmod_resids &lt;- resid(mod)\n\nholding_dat &lt;- dat %&gt;% \n  select(HoldingPeriod) %&gt;% \n  mutate(resids_sq = mod_resids^2)\n\nresids_mod &lt;- lm(resids_sq ~ HoldingPeriod, data = holding_dat)\nresid_preds &lt;- fitted(resids_mod)\nwgts &lt;- ifelse(resid_preds &gt; 0, 1 / sqrt(resid_preds), 0)\n\nfinal_mod &lt;- lm(LogP ~ 0 + ., \n                data = dat %&gt;% select(-HoldingPeriod),\n                weights = wgts)\n\n# indexes\nexp(coef(final_mod))\nHolding Period: the period between the first sale and last sale\n\n\"LogP\": See previous model's Specification\n\n\nBuilding-Floor\n    Higher floors in apartment building cost more\n    \n    Adjustment varies per building but there isn't enough training data to have a model per building\n        Solution: Bin apartment data by building size, then fit a model for each bin.\n        \n    Perform Market Price adjustment to prices before modeling price differences by floor\n    \nLocation\n    Buildings are of different ages, have various contractual constraints, and offer vastly different amenities.\n        Example: A subject propert in a very expensive building for this neighborhood. The comp property is in a building where similarly sized apartments tend to sell for lower prices. So an adjustment to the comp needs to be made.\n        \n    Spatial nearest neighbor models for ppsf.\n        To prepare the training data, we use the Market Price adjustment discussed already\n        \n        packages: {nngeo}\n        \nFeatures\n\nScrape traditional variables (from websites?) - Number of bedrooms - Number of bathrooms - Living area, square footage - price per square foot (PPSF) - Number of stories - Year built - property type (e.g. condo, house) - Furnished/not furnished - Fireplace/no fireplace - Heating/no heating - pool/no pool - garage space - ZIP code - Latitude and longitude - listing price of comparable homes in the area - sale price - days on the market\nOff-market data - tax assessments - prior sales and other publicly available records - Resident surveys - mobile phone signal patterns - Yelp reviews of local restaurants help identify “hyperlocal” patterns—granular trends at the city block level rather than at the city level.\nMarket trends - inflation - seasonality in demand - economic shocks\nGeographic indicators - Uber’s H3 grid system instead of zip codes, etc. - The quality of local schools - Employment opportunities - Proximity to shopping, entertainment, and recreational centers - Sources Google Places, Yelp, or SchoolDigger\nImage data - Satellite imagery - Mapbox’s Satellite API - allows you to query any location by its latitude and longitude coordinates - select from 20 different zoom levels - 50,000 free requests each month!! - number of roads and their condition and houses with pools - Method - Convolutional Neural Networks (CNNs) can extract visual features, revealing underlying information captured in photos - satellite - A classification network can distinguish between the top and bottom 15% of houses in a dataset to test this. A model achieved an outstanding 91% accuracy, using only satellite images. After interpreting the model to understand which visual patterns governed its decision process, we saw that it placed high importance on recreational areas such as parks and lakes. - exterior frontal images and interior photos - Features - the activeness of a street frontage - accessibility of the area (number of roads, condition of the roads) - amount of urbanization - proximity to parks, lakes and beaches (recreational areas) - amount of greenery - population density\nNLP - Identify the most important words in a description and learn which words tend to be used to describe more expensive or affordable houses.\nSociodemographic - Misc - Which data resolution is most predictive? (e.g. state, county, neighborhood, etc.) - Official government data sources are best (consistent, unbiased, and up-to-date indicators, with the largest sample sizes) - Median Household Income - Unemployment rate - crime rate - Ethnicity - Number of inhabitants - Average age - Poverty level - Source: American Community Survey (ACS) at census tract level - small subdivisions of a county (around 4,000 inhabitants on average), - designed to be relatively homogeneous units concerning population characteristics, economic status, and living conditions - Total population - Total employed population - population median age - Median household income - Population with income below poverty level - Minority percentage - Vacant housing units for rent - Median value for all owner-occupied housing units"
  },
  {
    "objectID": "qmd/retail.html",
    "href": "qmd/retail.html",
    "title": "16  Retail",
    "section": "",
    "text": "TOC\n\nMisc\nLead Scoring\nCatchment\nStore Network\n\nMisc\n\nDS Use Cases (intermediate level - data needs to be centralized and not siloed) - Demand forecasting models for optimizing the stock levels. - Market basket analysis models for creating better newsletter offers. - Dynamic pricing models using competitors’ prices for better pricing strategies. - Customer segmentation model for better understanding our customers’ shopping preferences and providing them with customized offers and loyalty discounts.\nApproach to deciding on a store to close - Considerations - Catchment Area: How to define the area that matters around my store - Store Network: What are the relationships between stores - Predicting Sales Transfer of your Customers (aka Customer Retention) - Overlap in catchment areas and understanding how customers behave in your store network can help in this estimation - Predicting the Impact on Acquiring New Customers (aka Customer Acquistion) - Bringing it together - Calculate 1-year, 3-year impact of removing a particular store - Rank these impacts to choose a store whose removal will have the least impact - Measurement & Model Improvement - e.g. including real estate data such as lease terms and market intelligence on competition and anchor stores - Impact on Customer Retention - Depends on the convenience and quality of other channels for doing business (such as online or in physical locations), customer loyalty to the company and its products, and market competition. - If you have historical data on customer retention after a store closure, you can use catchment and store network features to predict what will happen with customer retention if you decide to close a particular store. - Impact on Customer Acquistion - For a particular store, take the its catchment area and remove any section that overlaps with another store’s catchment. - This area will not have any support from a brick-and-mortar perspective to help in your acquisition efforts - Look at trends over time to get a sense of the relative importance of this store’s area to the overall chain and whether you should build a digitally supported strategy to offset the loss of acquiring new customers at a fraction of your cost of operating a retail store at that location\n\nLead Scoring\n\nAlso see - Algorithms, Marketing &gt;&gt; Propensity Model and Uplift Score Model\nUse case: identify customer cohorts that are unlikely to become paying customers and eliminate the low efficient outreach\nMap User Journey - Also see Marketing &gt;&gt; Customer Journey - Example: B2B\nQuestions - Find customer’s last touchpoint before conversion - e.g. proof of concept in B2B - Brainstorm on metrics that may correlate with conversion and form hypotheses - e.g. metrics that describe user behaviors in proof-of-concept - Which customer cohorts are likely to schedule sales calls with us and how do they behave without sales guidance - i.e. which cohorts are more likely to convert if they are given a sales call - look at how prospects behave in product trials and interact with marketing emails\nEDA - Plot metrics by cohort - Calculate correlation of metrics to conversion - Cohorts can be segmented by any grouping factor\nModel - If you have enough data, model conversion rate ~ predictors (beta regression?) or convert/no covert ~ predictors (logistic regression, ML, DL) - Otherwise, weight cohorts by their correlations with conversion\n\nCatchment\n\nRefers to the sphere of influence from which a retail location, such as a shopping center, or service, such as a hospital, is likely to draw its customers.\nMajor considerations – supply factors, market demand factors, drive time from customers, transportation access (e.g. interstate) and consumer interactions\nSteps - Use the customers that purchased at the store in the past 12 or 24 months (you need to be the judge on timeframe based on your business model), and map them to where they live, e.g., at census block group level. - If the area is massive, applying an optimization function, where you use a polygon and work to minimize the size of the area while keeping the cumulative percent of the total sales as large as possible. (70-80% of cumulative sales is typically optimal) - Assume this optimization function takes into account the considerations mentioned above\nMeasuring the amount of competition in the given catchment area is useful\nCatchment areas might overlap.\n\nStore Network\n\nUnderstanding how stores are connected beyond a catchment area is essential to make smarter optimization decisions (i.e. closing or opening stores).\nSteps - Pick a store and for each customer, note which of your other stores (including ecommerce) that customer has shopped at - (typically) 3 types of customers: - Customers who shop at this store exclusively - Customers who spend the majority of their $ with you at this store and spend a smaller amount at other stores, including e-commerce - Customers who spend a small amount of their $ at this store and spend the most significant amount at other stores, including e-commerce - For the ones that have shopped at multiple locations, those are generally your more loyal and high-value customers. - This data forms the basis of the network model - If the network gets too complicated, then pruning by adding addition rules might be necessary - e.g. establishing rule for a minimum amount spent to the store network"
  },
  {
    "objectID": "qmd/saas.html",
    "href": "qmd/saas.html",
    "title": "17  SaaS",
    "section": "",
    "text": "TOC\n\nMisc\nUser Segmentation\n\nMisc\n\nNotes from - Meaningful metrics: How data sharpened the focus of product teams\n\nUser Segmentation \n\nDiagram of users based on their activity on the language learning software, “duolingo.” - See article for an example of a user journey\nActivity States - New users: learners who are experiencing Duolingo for the first time ever - Current users: learners active today, who were also active in the past week - Reactivated users: learners active today, who were also active in the past month (but not the past week) - Resurrected users: learners active today, who were last active &gt;30 days ago - At-risk Weekly Active Users (WAU): learners who have been active within the past week, but not today - At-risk Monthly Active Users (MAU): learners who were active within the past month, but not the past week - Dormant Users: learners who have been inactive for at least 30 days\nRates - Retention Rates - New User Retention Rate (NURR): The proportion of day 1 learners who return on day 2. This day 2 transition puts these learners into another “active” state (Current User) - Deactivation Rates: e.g. WAU or MAU loss rate - Activation Rates: e.g. Reactivation or Resurrection rate\nProcess - Classify all users (past or present) into an activity state each day - Collect data: monitor rates of transition between states. - These transition probabilities are monitored as retention rates, activation rates, and deactivation rates - Use transition probabilities to create Markov model for simulations\nIdentify new metrics, through simulations of the Markov model, that - when optimized - are likely to increase a North Star or Primary metric. - Example - Simulation is for 3yrs into the future - “Lever” is the parameter being increased - In the sim, it’s increased 2% month-over-month - Daily Active Users (DAU) is the North Star metric that is being opitimized (i.e. increased) - Interpretation: Increasing Current User Retention Rate (CURR) increases DAU by 75% over the next 3yrs - Formulas at time, t - DAUt = ReactivatedUsert + NewUsert + ResurrectedUsert + CurrentUsert - WAUt = ReactivatedUsert + NewUsert + ResurrectedUsert + CurrentUsert + AtRiskWAUt - MAUt = ReactivatedUsert + NewUsert + ResurrectedUsert + CurrentUsert + AtRiskWAUt + AtRiskMAUt - ReactivatedUsert = ReactivationRatet * AtRiskMAUt-1 - ResurrectedUsert = ResurrectionRatet * DormantUserst-1 - CurrentUsert = (NewUsert-1 * NURRt) + (ReactivatedUsert-1 * RURRt) + (ResurrectedUsert-1 * SURRt) + (CurrentUsert-1 * CURRt) + (AtRiskWAUt-1 * WAURRt) - DormantUsert = (DormantUsert-1 * DormantRRt) + (AtRiskMAUt-1 * MAULossRatet) - AtRiskMAUt = (AtRiskMAUt-1 * ARMAURRt) + (AtRiskWAUt-1 * WAULossRatet) - AtRiskWAUt = (AtRiskWAUt-1 * ARWAURRt) + (CurrentUsert-1 * (1-CURRt)) + (ReactivatedUsert-1 * (1-RURRt)) + (NewUsert-1 * (1-NURRt)) + (ResurrectedUsert-1 * (1-SURRt))\nPerform A/B tests to see whether 1) the selected metric can be moved and 2) whether moving it actually moves the north star/primary metric"
  }
]