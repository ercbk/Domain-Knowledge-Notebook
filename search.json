[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Domain Knowledge",
    "section": "",
    "text": "Preface\nNotebook for various subjects and industries in the context of data science.\nInitially, I had no intent of publicly releasing these notes, so I haven’t attributed all the authors of the content inside this notebook. If you see your work and want credit, please let me know and I’ll do so.\nIf you see any mistakes or have any questions, please open an issue at the github repository.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "qmd/agriculture.html",
    "href": "qmd/agriculture.html",
    "title": "Agriculture",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Agriculture"
    ]
  },
  {
    "objectID": "qmd/agriculture.html#sec-ag-misc",
    "href": "qmd/agriculture.html#sec-ag-misc",
    "title": "Agriculture",
    "section": "",
    "text": "Commodity prices normally forecast using a cobweb model which leads to price risk\nVegetable price series are much more volatile than other commodities\n\nSeasonality regulates the supply and demand\nPerishable nature of the produce adds complications in stabalizing the price",
    "crumbs": [
      "Agriculture"
    ]
  },
  {
    "objectID": "qmd/banking-credit.html",
    "href": "qmd/banking-credit.html",
    "title": "Banking/Credit",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Banking/Credit"
    ]
  },
  {
    "objectID": "qmd/banking-credit.html#sec-bank-cred-misc",
    "href": "qmd/banking-credit.html#sec-bank-cred-misc",
    "title": "Banking/Credit",
    "section": "",
    "text": "Logistic Regression models typically with 8 to 10 predictors are common\n\n“To adopt a new algorithm, it not only had to outperform regression. The improvement also had to justify the effort of explaining the algorithm.”\n\nUsing SHAP, PDPs, etc. just adds more complexity because then you would have also needed to explain the method used to explain the model\n\n\nDS Use Cases\n\nCredit risk — predict default due to financial distress\nFraud — predict if customers do not intend to repay a loan\nPre-areas — identify customers in financial distress\nChurn — identify customers who intend to leave the bank\nMarketing — identify the best customers to promote a product to",
    "crumbs": [
      "Banking/Credit"
    ]
  },
  {
    "objectID": "qmd/banking-credit.html#sec-bank-cred-fraud",
    "href": "qmd/banking-credit.html#sec-bank-cred-fraud",
    "title": "Banking/Credit",
    "section": "Fraud",
    "text": "Fraud\n\nMisc\n\nFraud Score - Values that indicate how risky a user action is. Scoring determined by fraud rules.\nComputing ROI for a fraud model\n\nAssume the cost of fraud is the cost of the transaction\nCalculate the total cost of all the fraudulent transactions in the test dataset.\n\nCalculate the cost based on the model predictions.\n\nFalse Negatives: Observed frauds that weren’t predicted are assigned a cost equal to the value of the transaction.\nFalse Positives: Legitimate transactions that were marked as fraud are assigned $0 cost.\n\nThis likely isn’t true. There is the cost of having to deal with customers calling because the transaction was declined or the cost sending out texts for suspicious transactions, but this cost is very small relative to the cost of a fraudulent transaction.\nZhang, D. , Bhandari, B. and Black, D. (2020) Credit Card Fraud Detection Using Weighted Support Vector Machine. Applied Mathematics, 11, 1275-1291. doi: 10.4236/am.2020.1112087.\n\nOther costs can include deployment (e.g. DL model vs logistic regression)\nROI of the new model = costs of old model - cost of new model\nExample: article\n\n\nMetrics\n\nFN: Predicting “not fraud” for a transaction that is indeed fraud\n\nA false negative more costly than false positive\n\nRecall (aka sensitivity): Ratio of correct fraud predictions (TP) out of all fraud events (TP + FN)\nFN Rate: Ratio of fraud events  the model failed to predict out of all fraud events\n\nComplement of Recall, (FN/(TP+FN))\nMost expensive to organizations in terms of direct financial losses, resulting in chargebacks and other stolen funds\n\nFP Rate: Rate at which a model predicts fraud for a transaction that is not actually fraudulent\n\nFP / (FP + TN)\nMeasures the incovenience for the customer that the model inflicts\nSeems to be a metric to monitor by group to see if the model is ethically biased\n\n\nModel Monitoring\n\nFar more false positives in production than the validation baseline\n\nResults in legitimate purchases getting denied at the point of sale and annoyed customers.\n\nA dip in aggregate accuracy\n\nInvestigate prediction accuracy by group\nExample: The fraud model isn’t as good at predicting smaller transactions relative to the big-ticket purchases that predominated in the training data\n\nFeature performance heatmaps can be the difference between patching costly model exploits in hours versus several days\nScenario Examples\n\nPrediction Drift\n\nPossible Drift Correlation: An influx and surge of fraud predictions could mean that your model is under attack! You are classifying a lot more fraud than what you expect to see in production, but (so far) your model is doing a good job of catching this. Let’s hope it stays that way.\nReal-World Scenario: A hack of a health provider leads to a surge of identity theft and credit card numbers sold on the dark web. Luckily, the criminals aren’t novel enough in their exploits to avoid getting caught by existing fraud models.\n\nActuals Drift (No Prediction Drift)\n\nPossible Drift Correlation: An influx of fraud actuals without changes to the distribution of your predictions means that fraudsters found an exploit in your model and that they’re getting away with it. Troubleshoot and fix your model ASAP to avoid any more costly chargebacks.\nReal-World Scenario: A global crime ring sends unemployment fraud to all-time highs using new tactics with prepaid debit cards, causing a dip in performance for fraud models trained on pre-COVID or more conventional credit card data.\n\nFeature Drift\n\nPossible Drift Correlation: An influx of new and/or existing feature values could be an indicator of seasonal changes (tax or holiday season) or in the worst case be correlated with a fraud exploitation; use drift over time stacked on top of your performance metric over time graph to validate whether any correlation exists.\nReal-World Scenario: An earlier holiday shopping season than normal takes hold, with bigger ticket purchases than prior years. It might be a sign of record retail demand and changing consumer behavior or a novel fraud exploitation (or both).",
    "crumbs": [
      "Banking/Credit"
    ]
  },
  {
    "objectID": "qmd/budget-allocation.html",
    "href": "qmd/budget-allocation.html",
    "title": "Budget Allocation",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Budget Allocation"
    ]
  },
  {
    "objectID": "qmd/budget-allocation.html#sec-budallo-perform-cuts",
    "href": "qmd/budget-allocation.html#sec-budallo-perform-cuts",
    "title": "Budget Allocation",
    "section": "Performance-based Budget Cuts",
    "text": "Performance-based Budget Cuts\n\nNotes from: The Politics of Performance Measurement - Scenario: “Criminal Justice Division (CJD) of the Texas Governor’s Office received news all government agencies dread: budgets were to be cut. CJD oversaw a grant program that funded specialty courts throughout the state, however it was now being told that the program’s budget of $10.6m would be reduced 20% to $8.5m by 2018.”\n\nHow should these cuts be distributed among grant holders?\nGoal: Develop a data collection and performance assessment process to allocate budget cuts in a manner widely accepted\n\nOptions\n\nCut across the board. The Advisory Council would employ the same scoring method as the previous year but reduce each grant amount by 20%.\n\nThis option would leave long-running grantees scrambling to make up for this shortfall by reducing services, laying off staff, or spending more of their limited local funds. Worse, it would punish all grantees equally — our most successful programs would be arbitrarily defunded.\n\nFewer grants. Grants were scored based on the quality of their application and all grants that passed a certain threshold got funded. The Advisory Council would employ the same scoring method as the previous year but instead of funding the top $10.6m worth of grants, they would fund the top $8.5m worth.\n\nThis seemed a less bad option than cutting across the board, but we would still run into the problem of arbitrarily defunding successful programs. Grants near the bottom of the Advisory Council’s cutoff that got funded the previous year would be denied renewal only because the goalposts had moved.\n\n\n\n\nTargeted funding. The Advisory Council would incorporate performance data and statewide strategic plan alignment into their scoring method and make cuts accordingly.\n\nEngage stakeholders and define performance\n\nConvene a strategy session with the stakeholders to discuss how to proceed as part of a broader strategic plan\n\nAchieve consensus on high-level goals (e.g. fund strategically, focus on success, build capacity)\nLarger plan agreed upon that would also include: capacity building, training and technical assistance, helping courts obtain non-CJD sources of funding, and steering grantees toward established best practice.",
    "crumbs": [
      "Budget Allocation"
    ]
  },
  {
    "objectID": "qmd/economics.html",
    "href": "qmd/economics.html",
    "title": "Economics",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Economics"
    ]
  },
  {
    "objectID": "qmd/economics.html#sec-econ-misc",
    "href": "qmd/economics.html#sec-econ-misc",
    "title": "Economics",
    "section": "",
    "text": "Dynamic stochastic general equilibrium (DSGE) models, which are popular in macroeconomic modeling, are garbage (article)\n\nEven under extremely ideal conditions they don’t retrieve the parameters and using them to forecast is no better than chance.",
    "crumbs": [
      "Economics"
    ]
  },
  {
    "objectID": "qmd/economics.html#sec-econ-terms",
    "href": "qmd/economics.html#sec-econ-terms",
    "title": "Economics",
    "section": "Terms",
    "text": "Terms\n\nAdverse Selection - a market situation where buyers and sellers have different information. The result is that participants with key information might participate selectively in trades at the expense of other parties who do not have the same information\n\ne.g. A person waits until he knows he is sick and in need of health care before applying for a health insurance policy. The buyer has more knowledge (i.e., about their health). To fight adverse selection, insurance companies reduce exposure to large claims by limiting coverage or raising premiums\n\nConsumer Price Index (CPI) - A common index that is used to measure inflation, the Bureau of Labor Statistics constructs the market basket using more than 80,000 items.\nDeflation - A sustained decrease in the price level of goods and services.\n\nDisinflation - A decrease in the rate of inflation.\nInflation - A sustained increase in the price level of goods and services.\nIntertemporal Price Discrimination - charging a high price initially, then lowering price after time passes.\n\nA method for firms to separate consumer groups based on willingness to pay\ne.g. last minute travel bookings (opposite direction since last minute bookings usually cost more)\n\nPrice Elasticity of Demand (PED)- the percent change in demand given the percent change in price assuming that everything else doesn’t change\n\nHow sensitive the quantity demanded is to its price. When the price rises, quantity demanded falls for almost any good, but it falls more for some than for others.\nThe elasticity of a good or service can vary according to the number of close substitutes available, its relative cost, and the amount of time that has elapsed since the price change occurred.\nWhen the price of a good or service has reached the [point of elasticity, sellers and buyers quickly adjust their demand for that good or service.\nAn inelastic product is one that consumers continue to purchase even after a change in price\nProducts or services that are elastic are either unnecessary or can be easily replaced with a substitute.\n\nSecond Degree Price Discrimination - charging a different price for different quantities at the same time",
    "crumbs": [
      "Economics"
    ]
  },
  {
    "objectID": "qmd/economics.html#sec-econ-pricelas",
    "href": "qmd/economics.html#sec-econ-pricelas",
    "title": "Economics",
    "section": "Price Elasticity",
    "text": "Price Elasticity\n\nAlso see Algorithms, Products &gt;&gt; Price Optimization\nBy identifying the price elasticity of demand, you can try to determine the amount of price you can increase without hurting the demand, as well as check at what point an increase in price starts to affect the market.\nPrice is NOT the only variable that influences whether you purchase a product or service. Therefore, looking at quantity purchased at each price to determine price elasticity is not enough.\nGuidelines\n\nIf the PED is greater than one (PED &gt; 1), it is known as “elastic”, meaning changes in price causes a significant change in demand.\nIf the PED is equal to 1 (PED = 1), then this means any change in price causes equivalent changes in demand.\nIf the PED is less than one (PED &lt; 1), it is known as “inelastic”. This means changes in price don’t affect the demand that much.\nIf the PED is equal to 0 (PED = 0), known as “perfectly inelastic”, meaning any change in price doesn’t cause a change in demand.\n\nProcess\n\nFilter on the specific subset of sales data relevant to the dimension which you are estimating elasticity (e.g. if estimating the price elasticity for red wine, filter on only red wine sales)\nPerform a log transformation on the future sales target variable and on the current price feature\nTrain a multivariable linear regression model to accurately predict future sales\nThe price elasticity estimate will be the coefficient of the log transformed, price feature\nRepeat steps 1–4 for each elasticity estimate",
    "crumbs": [
      "Economics"
    ]
  },
  {
    "objectID": "qmd/environment.html",
    "href": "qmd/environment.html",
    "title": "Environment",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Environment"
    ]
  },
  {
    "objectID": "qmd/environment.html#sec-environ-metrics",
    "href": "qmd/environment.html#sec-environ-metrics",
    "title": "Environment",
    "section": "Metrics",
    "text": "Metrics\n\nExisting Tree Canopy: The amount of urban tree canopy present when viewed from above using aerial or satellite imagery.\n\nETC % = tree canopy / land area\n\nPossible Tree Canopy - Vegetated: Grass or shrub area that is theoretically available for the establishment of tree canopy.\n\ne.g. residential areas\n\nPossible Tree Canopy - Impervious: Asphalt, concrete or bare soil surfaces, excluding roads and buildings, that are theoretically available for the establishment of tree canopy without having to remove paved surfaces\n\ne.g. any areas with no trees, buildings, roads, or bodies of water\nPossible-Vegetation category should serve as a guide for further analysis, not a prescription of where to plant trees since other factors, such as land use, social, and financial (e.g. golf courses, agricultural and recreational fields), are involved.\n\nNot Suitable: Areas where it is highly unlikely that new tree canopy could be established (primarily buildings and roads).\nRelative tree canopy change - change of tree canopy over a period of time\n\ne.g (for 1 hexagon) relative tree canopy change % = (tree_canopy_area_2019 - tree_canopy_area_2012) / tree_canopy_area_2012 - Acre gain per \n\nCanopy height - proxy for tree age\n\nSteps\n\nSegment tree canopy into polygons approximating individual trees\nAttribute each polygon with a height from both the starting date to end date (e.g. 2012 and 2019 ) LiDAR data\n\nInterpretation example\n\nTrees in the 0-60 foot height class experienced gain, while there was minimal gain in the other taller height classes.\n\nTherefore, many new trees planted and canopy expanding on existing trees.\nDiverse height structure corresponds to a healthy and diverse tree age distribution\nVery mature trees in the 130 height class points to the height potential for certain tree species provided the right conditions",
    "crumbs": [
      "Environment"
    ]
  },
  {
    "objectID": "qmd/environment.html#sec-environ-canopy",
    "href": "qmd/environment.html#sec-environ-canopy",
    "title": "Environment",
    "section": "Canopy Assessment",
    "text": "Canopy Assessment\n\nNotes from Lousiville Tree Canopy Assessment 2012-2019\nTree benefit: reducing stormwater runoff near streets and decreasing the urban heat island effect\nAbove surface factors such as sidewalks to utilities can affect the suitability of a site for tree planting.\nImportant to preserve trees in the 10-50 foot height range, so they can grow into the 60+ foot range while planting a variety of new trees to continue the lifecycle\nLosses are generally easier to detect than gains as losses tend to be due to a large event, such as tree removal, whereas gains are incremental growth or new tree plantings, both of which are smallerin size\nFactors that can affect change in tree canopy\n\nNatural\n\nInvasive species\nNatural disasters such as storms\nClimate change may cause trees to grow more quickly but could also result in inhospitable conditions for native species\n\nAnthropogenic\n\nPreservation and conservation efforts, the strength of tree ordinances, and the impacts of new development\nTree removal due to homeowner preferences and not being replaced by new trees\nProximity to roads: Regular salting, compaction, limited space, clearance pruning, and plow collisions\n\n\nData sources\n\nLiDAR\n\nFeatures distinguished by their spectral (color) properties\nTrees and shrubs can appear spectrally similar or obscured by shadow, LiDAR, which consists of 3D height information enhances the accuracy of the mapping\nResolution of 30-meters\n“LiDAR datasets were acquired under leaf-off conditions and thus tend to underestimate tree canopy slightly” (i.e. Fall or Winter?)\nLiDAR and imagery datasets are not directly comparable due to differences in the sensor, time of acquisition, and processing techniques employed.\nResources:\n\nPaper summarizing their tree canapy mapping approach\nGetting to Land Use/Land Cover\nThreshold Classification in eCognition\nObject-based approach to LiDAR\n\n\n\n%  using 500-acre hexagons\n\nUse LiDAR Hill shade map with % canopy change to highlight local areas\n\nLand Use Categories\n\nOverall: residential, commercial, and recreational\nMetric change by category | acres lost (orange)/gained (green) by category\n\n\nChange per Council District (by metric)",
    "crumbs": [
      "Environment"
    ]
  },
  {
    "objectID": "qmd/epidemiology.html",
    "href": "qmd/epidemiology.html",
    "title": "Epidemiology",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Epidemiology"
    ]
  },
  {
    "objectID": "qmd/epidemiology.html#sec-epidemi-dismap",
    "href": "qmd/epidemiology.html#sec-epidemi-dismap",
    "title": "Epidemiology",
    "section": "Disease Mapping",
    "text": "Disease Mapping\n\nGoals\n\nProvide accurate estimates of mortality/incidence risks or rates in space and time\nUnveil underlying spatial and spatio-temporal patterns\nDetect high-risk areas or hotspots\n\nRisk estimation using metrics such as Standardized Mortality Ratio (SMR) when analyzing rare diseases or low-populated areas are highly variable over time, so it’s diffficult to spot patterns and form hypotheses\n\nSMR = Observed number of cases / Expected number of cases\n\nSMR &gt; 1: risk is greater than the whole region under study\nGuessing “Expected number of cases” is the average number of cases for the whole study region\n\n\nStatistical models smooth risks by borrowing information from spatio-temporal neighbors\n\nThe smoothed gradient over the entire study region makes it easier to detect patterns and form hypotheses than highly variable, local area metric estimates (e.g. SMR in a low populated county)\n\nTraditional Models\n\nTypes\n\nMixed Poisson with conditional autoregressive (CAR) priors for “space” and random walk priors for “time” that include space ⨯ time interactions (Knorr-Held, 2000, Bayesian modeling of inseperable space-time variation in disease risk)\nReduced rank multidimensional P-splines (Ugarte et al, 2017, One-dimensional, two-dimensional, and three dimensional B-splines to specify space-time interactions in Bayesian disease mapping)\n\nIssues\n\nEstimating the cov-var matrix becomes intractable with big data and many areas since the covariance must be estimated between each pair of areas\nCAR models assume the same level of spatial dependence between all areas which isn’t likely.\n\n\n{bigDM}\n\nScalable non-stationary Bayesian models for high-dim, count data\nDependencies\n\nUses {future} for distributed computing\nIntegrated, nested laplace approximation (INLA) method through {R-INLA}\n\nK-order neighborhood model\n\nBreaks up local spatial or spatio-temporal domains so that estimations can distributed and local area dependencies (neighborhoods) can be accounted for.\n“Areas” are usually districts, counties, provinces, etc.\n\nPackage does provide a method to create a “random” area grid\n\nMight be useful to compare a random grid model with the e.g. county model to see how much county boundaries influence the estimates\n\n\nEach local area model includes k adjacent areas which creates a partition\n\nThe local area estimate is smoothed by taking information from the adjacent areas\nAdjacent areas also have estimate posteriors computed\nEach area will have multiple posterior estimates from local area models where the area is the local area or where it is the adjacent area\n\nMerge or don’t merge estimate posteriors for each area\n\nMerge: use weights proportional to the conditional predictive ordinates (CPO) ???\nDon’t Merge: Use the posterior marginal risk estimates of an area corresponding to the original submodel.\n\ni.e. use the posterior where the area is the “local area” in that local area model and not an adjacent area.\n\nPrimary functions\n\nCAR.INLA() fits several spatial CAR models for high dim count data\nSTCAR.INLA() fits several spatio-temporal CAR models for high dim count data",
    "crumbs": [
      "Epidemiology"
    ]
  },
  {
    "objectID": "qmd/finance-glossary.html",
    "href": "qmd/finance-glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Amortization - Similar to depreciation except for intangible assets. A non-cash expense that reduces the value of a company’s definite life intangible assets and also reduces reported earnings\nBasis Point - 1/100th of a percentage point\nBasis Risk - The risk that an asset and a hedge will not move in opposite directions as expected; “basis” refers to the discrepancy.\nBeta - A measure of a stock’s or a portfolio’s sensitivity to market movements. It quantifies the relationship between the price movements of a security and the overall market. A beta of 1 indicates that the security tends to move in line with the market, while a beta greater than 1 implies that the security is more volatile than the market. A beta less than 1 suggests that the security is less volatile than the market. Often used as a tool to assess the risk or volatility of a particular stock or portfolio in comparison to the overall market\n\nCalculated as the covariance between the rate of return of a company’s stock and the overall market return.\n\nCapital Expenditure (CapEx) - Money that is spent to acquire, repair, update, or improve a fixed company asset, such as a building, business, or equipment. For assets to fall under the CapEx designation, the investments must have a useful life of one year or more. A CapEx is amortized, or its value is deducted a little each year based on the total cost and its expected useful life.\n\nUseful life refers to the estimated and generally agreed upon shelf life of a specific business asset.\n\nAccording the IRS, Car’s useful life is 5yrs and new building’s is 39yrs\n\nAlso see What Is a Capital Expenditure (CapEx)? Definition and Guide (example, calculations, relation to operating expenditure (OpEx)\n\nCash Flow - The net balance of cash moving into (positive) and out (negative) of a business at a specific point in time. Cash flow is not profit. A negative profit means expenses &gt; revenue while a negative cashflow could mean an investment &gt; return in a project. (link)\n\nOperating cash flow: This refers to the net cash generated from a company’s normal business operations. In actively growing and expanding companies, positive cash flow is required to maintain business growth.\nInvesting cash flow: This refers to the net cash generated from a company’s investment-related activities, such as investments in securities, the purchase of physical assets like equipment or property, or the sale of assets. In healthy companies that are actively investing in their businesses, this number will often be in the negative.\nFinancing cash flow: This refers specifically to how cash moves between a company and its investors, owners, or creditors. It’s the net cash generated to finance the company and may include debt, equity, and dividend payments.\nFree cash flow: The net amount of cash left over after taxes are paid; depreciation, amortization, and changes in working capital are accounted for; and capital expenditures (property, equipment, and technology investments) are subtracted. In short: It’s the cash left over that doesn’t need to be allocated anywhere.\n\nCompound Annual Growth Rate (CAGR) - The rate of return (RoR) that would be required for an investment to grow from its beginning value (BV) to its ending value (EV), assuming the profits were reinvested at the end of each period of the investment’s life span (t).\n\\[\n\\text{CAGR} = \\left ( \\left (\\frac{\\text{EV}}{\\text{BV}} \\right ) ^{1/t} - 1 \\right) \\times 100\n\\]\n\nIgnores Time Value of Money.\n\nCost of Carry or Carrying Charge - The cost of holding a security or a physical commodity over a period of time. The carrying charge includes insurance, storage and interest on the invested funds as well as other incidental costs\n\nFor a stock, it’s is the opportunity cost of the capital that goes into it plus the risk you take on for holding it (this is what the idea of risk neutral valuation is based on).\n\nCost of Goods Sold (COGS) or Cost of Sales - Direct costs of producing the goods sold by a company. This amount includes the cost of the materials and labor directly used to create the good. It excludes indirect expenses, such as distribution costs and sales force costs.\nCoupon - Bonds pay a set return each year, called a coupon, over a set period ending on a maturity date. The amound of the coupon depends on the interest rate that was determined when the bond was issued.\nDays Sales Outstanding (DSO) - An accounting metric that measures how long it takes a company to collect payment for goods and services purchased on credit. DSO can also represent the average number of days it takes credit sales to be converted into cash. Lower is better.\n\\[\n\\text{DSO Ratio} = \\frac{\\text{AR}}{\\text{Sales}} \\times 365\n\\]\n\n\\(AR\\) is accounts receivable.\n\nDepreciation - A non-cash expense that approximates the reduction of the book value of a company’s long-term fixed assets or property, plant, and equipment (PP&E) over an estimated useful life (i.e. amount of time where the asset is expected to contribute to company operations) and reduces reported earnings. (see CAPEX for details on Useful Life)\n\nStraight-Line Depreciation assumes a constant depreciation expense over the estimated useful life of the asset\nAccelerated Depreciation assumes the asset loses most of its value in the early years of its life.\nIf two equal companies are using the same depreciation method but have different useful life lengths, then the company with the shorter useful life on its equipment will have a lower EBIT multiple. So, it’s an apples to oranges comparison if your using multiples.\n\nDerivatives - Securities that move in correspondence to one or more underlying assets. They include options, swaps, futures and forward contracts. The underlying assets can be stocks, bonds, commodities, currencies, indices or interest rates. Derivatives can be effective hedges against their underlying assets, since the relationship between the two is more or less clearly defined (if they’re negatively correlated? Or maybe if the underlying asset goes down, there’s a lag between the asset going down and the derivative going down. Therefore, you can sell the derivative before it goes down. Thus, hedging your risk). Knowing the value of an underlying asset helps traders determine the appropriate action (buy, sell, or hold) with their derivative.\nDiscount Rate - Represents a minimum rate of return acceptable to the investor, generally considered to be the investor’s cost of capital.\n\nFor stock dividend valuation (video), the discount rate, \\(k = \\text{90-day T-Bill Rate} + (\\text{Stock Beta} \\times \\text{Stock Market Risk Premium})\\)\n\n\\(\\text{Stock Market Risk Premium}\\) might be the equity risk premium which is the expected return on stocks - risk free rate (e.g. S&P average return rate - 10yr treasury bill rate)\n\nFor project valuation, see Finance, Valuation\n\nDiscounted Cash Flow - Takes the earnings of an investment and discounts each of the cash flows based on a discount rate. The goal of a DCF model is to buy a stream of cashflows (i.e. via a stock) for less than they are worth. It values the cash flows of a company and determines whether they are over or under-valued via the stock price.\nDrawdown - A peak-to-trough decline during a specific period for an investment, trading account, or fund. If a trading account has $10,000 in it, and the funds drop to $9,000 before moving back above $10,000, then the trading account witnessed a 10% drawdown.\n\nThe larger the drawdown (%), the larger uptick (%) needed to get back to initial peak. (e.g. dd of 1% requires 1.01% uptick while 20% dd requires a 25% uptick). Some investors choose to avoid drawdowns of greater than 20% before cutting their losses and turning the position into cash instead.\n\nEarnings or Net Income - Represents after-tax profit. Typically the bottom line of the income statement. EBIT and EBITDA are better for comparing companies.\\(\\text{Earnings} = \\text{Revenue} - \\text{Expenses} - \\text{Interest} - \\text{Taxes}\\)\n\nEarnings Before Interest and Taxes (EBIT) - (aka Operating Income, Operating Profit) Common method for measuring profitability. \\(\\text{EBIT} = \\text{Operating Expenses} - \\text{Sales Revenue}\\) where the sales revenue excludes tax and interest. Located in the company’s income statement. (Also see Profit)\nEarnings Before Interest Tax Depreciation Amortization (EBITDA) - Metric used to analyze and compare pre-tax cash flow of a business. It eliminates non-cash factors (depreciation and amortization) and backs out financing costs (interest). Using net profit (aka earnings) would not be an apples-to-apples comparison. Preferred over EBIT in industries with a high level of fixed assets.\n\nAdjusted EBITDA - Normalizes expenses by adding back any personal and non-business related expenses by owners who include these for tax purposes and isn’t truly a business cost. Used when selling a business.\nSellers Discretionary Earnings - Alternative to Adjusted EBITDA (Video). Adjusted EBITDA more commonly used.\n\nEffective Tax Rate: An average rate of tax payable. Useful to compare individuals or corporations. \\(\\text{Tax Rate}_{\\text{eff}} = \\text{Tax Payed} / \\text{Pre-Tax Earnings}\\) As of 2022, the average (nonfinancial) S&P 500 corporate effective tax rate was around 19%. Contrast with Marginal Tax Rate.\nExchange Traded Fund (ETF) - A mutual fund that may be traded daily like a stock or bond.\nExpenditure - A payment or the incurrence of a liability in exchange for goods or services. Evidence of the documentation triggered by an expenditure is a sales receipt or an invoice. (link, link)\n\nThe key difference between an expense and an expenditure is that an expense recognizes the consumption of a cost, while an expenditure represents the disbursement of funds. An expense is usually recognized when a related sale is recognized or when the item in question has no future utility. An expenditure is usually recognized either when cash is paid out or a liability is incurred. (Also see Expense for an example)\nMy idea is that an expenditure is something coming into the company. It’s the payment for something tangible that is used by the company for a period of time. Although, expenditures can also be a payment to reduce the outstanding balance of a loan, and a payment to distribute dividends to shareholders (¯\\_(ツ)_/¯).\n\nExpense - The reduction in value of an asset as it is used to generate revenue. If the underlying asset is to be used over a long period of time, the expense takes the form of depreciation, and is charged ratably over the useful life of the asset. If the expense is for an immediately consumed item, such as a salary. (link, link)\n\nMy idea of an expense is something that’s leaving a company. It occurs immediately as an asset gets used or sold. When something dissipates, I think of an expense (e.g value of an asset disappearing as depreciation, electricity being used in the building, salary being paid).\nExample: An organization makes an expenditure of $3,000 for a desktop computer. It then charges the computer to expense over the next three years, which results in an annual depreciation expense of $1,000.\n\nFalse Strategy Theorem - Gives the threshold for which a Sharpe Ratio greater than this threshold would be significant.\n\nGiven a sample of estimated performance statistics (e.g. sharpe ratios), \\({S_k}\\) for k = 1, …, K, where each S ∈ N(0, 1) \\[\n        \\mathbb{E}[\\max_{k} {S_k}] \\approx (1-\\gamma) Z^{-1} \\left[1-\\frac{1}{K} \\right] + \\gamma Z^{-1} \\left[{1 - \\frac{1}{Ke}}\\right]\n        \\]\n\n\\(Z^{-1}\\) is the inverse of the standard Gaussian cdf\n\\(e\\) is the exponential constant (i.e. 2.71…)\n\\(\\gamma\\) is the Euler-Mascheroni constant (approx. 0.5772156649…)\n\nUseful for backtesting multiple strategies and deciding whether the strategy with the maximum sharpe ratio is significant (mitigates multiple testing bias)\n\nFixed Asset - Property with a useful life greater than one reporting period (i.e. period covered by finanacial statments), and which exceeds an entity’s minimum capitalization limit (i.e. paid amount threshold to classify it as a long-term asset).\nFutures - An obligation to the buyer and a seller. The seller of the future agrees to provide the underlying asset at expiry, and the buyer of the contract agrees to buy the underlying at expiry. The price they receive and pay, respectively, is the price they entered the futures contract at. Most futures traders close out their positions prior to expiration since retail traders and hedge funds have little need to take physical possession of barrels of oil, for example. But, they can buy or sell the contract at one price, and if it moves favorably they can exit the trade and make a profit that way. Futures are a derivative because the price of an oil futures contract is based on the price movement of oil, for example.\nGross Margin - A measure a company’s profitability as a percentage. Gross Profit is the numerator.\n\\[\n\\text{Gross Margin} = \\frac{\\text{Revenue} - \\text{COGS}}{\\text{Revenue}}\n\\]\nGrowth Rate - \\(r = \\frac{\\text{Ending Value} - \\text{Before Value}}{\\text{Before Value}}\\)\nHedge - An investment that is made with the intention of reducing the risk of adverse price movements in an asset. Normally, a hedge consists of taking an offsetting or opposite position in a related security. An example could be investing in both cyclical and counter-cyclical stocks.\nHedge Ratio (delta) - The effectiveness of a derivative hedge, delta, is the amount the price of a derivative moves per $1 movement in the price of the underlying asset.\nHolding Period Return (HPR) - The total return (%) received from holding an asset or portfolio of assets over a period of time.\n\\[\n\\text{HPR} = \\frac{(\\text{End Value} - \\text{Initial Value}) + \\text{Income}}{\\text{Initial Value}}\n\\]\n\nExample: What is the HPR for an investor who bought a stock a year ago at $50 and received $5 in dividends over the year if the stock is now trading at $60?\n\\[\n\\text{HPR} = \\frac{5 + (60 - 50)}{50} = 0.30 \\;\\text{or}\\; 30\\%\n\\]\nExample: What are the annualized HPRs for a fund A with an HPR = 55% over 3yrs and fund B with an HPR = 65% over 4yrs\n\\[\n(1 + 0.55)^{1/3} - 1 = 15.73\\% \\\\\n(1 + 0.65)^{1/4} - 1 = 13.34\\%\n\\]\nExample: Your stock portfolio had the following returns in the four quarters of a given year: +8%, -5%, +6%, +4%. How did it compare against the benchmark index, which had total returns of 12% over the year?\n\\[\n[(1 + 0.8)\\times (1 - 0.5)\\times (1 + 0.6)\\times (1 + 0.4)] - 1 = 13.1\\%\n\\]\n\nInternal Rate of Return (IRR) - A flawed indicator of strength for capital projects. It should only be used when a project has no interim cashflows or is somehow able reinvest those interim cashflows at the same IRR for the duration of the project (See disadvantages). (Also see MIRR and Finance, Valuation &gt;&gt; Rates of Return)\n\nThe internal rate of return is a discount rate that makes the net present value (NPV) of all cash flows from a particular project or investment equal to zero. In general, projects with higher IRRs are more favorable than projects with lower IRRs, as the expected rate of return on these projects is greater.\n\nLeg - One part or one side of a multistep trade. Legs should be exercised at the same time in order to avoid any risks associated with fluctuations in the price of the related security. So a purchase and sale should be made around the same time to avoid any price risk. Strategy often associated with derivatives trading.\nLimited Liability Corporation (LLC) - A corporation is a business organization that issues stock to its shareholders. A limited liability company is a business organization composed of members with membership interests. a type of legal entity that can be used when forming a business that offers protection to the owner(s) from personal liability for debts and other obligations that a business might incur. In other words, the personal assets of the owner cannot be used for legal claims against the business.The differences don’t really matter much at the taxation or day-to-day corporate level except in scale: LLCs tend to be smaller than corporations (more or less; a lot of people form small business corporations for good reasons and ignorant ones). (See Differences between a LLC and S Corp)\nMarginal Tax Rate: The maximum percentage of income tax that anyone is liable to pay in a system that applies tax burdens to people depending on their respective actual taxable incomes (i.e your tax bracket). As of 2023, the corporate tax rate is 25%.\nMarket Capitalization - The total dollar market value of a company’s outstanding shares. Calculated by multiplying the total number of a company’s shares by the current market price of one share.\nModified Internal Rate of Return (MIRR) - Better alternative to IRR that assumes the cash inflows are reinvested at cost of capital of the company instead of at the IRR. (Also see Finance, Valuation &gt;&gt; Rates of Return)\nMomentum - The rate of acceleration of a security’s price—that is, the speed at which the price is changing. In general, Momentum = Today’s price - Price from X days ago. Positive: bullish, Negative: bearish. More sophisticated indicators can be calculated, see https://www.investopedia.com/terms/m/marketmomentum.asp\n\n12-Month Momementum:\n\\[\nM = \\frac{\\text{Closing Price}_{\\text{end of month}}- \\text{Closing Price}_{\\text{12 months ago}}}{\\text{Closing Price}_{\\text{12 months ago}}}\n\\]\n\nNet Present Value - A way to compare potential investments or projects in today’s dollars. It’s the difference between the present value of a future stream of cash inflows and outflows. Used in capital budgeting and investment planning to analyze the profitability of a projected investment or project. (Also see Finance, Valuation &gt;&gt; Rates of Return &gt;&gt; AIRR &gt;&gt; Formula Using Time-Varying Cost of Capital)\n\\[\n\\text{NPV} = F_0 + \\frac{F_1}{1+r} + \\cdots + \\frac{F_n}{(1+r)^n}\n\\]\n\n\\(F_*\\): Cash Flows\n\\(r\\): Cost of Capital\n\\(n\\): Last period of the project\n\nOpen, High, Low, Close (OHLC) - Data that shows open, high, low, and closing prices for each period. OHLC charts are useful since they show the four major data points over a period, with the closing price being considered the most important by many traders.\nOptions - An option on stock XYZ gives the holder the right to buy or sell XYZ at the strike price up until expiration. The underlying asset for the option is the stock of XYZ. The writer must either buy or sell the underlying asset to the buyer on the specified date at the agreed-upon price. The buyer is not obligated to purchase the underlying asset, but they can exercise their right if they choose to do so. If the option is about to expire, and the underlying asset has not moved favorably enough to make exercising the option worthwhile, the buyer can let the expire and they will lose the amount they paid for the option.\n\nCall options - (Useful if you think the price might go up) Contract giving the owner the right, but not the obligation, to buy a specified amount of an underlying security at a specified price within a specified time. The specified price is known as the strike price and the specified time during which a sale is made is its expiration (expiry) or time to maturity. As the price of the stock goes up, the value of the call option contract goes up. The contract can be sold at any time or you can purchase the stock at the guaranteed price on the expiration date. The price of the call option is called the premium.\n\nIf Apple is trading at $110 at expiry (aka expiration date), the strike price is $100, and the options cost the buyer $2, the profit is $110 - ($100 +$2) = $8. If the buyer bought one contract that equates to $800 ($8 x 100 shares), or $1,600 if they bought two contracts ($8 x 200). If at expiry Apple is below $100, then the option buyer loses $200 ($2 x 100 shares) for each contract they bought.\nSuppose that Microsoft shares are trading at $108 per share. You own 100 shares of the stock and want to generate an income above and beyond the stock’s dividend. You also believe that shares are unlikely to rise above $115.00 per share over the next month. You take a look at the call options for the following month and see that there’s a 115.00 call trading at $0.37 per contract. So, you sell one call option and collect the $37 premium ($0.37 x 100 shares), representing a roughly four percent annualized income. If the stock rises above $115.00, the option buyer will exercise the option and you will have to deliver the 100 shares of stock at $115.00 per share. You still generated a profit of $7.00 per share, but you will have missed out on any upside above $115.00. If the stock doesn’t rise above $115.00, you keep the shares and the $37 in premium income.\n\nPut option - (Useful if you think the price might go down) If Morty buys 100 shares of Stock plc (STOCK) at $10 per share, he might hedge his investment by buying an American put option with a strike price of $8 expiring in one year. This option gives Morty the right to sell 100 shares of STOCK for $8 any time in the next year. Let’s assume he pays $1 for the option, or $100 in premium. If one year later STOCK is trading at $12, Morty will not exercise the option and will be out $100. He’s unlikely to fret, though, since his unrealized gain is $100 ($100 including the price of the put). If STOCK is trading at $0, on the other hand, Morty will exercise the option and sell his shares for $8, for a loss of $300 ($300 including the price of the put). Without the option, he stood to lose his entire investment.\nVanilla Option - The most basic, liquid, and commonly traded form of options contracts — backbone of the options market. They refer to standard call options and put options with the following characteristics:\n\nCan have either the European-style (expiration date only) or American-style (anytime until expiration) exercise feature.\nWritten on common underlying assets like stocks, stock indices, currencies, or commodities.\nDo not have any special features or conditions attached to them such as barrier options, Asian options, or options with path-dependent payoffs.\nThey can be settled either through physical delivery of the underlying asset or cash settlement, depending on the specific contract.\n\nExample: From Veritasium\n\n\nStock trading: Buy a stock at $100\n\nPrice goes down to $70, lose $30 (-30%)\n\nThere is also the possibilty the price goes to $0 and you lose your investment entirely before you have a chance to sell.\n\nPrice goes up to $130, gain $30 (+30%)\n\nOptions Trading: Buy the $100 stock’s option for $10 at a strike price of $100\n\nPrice goes down to $70, lose $10 (price of option)(-100%)\nPrice goes up to $130, gain $20 (+200%) by selling the option for $30 and pocketing the profit ($30 - $10).\n\nHedging: Stock price is $100 and call/put option is $10 at a strike price of $100\n\nBuy the Call Option because you expect to price to increase. Then stock increases to $130, you exercise your option to buy the stock at $100 strike price. You sell the stock at $130 and make a profit of \\(\\$130-(\\$100 + \\$10) = \\$20\\).\n\nAlternatively, if the stock decreases in price, you don’t exercise your option and lose $10.\n\nBuy the Put Option because you expect the price to decrease. The stock decreases to $70 and you buy the stock. Then, you exercise your option to sell the stock at $100 and make a profit of \\(\\$100 - (\\$70 + \\$10) = \\$20\\).\n\nAlternatively, if the stock increases in price, you don’t exercise your option and lose $10.\n\n\n\n\nOutstanding Shares - The number of stocks that a company has issued. This number represents all the shares that can be bought and sold by the public, as well as all the restricted shares that require special permission before being transacted.\nProfit - The balance that remains when all of a business’s operating expenses are subtracted from its revenues. (link)\n\nGross profit: Gross profit is defined as revenue minus the cost of goods sold. It includes variable costs, which are dependent upon the level of output, such as cost of materials and labor directly associated with producing the product. It doesn’t include other fixed costs, which a company must pay regardless of output, such as rent and the salary of individuals not involved in producing a product.\nOperating profit: Like operating cash flow, operating profit refers only to the net profit that a company generates from its normal business operations. It typically excludes negative cash flows like tax payments or interest payments on debt. Similarly, it excludes positive cash flows from areas outside of the core business. It’s sometimes referred to as earnings before interest and tax (EBIT).\nNet profit: This is the net income after all expenses have been deducted from all revenues. Typically, this includes expenses like tax and interest payments.\n\nRate of Return (RoR) - The net gain or loss of an investment over a specified time period, expressed as a percentage of the investment’s initial cost. The rate of return disregards some key factors in an investment, like the time value of money, the timing and size of cash flows, and the risk and uncertainty associated with any investment or in the case of stocks — taxes and investing fees. (See Internal Rate of Return, MIRR, and Finance, Valuation &gt;&gt; Rates of Return)\nRelative Performance - The price ratio of two stocks\nReturn on Investment (ROI) - \\(\\text{ROI} = \\frac{\\text{Net Profit}}{\\text{Investment}}\\) where investment can include working capital, R&D, capital spending, acquisitions, etc.\nRevenue aka Sales - The income a company generates before deducting expenses. Usually the top line of the income statement\nSelling, General, and Administrative Expenses (SG&A) - Includes all general and administrative expenses (G&A) as well as the direct and indirect selling expenses of the business. This line item includes nearly all business costs not directly attributable to making a product or performing a service. SG&A includes the costs of managing the company and the expenses of delivering its products or services.\nSharpe Ratio (annualized) - Measures the performance of an asset relative to violitility (i.e. riskiness)\n\n(expected excess returns relative to a risk free asset (e.g. treasury bond) / sd of those expected excess returns) * √number_of_observations_in_a_year\nSharpe ratios above 1.0 are generally considered “good,” as this would suggest that the portfolio is offering excess returns relative to its volatility\n\nEven if your sharpe ratio is above 1 it may not be good if it is below the average sharpe ratio of peer group portfolios.\n\nMultiplying by √number_of_observations_in_a_year makes “annualizes” the sharpe ratio and makes sharpe ratios comparable\nShould NOT be thought of as t-stats for testing significance of the sample mean (i.e. p-values for estimates) since it doesn’t account for the number of observations\n\nSee Sharpe Ratio (deflated)\nInvestment professionals often use a rule of thumb of dividing the sharpe ratio by 2 when backtesting to avoid overfitting, there is no statistical basis for this.\n\n\nSharpe Ratio (probabilistic) - Allows you test the significance of the Sharpe Ratio under assumptions of ergodicity and stationarity (Paper)\nSharpe Ratio (deflated) - The probability that an observed Sharpe Ratio was drawn from a distribution with positive mean after controlling for sample size (aka backtest length), skewness, kurtosis, and number of strategy variations explored.\n\nCombines probabilistic sharpe ratio and false strategy theorem\nPaper: The Deflated Sharpe Ratio: Correcting for selection bias, backtest overfitting and non-normality\n\n\nShows that if a strategy has a maximum sharpe ratio of 1 but had 3 variations backtested, it’s deflated sharpe ratio drops below the 95% CI for a sharpe ratio = 1.\n\nReturns from investment strategies often exhibit autocorrelation, fat tails, and negative skewness which further “deflates” the deflated sharpe ratio\n\nSlippage- The difference between where the computer signaled the entry and exit for a trade and where actual clients, with actual money, entered and exited the market using the computer’s signals. Along with transaction costs, it’s a cost that needs to be taken into account when analyzing the profitability of a strategy.\nSpread (Bid-Ask): The difference between two prices, rates or yields.\n\nThe gap between the bid and the ask prices of a security or asset, like a stock, bond or commodity.\nThe gap between a short position (that is, selling) in one futures contract or currency and a long position (that is, buying) in another. This is officially known as a spread trade\n\nStochastic Volatility Model - Used to describe the behavior of the volatility of an underlying asset or security over time. Unlike traditional models, which assume that volatility is constant or follows a deterministic process, SV models treat volatility as a random process itself, allowing it to fluctuate randomly over time.\n\nThe volatility of the asset is modeled as a separate stochastic (random) process, often following a mean-reverting process, such as an Ornstein-Uhlenbeck process or a GARCH process. This means that the volatility is assumed to fluctuate randomly around a long-term mean level, with deviations from the mean being temporary and eventually reverting back to the mean over time.\nClaude chat discussing how this model fits into an options pricing workflow\n\nTime Value of Money - Money that isn’t invested loses buying power over time (i.e. inflation). The concept of the time value of money can help guide investment decisions (e.g. a project with a $1M payout in year 1 has a higher present value than one that pays the same amount in 5yrs)\n\nAnnual\n\\[\n\\text{FV} = \\operatorname{PV} \\left( 1 + i \\right)^{\\large{t}}\n\\]\n\nVariables\n\n\\(FV\\): Future value of money\n\\(PV\\): Present value of money\n\\(i\\): Annual Interest Rate\n\\(t\\): Number of years\n\n\nQuarterly or other periods\n\\[\n\\text{FV} = \\operatorname{PV} \\left( 1 + \\frac{i}{n} \\right)^{\\large{n \\times t}}\n\\]\n\n\\(n\\): Number of compounding periods in a year\n\nIn the case of annuity or perpetuity payments, the generalized formula has additional or fewer factors (See wiki, Investopedia)\n\nVolatility - Typical measured by the standard deviation of a stock over a specified time period. A higher standard deviation indicates higher volatility, meaning larger price swings in either direction. Recommended to compare the volatility of a specific stock to its historical levels and the overall market to understand its relative risk.\n\nAverage True Range (ATR): This method focuses on the absolute size of price changes, regardless of direction. It calculates the average difference between the highest and lowest price of a specific period for each observation. Typically calculated using a 14-period moving average.\n\nSteps\n\nCalculate the True Range (TR) over each period (1-day) which is the greatest value of either of the following\n\nCurrent High - Current Low\nAbsolute Value (Current High - Previous Close)\nAbsolute Value (Current Low - Previous Close)\n\nCalculate the moving average of the TRs.\n\n\nBollinger Bands - These bands are drawn around a stock’s moving average, with the width of the bands reflecting volatility. Wider bands represent higher volatility, suggesting larger potential price swings.\nImplied Volatility - This measure is derived from option prices and reflects the market’s expectation of future volatility for a particular stock. Higher implied volatility signifies greater uncertainty and possible larger price movements.\nParkinson Range (PR) - Uses the naturnal logarithm to account for the uses the natural logarithm to account for the non-linear nature of price movements. Higher PR means more volatility. Often used in conjunction with other volatility measures like Average True Range (ATR) to provide a more comprehensive view of price fluctuations. \\(\\text{PR} = \\ln(\\text{closing price}) - \\ln(\\text{opening price})\\)\n\nWeighted Average Cost of Capital (WACC) - The weighted cost of a company’s invested capital (both debt and equity). Most commonly used as a discount rate. (Also see Finance, Valuation)\nWorking Capital or Net Working Capital (NWC) - A measure of a company’s liquidity, operational efficiency, and short-term financial health. Also, its a measure of how much cash a company needs to fund its operations on an ongoing basis. The NWC is the difference between a company’s non-cash/cash equivalent current assets and its non-interest-bearing current liabilities (i.e. no long term debt)\n\nCurrent assets include accounts receivable (A/R), inventory, prepaid expenses, and other assets that are expected to be liquidated or turned into cash in less than one year.\nCurrent liabilities include accounts payable (A/P), wages, taxes payable, and the current portion of long-term debt that’s due within one year.\nAn increase in year-to-year NWC is typical for a growing company wishing to support sales growth while a decrease represents a source of cash that is retained as opposed to being paid out.\n\nYield - A return measure for an investment over a set period of time, expressed as a percentage.\n\nIncludes price increases as well as any dividends paid, calculated as the net realized return divided by the principal amount (i.e. amount invested).\nHigher yields are perceived to be an indicator of lower risk and higher income, but a high yield may not always be a positive, such as the case of a rising dividend yield due to a falling stock price.",
    "crumbs": [
      "Finance",
      "Glossary"
    ]
  },
  {
    "objectID": "qmd/finance-investing.html",
    "href": "qmd/finance-investing.html",
    "title": "Investing",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Finance",
      "Investing"
    ]
  },
  {
    "objectID": "qmd/finance-investing.html#sec-finance-invest-misc",
    "href": "qmd/finance-investing.html#sec-finance-invest-misc",
    "title": "Investing",
    "section": "",
    "text": "Resources\n\nTidy Finance - R and Python versions\nAdvances in Financial Machine Learning - de Prado (R &gt;&gt; Documents &gt;&gt; Financial)\nMachine Learning for Algorithmic Trading\n\nGithub repo with the code that’s referred to in the book\n\n\nPackages\n\nCRAN Task View\n{{zipline-reloaded}} - Backtesting and live-trading engine by crowd-sourced investment fund Quantopian\n{{pyfolio}} - Performance and risk analysis for financial portfolios\n{{skfolio}} - Portfolio optimization built on top of scikit-learn. It offers a unified interface and tools compatible with scikit-learn to build, fine-tune, and cross-validate portfolio models.\n{{alphalens}} - Performance analysis of predictive stock factors\n\nOpenBB - Open source bloomberg terminal (overview article)\n\nFree data and seemingly hundreds of different tools including some decent forecast methods.\n\nTechnical vs Fundamental Analysis\n\nTechnical analysis in many cases tends to amplify stock price trends because many technical traders buy when stocks are rising and sell when they fall.\nFundamental analysis on the contrary often shows mean-reverting behaviour because its adherents try to buy undervalued stocks and sell overvalued ones\n\nSmoothers\n\nSimple Moving Average (SMA) is simple, easy to implement but reacts slowly to recent price changes due to equal weighting of all prices in the window.\nExponential Moving Average (EMA) reacts faster to recent price changes due to more weight to the latest prices.\nKalman Filter is more sophisticated and can adaptively weigh incoming measurements based on their estimated noise level, potentially providing superior results in many applications. However, it has key parameters that must be tuned to get good results.\n\nStrategies\n\nHedgefund - factor rotations (longer term)\nSomething about futures - factor tilts (shorter term)\n\nMisleading chart (Thread)\n\n\n\nNo transaction costs\nNo fees\nNo taxes\nUS “exceptionalism”\nOverlapping time period analysis\n\nExample: if I look at 1920-1950 and then 1921-1951, twenty eight of the thirty data points are overlapping. So there’s not a lot of “unique” data in this set.\n\nNominal vs real returns - Time weighted vs dollar weighted returns\n\nWorkflow\n\n\n\nFrom Machine Learning for Algorithmic Trading\n\n\nAny market forecasting model, especially involving short term horizons, should probably have a random walk in an ensemble. (Is the Market Just Random Noise?)",
    "crumbs": [
      "Finance",
      "Investing"
    ]
  },
  {
    "objectID": "qmd/finance-investing.html#sec-finance-invest-consid",
    "href": "qmd/finance-investing.html#sec-finance-invest-consid",
    "title": "Investing",
    "section": "Considerations",
    "text": "Considerations\n\nCosts that need to be accounted for when backtesting\n\nTransaction costs\nFees\nSlippage\nTaxes\n\nA model that performs well in a specific market condition may incur substantial losses when the market shifts, be it due to news, events, or economic changes.\n\nSee Finance, Snippets &gt;&gt; Misc &gt;&gt; Test Assumptions\n\nBeware of survivorship bias when backtesting algorithms\n\nExample: S&P 500\n\nA trader is creating an algorithm to predict prices of all the stocks in the S&P 500. If the trader uses the current roster of companies, the list only includes companies that have made it to now without shutting down or losing so much value they drop off the list. The trader should use the list of companies as it was at the start of the training data.\n\n\nUse proper stop-loss and position sizing strategies to manage risk\nIteratively using the same data to design and modify a strategy can lead to overfitting, where the strategy becomes overly tailored to the data and performs poorly in real-world trading.\nIgnorance of macro-economic indicators and trends can result in unexpected market moves, significantly impacting algorithmic strategies and their performance.\\\nThere are 435 choices for start and end dates of each monthly investment cycle\n\ni.e. Easy to have a selective endpoints fallacy for an investment strategy.",
    "crumbs": [
      "Finance",
      "Investing"
    ]
  },
  {
    "objectID": "qmd/finance-investing.html#sec-finance-invest-6040",
    "href": "qmd/finance-investing.html#sec-finance-invest-6040",
    "title": "Investing",
    "section": "60/40 Portfolio Strategy",
    "text": "60/40 Portfolio Strategy\n\nMisc\n\n{{QSTrader}}\nNotes from - The 60/40 Benchmark Portfolio\nFor portfolio construction in general, positions are optimized using Expected Returns, Return Variance (i.e. volatility), and Co-Movements (i.e. covariance) in order to maximize Risk Adjusted Returns.\n\nDescription\n60/40 US Equities/Bonds strategy is a simple long-term investment approach that is widely utilised in the investment industry. It seeks to ensure that at any point during the lifetime of the investment that 60% of account equity is invested in one or more assets representing a broad selection of US equities (such as an S&P500 ETF), while 40% of account equity is invested in one or more assets representing a broad selection of US treasury bonds (such as a treasury bond ETF).\nSince the actual percentage allocations of each asset class can deviate over time due to relative growth of the respective assets a ‘rebalance’ approach is often carried out. This means that trades are issued on a relatively infrequent basis to buy/sell amounts of each asset class to periodically bring the account equity allocations back into the 60/40 split. For the particular strategy implemented here we are using an end of month rebalance frequency.",
    "crumbs": [
      "Finance",
      "Investing"
    ]
  },
  {
    "objectID": "qmd/finance-investing.html#sec-finance-invest-momassallo",
    "href": "qmd/finance-investing.html#sec-finance-invest-momassallo",
    "title": "Investing",
    "section": "Momentum Tactical Asset Allocation Strategy",
    "text": "Momentum Tactical Asset Allocation Strategy\n\nMisc\n\n{{QSTrader}}\nNotes from\n\nSystematic Tactical Asset Allocation: An Introduction - Tutorial\n\n\nDescription\n\nThe US sector momentum strategy is a long-only dynamic tactical asset allocation strategy that attempts to exceed the performance of simply going long the S&P500.\nAt the end of every month the strategy calculates the holding period return (HPR) based momentum of all of the SPDR sector ETFs (the ticker symbols of which begin with the prefix XL) and selects the top N to invest in for the forthcoming month, where N is usually between 3 and 6.\nIn the implementation given here the HPR momentum is calculated over the previous 126 days (approximately six months of business days) and the top three sector ETFs are chosen for the portfolio.\nThe portfolio allocation is equally weighted between each of these three sector ETFs. Irrespective of changes in signal the portfolio is rebalanced once per month to weight the assets equally.",
    "crumbs": [
      "Finance",
      "Investing"
    ]
  },
  {
    "objectID": "qmd/finance-investing.html#dual-momentum-sector-rotation",
    "href": "qmd/finance-investing.html#dual-momentum-sector-rotation",
    "title": "Investing",
    "section": "Dual Momentum Sector Rotation",
    "text": "Dual Momentum Sector Rotation\n\nNotes from Simulation of Gary Antonacci’s Dual Momentum Sector Rotation Strategy\nCharacteristics\n\nConditions for investing offensively or defensively: the 12-month momentum of the S&P500 (absolute mom)\nList of offensive investment vehicles: the 11 sectors that make up the S&P500 index (relative strength)\nNumber of sectors to be selected when the strategy is invested offensively: 4\nSector selection criteria: 12-month momentum\nList of defensive investment vehicles: US Aggregate Global Bond or Money Market (Cash)\nNumber of instruments to be selected when the strategy is invested defensively: 1\n\nProcess\n\n12-month momentum of the S&P500 used to decide whether the market is in an uptrend or a downtrend, i.e. to invest offensively or defensively.\nIf the S&P500 is in an uptrend, invest offensively. In this case, the allocation is spread evenly over the 4 sectors with the highest 12-month momentum, 25% each.\n\nIf fewer than 4 sectors have positive momentum, then the remainder not invested offensively is allocated to a defensive position.\n\nIf the S&P500 is trending downwards (negative momentum), I invest defensively. In this case, I allocate the entire portfolio to US government bonds or the money market, depending on which market has offered the highest returns over the last 12 months.\n\nThose familiar with the GEM strategy will note that the author initially recommended using only US government bonds as a defensive asset. However, since interest rates have fallen so much over the last 10 years, since the publication of his study, it now seems more reasonable to compare the US government bond market with the performance of the money market, in order to select the market that offers the best risk/return trade-off.\n\nCalculate 12-month momentum: ( Price at the end of the month – Price 12 months ago ) / ( Price 12 months ago )\nMomentum calculations and any arbitrages are carried out at the end of each month.\n\nComparison\n\nCumulative Performance: DMSR vs. SPY vs. GEM\n\n\nSimple Cumulative Performance (%) is computed every month (I think)\n\nRate of Return\n\\[\n\\text{RoR} = \\frac{\\text{Ending Value} - \\text{Starting Value}}{\\text{Starting Value}} \\times 100\n\\]\nCumulative Performance (CP)\n\\[\n\\text{CP} = (1 + \\text{RoR}_1) \\times (1 + \\text{RoR}_2) \\times \\cdots \\times (1 + \\text{RoR}_n) - 1\n\\]\n\n\nStatistics",
    "crumbs": [
      "Finance",
      "Investing"
    ]
  },
  {
    "objectID": "qmd/finance-investing.html#sec-finance-invest-vtp",
    "href": "qmd/finance-investing.html#sec-finance-invest-vtp",
    "title": "Investing",
    "section": "Volatility Targeting Portfolio",
    "text": "Volatility Targeting Portfolio\n\nMisc\n\nThe Impact of Volatility Targeting (paper)\nQuant Science article uses {{riskfolio}} to generate volatility weights for equities. (code)\n\n{{riskfolio}} - Good docs with plenty of examples\n\n\nVolatility targeting seeks to counter the fluctuations in volatility:\n\nIt leads to leveraging a portfolio at times of low volatility, and scaling down exposures at times of high volatility.\nThis approach targets a constant level of volatility, rather than a constant notional exposure.\n\nVolatility clustering is a key feature of financial asset returns:\n\nHigh volatility over the recent past tends to be followed by high volatility in the near future.\n\nImpact on Sharpe ratio\n\nVolatility targeting improves the Sharpe ratio of “risk assets” (equities and credit), and that of “balanced” and “risk parity” portfolios that have a substantial allocation to these risk assets.\nFor equity and credit, volatility targeting effectively introduces some momentum overlay due to the so-called leverage effect: the negative relationship between returns and changes in volatility.\nIn contrast, for bonds, currencies, and commodities the impact on the Sharpe ratio is negligible.\n\nImpact on likelihood of tail events\n\nVolatility targeting reduces the likelihood of extreme returns for all asset classes.\nImportantly, “left-tail” events tend to be less severe, as they typically occur at times of elevated volatility, when a target-volatility portfolio has a scaled-down notional exposure.",
    "crumbs": [
      "Finance",
      "Investing"
    ]
  },
  {
    "objectID": "qmd/finance-investing.html#sec-finance-invest-meanrev",
    "href": "qmd/finance-investing.html#sec-finance-invest-meanrev",
    "title": "Investing",
    "section": "Mean Reversion Strategy or Pairs Trading",
    "text": "Mean Reversion Strategy or Pairs Trading\n\nPapers\n\nPairs Trading Using a Novel Graphical Matching Approach\n\nApplication of graph theory to the selection of stock pairs. Results in robust risk-adjusted performance but also achieves annualized returns comparable to the S&P 500 with a superior Sharpe ratio\n\nSelection based solely upon the significance of cointegration between pairs can result in portfolios overly concentrated in a limited number of stocks, which elevates portfolio variance and diminishes risk-adjusted returns\n\nUtilizes of graph theory to the selection of stock pairs to decrease variance with the t-statistic from a cointegration statistical test as the weight for the edges.\n\n\nMatch two trading vehicles that are highly correlated, trading one long and the other short when the pair’s price ratio diverges “x” number of standard deviations - “x” is optimized using historical data. If the pair reverts to its mean trend, a profit is made on one or both of the positions.\nChart the relative performance (price ratio)\n\nThe center white line represents the mean price ratio over the past two years. The yellow and red lines represent one and two standard deviations from the mean ratio, respectively.\nThe potential for profit can be identified when the price ratio hits its first or second deviation. When these profitable divergences occur it is time to take a long position in the underperformer and a short position in the overachiever. (i.e. shorting the stock that’s rising in relation to the other, and buying the other stock that hasn’t risen yet)\nThe revenue from the short sale can help cover the cost of the long position, making the pairs trade inexpensive to put on.\nPosition size of the pair should be matched by dollar value rather than number of shares; this way a 5% move in one equals a 5% move in the other.\nAs with all investments, there is a risk that the trades could move into the red, so it is important to determine optimized stop-loss points before implementing the pairs trade.\n\nChart the spreads between returns\nLook for stocks that move closely together and their:\n\nSpreads zig-zag around zero\nPrices separately (marginally) are normally distributed and joint-normally distributed (assumes linear correlation)\n\nIf both, then they move up AND down together (symmetric)\nIf only marginally, then they probably only move in one direction together (asymmetric)\n\n\nIf your two assets returns (differenced prices are stationary) do not correlate within a [0.7..0.8] correlation coefficient range at least 70% of the times (70% of all observations) then you’re probably dealing with a very bad hedging instrument\n\nLook to cover the remaining data points and research why correlations broke down during those periods. If you can derive a mathematical relationship you could possibly formulate an approach in which you make adjustments to the hedge ratio through an adjustment in your beta/correlation coefficient.\nFigure out the optimal time interval to retrain model due to data drift\nRolling 20-day correlation is a common way to monitor stock correlation\n\nBeware of two correlated stocks that have similar Beta or some other confounding variable\n\nGraphical LASSO can help remove effects such as market Beta and recover real, direct relationships between stocks.\n\nIt makes intuitive sense that in a large universe, most stocks would be conditionally independent, and Graphical LASSO is able to refine this sparsity condition by tuning it’s only parameter\nSee Association, General &gt;&gt; Partial Correlation for a stock example\n\n\nCopulas\n\nSee Association, Copulas\nMeasures non-linear association\nFor Value-at-Risk (VAR) calculations, Gaussian copula is overly optimistic and Gumbel is too pessimistic\nCopulas with upper tail dependence: Gumbel, Joe, N13, N14, Student-t.\nCopulas with lower tail dependence: Clayton, N14 (weaker than upper tail), Student-t.\nCopulas with no tail dependence: Gaussian, Frank.\n\nCointegration allows us to construct a stationary time series from two asset price series, if only we can find the magic weight, or more formally, the cointegration coefficient β. Then we can apply a mean-reversion strategy to trade both assets at the same time weighted by β. There is no guarantee that such β always exists, and you should look for other asset pairs if no such β can be found.\n\nSee Forecasting, Statistical &gt;&gt; Terms\nCointegrated assets share common nonstationary components, which may include trend, seasonal, and stochastic parts\nMight have low correlation, and highly correlated series might not be cointegrated at all.\nDescribes a long-term relationship between the prices (correlation describes a short-term relationship between the returns). The resulting stationary series is the spread between the prices of both assets\nShould have similar risk exposure so that their prices move together\nGood candidates for cointegrated pairs could be:\n\nStocks that belong to the same sector.\nWTI crude oil and Brent crude oil.\nAUD/USD and NZD/USD.\nYield curves and futures calendar spreads",
    "crumbs": [
      "Finance",
      "Investing"
    ]
  },
  {
    "objectID": "qmd/finance-investing.html#sec-finance-invest-bonds",
    "href": "qmd/finance-investing.html#sec-finance-invest-bonds",
    "title": "Investing",
    "section": "Bonds",
    "text": "Bonds\n\nMisc\n\nResources\n\nNerdWallet Compound Interest Calculator\n\nExample: if you put $5,000 in a (5yr) bond with a 4% yield, assuming you reinvest your interest payments, you will have $6083 by the time it matures.\n\n\n\nNotes from\n\nIt’s a Good Time to Buy Bonds. Just Know What You’re Getting Into.\n\nThe price of a bond moves in the opposite direction to interest rates.\n\nAs rates rise, the value of older bonds with lower coupons declines. \n\nMost often, people will compare the 2-year Treasury with the 10-year. So, any generic mention of the spread, in a bond context, refers to the difference in yield between these two bonds.\nBond traders buy existing bonds in the secondary bond market, and sell them at a discount to their face value.\n\nThe amount of the discount depends partially on how many payments are still due before the bond reaches maturity.\nThe price also is a bet on the direction of interest rates. If a trader thinks interest rates on new bond issues will be lower, the existing bonds may be worth a little more.\n\nZero-Coupon Bonds (aka Discount Bonds)\n\nOwners receive no payments for their money until the bond has matured.\nThey buy the bond for an amount that is less than its face value. When it reaches its maturity, they are paid the full face value of the bond.\nMost zero-coupon bonds have a pre-set face value and therefore pay a pre-set amount of money at maturity.\n\nSome bonds are inflation-indexed, meaning the face value is determined at maturity. The amount paid will be based on a standard measure such as the consumer price index plus a premium.\n\nConsidered short-term investments that typically have a maturity of no more than one year. These short-term bonds are usually called bills.\n\n\nQuestions\n\nShould you buy the highest-yielding bonds, or is it better to lock in good rates for longer?\nIs it better to buy bonds directly from the government, a brokerage or as part of a bond fund?\nHow much do I make on bonds after taxes and fees?\nHow many bonds should I buy?\n\nShort-Term Bonds (e.g. 2-year Treasury)\n\nPros:\n\nHigher yields (i.e. return) than long term bonds (e.g. 10-year Treasury)\n\nCons:\n\nWhen interest rates are high, you lose the opportunity (i.e. by not buying longer term bonds) to lock in a potentially higher return for longer should rates drop in the future.\n\n\nBond Ladder\n\nDivide your investments among bonds with progressively later maturity dates.\n\nInvestors should also choose bonds based on savings goals and how soon they will need the money\n\nThis takes advantage of both higher short term coupons and longer term yields.\n\nConsiderations\n\nCheck a bond’s credit ratings before you buy. A poor rating means the bond issuer, such as a corporation, is less likely to pay you back.\n\nThis can be enticing because the issuer will likely offer you a higher interest rate. It also means they are more likely to default, which would mean you lose money.\n\nWatch out for hidden expenses like markup fees for purchasing an individual bond\n\nCan be anywhere from 1% to 5%, depending on the broker.\n\nYou are often not buying directly from the issuer. Instead, you are buying from another investor and the price of that bond probably has shifted since it was initially issued.\n\nOne advantage of using a brokerage is that their sites will take the guesswork out of how much the change in the market price of your bond will affect your final return.\n\nTaxes\n\nCD interest, which may at time near bond yields, is taxed as ordinary income, as high as 37%.\nThe interest on Treasurys is exempt from state taxes.\nMunicipal bonds are exempt from both state and local taxes if an investor buys the bond from the same state they live in.\nCorporate bonds have no tax benefits, so investors still need to pay both state and federal taxes on them.\nBonds within a tax-sheltered account like a 401(k) or an IRA aren’t taxed until you withdraw the money\nIf a zero-coupon bond is issued by a U.S. local or state government entity, interest is free from federal tax and generally exempt from state and local tax as well.\n\nZero-Coupon bonds subject to taxation in the U.S. can be held in a tax-deferred retirement account, allowing their investors to avoid tax on future income.\n\n\n\nPurchasing\n\nTreasurys and most other bonds through a brokerage such as Charles Schwab, Fidelity or Vanguard.\nGovernment bonds can also be bought directly from TreasuryDirect, a government website.\nBond funds and ETFs allow you to diversify your investment.\n\nBonds funds often don’t hold bonds until maturity.\n\n\n\n\n\n\nFrom Machine Learning for Algorithmic Trading",
    "crumbs": [
      "Finance",
      "Investing"
    ]
  },
  {
    "objectID": "qmd/finance-snippets.html",
    "href": "qmd/finance-snippets.html",
    "title": "Snippets",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Finance",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/finance-snippets.html#sec-finance-snip-misc",
    "href": "qmd/finance-snippets.html#sec-finance-snip-misc",
    "title": "Snippets",
    "section": "",
    "text": "Notes from\n\nTidy Finance and Accessing Financial Data (slides, video)\n\nCalculate Returns from Stock Prices\n\nRPython\n\n\nreturns &lt;- \n  prices |&gt;\n  group_by(symbol) |&gt;\n  mutate(ret = adjusted / lag(adjusted) - 1) |&gt;\n  select(symbol, date, ret) |&gt;\n  drop_na(ret)\n\n\nall_returns = (\n  index_prices\n  .assign(ret=lambda x: x.groupby(\"symbol\")[\"adjusted\"].pct_change())\n  .get([\"symbol\", \"date\", \"ret\"])\n  .dropna(subset=\"ret\")\n)\n\n\n\nBasic Calculation of Volatility\nVTI &lt;- VTI %&gt;%\n  # calculate volatility as pct range\n  mutate(range_pct = (high/low - 1) * 100)\n\nVTI is daily OHLC data\nVolatility as a percentage range between daily highs and daily lows\n\nTest whether assumptions are consistent over a time period\n\nNotes from How to Test the Assumption of Persistence\nIf your strategy only works when certain conditions are present, then you need to see how consistent they are across time.\nDon’t use overlapping windows (i.e. rolling) in your analysis of persistence. It creates relationships between subsequent daily estimates of your statistic.\nUse Case Examples\n\nCorrelation or covariance between pairs of assets.\n\nYou’ll typically find that correlations are also persistent, but that this persistence is much noisier than than the volatility example below\n\nAsset Returns\n\nYou’ll typically find almost no persistence at all, other than a very weak negative relationship at the daily timescale in some assets.\n\n\nExample: How persistent is volatility over time?\n\nStatistically, you can calculate the ACF. For relationships further back than one day, ACF(2), ACF(3), etc.\n\n\nOverall Present Volatility vs 1 Day LeadOver YearsAcross ETFs\n\n\n\nVTI &lt;- \n  VTI %&gt;%\n  # calculate volatility as pct range\n  mutate(range_pct = (high/low - 1) * 100) %&gt;%\n  # ensure our observations are arranged by date\n  arrange(date) %&gt;%\n  # shift our observations\n  mutate(next_day_range_pct = lead(range_pct, 1)) %&gt;%\n  # remove NAs (we'll have an NA on our last observation as there's no tomorrow)\n  na.omit()\n\nVTI %&gt;%\n  ggplot(aes(x=range_pct, y=next_day_range_pct)) +\n  scale_x_log10() +\n  scale_y_log10() +\n  geom_point() +\n  # add regression line\n  geom_smooth(method = \"lm\", formula = 'y ~ x') +\n  labs(\n    title = \"Daily range vs next day's range\"\n  )\n\nI think the data is over a 20 year span.\nUsing log scaled axes because the points were clumped towards zero and obscuring the relationsip.\nThe seems to be a fairly persistent relationship between today’s and tomorrow’s volatility\n\n\n\n\nVTI %&gt;%\n  mutate(year = year(date)) %&gt;%\n  ggplot(aes(x=range_pct, y=next_day_range_pct)) +\n  scale_x_log10() +\n  scale_y_log10() +\n  geom_point() +\n  geom_smooth(method = 'lm', formula = 'y ~ x') +\n  facet_wrap(~year, scales = \"free\") +\n  labs(\n    title = \"Daily range vs next day's range by year\"\n  )\n\nSome years are more volatile than others, the relationship between today’s and yesterday’s volatilities has been reasonably consistent over time. And while the slope of the regression line has changed over the years, it’s always up and to the right (i.e. reasonably persistent).\n\n\n\n\nprices %&gt;%\n  # calculate the daily range\n  mutate(range_pct = (high/low - 1) * 100) %&gt;%\n  # perform subsequent operations on each ticker separately\n  group_by(ticker) %&gt;%\n  arrange(date) %&gt;%\n  mutate(next_day_range_pct  = lead(range_pct, 1)) %&gt;%\n  na.omit() %&gt;%\n  # plot each ticker separately\n  ggplot(aes(x=range_pct, y=next_day_range_pct)) +\n  scale_y_log10() +\n  scale_x_log10() +\n  geom_point() +\n  geom_smooth(method = 'lm', formula = 'y ~ x') +\n  facet_wrap(~ticker, scales = \"free\") +\n  labs(\n    title = \"Daily range vs next day's range by ticker\"\n  )\n\nStill fairly consistent",
    "crumbs": [
      "Finance",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/finance-snippets.html#sec-finance-snip-basic",
    "href": "qmd/finance-snippets.html#sec-finance-snip-basic",
    "title": "Snippets",
    "section": "Get Data",
    "text": "Get Data\n\nPackages\n\n{fredr} - The Federal Reserve Bank of St. Louis provides more than 818,000 US and international time series from 109 sources via the API FRED. The data is freely available and can be browsed online on the FRED homepage.\n{simfinapi} - Simfin make fundamental financial data freely available to private investors, researchers, and students. The data provider applies automating data collection processes to collect a large set of publicly available information from firms’ financial statements.\n{Rblpapi} - Bloomberg’s Fundamental coverage includes current and normalized historical data for the balance sheet, income statement, cash flows statement, and financial ratios. Additionally, it provides industry-specific data for communications, consumer, energy, health care, and many more. In order to retrieve Bloomberg data, a paid subscription is needed.\n{Quandl} - Quandl is a publisher of alternative data. Quandl publishes free data, scraped from many different sources from the web. However, some of the data requires specific subscriptions on the Quandl platform.\n{edgarWebR} - The EDGAR database provides free public access to corporate information, allowing you to research a public company’s financial information and operations by reviewing the filings the company makes with the SEC. You can also research information provided by mutual funds (including money market funds), exchange-traded funds (ETFs), and variable annuities.\n\nStock Prices\n\n{tidyquant}{{yfinance}}\n\n\nlibrary(tidyverse)\nlibrary(tidyquant)\n\n# Download symbols of DOW index\nsymbols &lt;- \n  tq_index(x = \"DOW\") |&gt; \n  filter(company != \"US DOLLAR\")\n\n# Download prices of DOW index constituents\nprices &lt;- \n  tq_get(x = symbols, \n         get = \"stock.prices\", \n         from = \"2000-01-01\", \n         to = \"2022-12-31\")\n\n\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\n\n# Download symbols of DOW index\nurl = (\"https://www.ssga.com/us/en/institutional/etfs/library-content/\"\n       \"products/fund-data/etfs/us/holdings-daily-us-en-dia.xlsx\")\nsymbols = (\n  pd.read_excel(url, skiprows=4, nrows=30)\n  .get(\"Ticker\")\n  .tolist()\n)\n\n# Download prices of DOW index constituents\nindex_prices = (\n  yf.download(tickers=symbols, start=\"2000-01-01\", end=\"2022-12-31\")\n  .melt(ignore_index=False, var_name=[\"variable\", \"symbol\"])\n  .reset_index()\n  .pivot(index=[\"Date\", \"symbol\"], columns=\"variable\", values=\"value\")\n  .reset_index()\n  .rename(columns={\"Date\": \"date\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \n                   \"Close\": \"close\", \"Adj Close\": \"adjusted\", \"Volume\": \"volume\"})\n)\n\n\n\nFama French Factors\n\nExample: {frenchdata}\nlibrary(frenchdata)\n\nfactors_ff3_monthly_raw &lt;- download_french_data(\"Fama/French 3 Factors\")\nfactors_ff3_monthly &lt;- \n  factors_ff3_monthly_raw$subsets$data[[1]] |&gt;\n  mutate(\n    month = floor_date(ymd(str_c(date, \"01\")), \"month\"),\n    across(c(RF, `Mkt-RF`, SMB, HML), ~as.numeric(.) / 100),\n    .keep = \"none\"\n  ) |&gt;\n  rename_with(str_to_lower) |&gt;\n  rename(mkt_excess = `mkt-rf`) |&gt;\n  select(month, everything())\n\nprint(factors_ff3_monthly, n = 5)\n#&gt; # A tibble: 1,170 × 5\n#&gt;   month      mkt_excess     smb     hml     rf\n#&gt;   &lt;date&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 1926-07-01     0.0296 -0.0256 -0.0243 0.0022\n#&gt; 2 1926-08-01     0.0264 -0.0117  0.0382 0.0025\n#&gt; 3 1926-09-01     0.0036 -0.014   0.0013 0.0023\n#&gt; 4 1926-10-01    -0.0324 -0.0009  0.007  0.0032\n#&gt; 5 1926-11-01     0.0253 -0.001  -0.0051 0.0031\n\nq-Factors\n\nExample:\nfactors_q_monthly_link &lt;-\n  \"https://global-q.org/uploads/1/2/2/6/122679606/q5_factors_monthly_2022.csv\"\n\nfactors_q_monthly &lt;- read_csv(factors_q_monthly_link) |&gt;\n  mutate(month = ymd(str_c(year, month, \"01\", sep = \"-\"))) |&gt;\n  select(-R_F, -R_MKT, -year) |&gt;\n  rename_with(~ str_remove(., \"R_\")) |&gt;\n  rename_with(~ str_to_lower(.)) |&gt;\n  mutate(across(-month, ~ . / 100)) \n\nprint(factors_q_monthly, n = 5)\n#&gt; # A tibble: 672 × 5\n#&gt;   month            me       ia     roe       eg\n#&gt;   &lt;date&gt;        &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 1967-01-01  0.0683  -0.0297  0.0192  -0.0218 \n#&gt; 2 1967-02-01  0.0165  -0.00227 0.0354   0.0222 \n#&gt; 3 1967-03-01  0.0200  -0.0178  0.0184  -0.0104 \n#&gt; 4 1967-04-01 -0.00690 -0.0288  0.0106  -0.0173 \n#&gt; 5 1967-05-01  0.0285   0.0252  0.00692  0.00158\n\nMacroeconomic Predictors\n\nExample: Welch and Goyal (2008)\ndownload.file(\n  url = \"https://docs.google.com/spreadsheets/d/1g4LOaRj4TvwJr9RIaA_nwrXXWTOy46bP/export?format=xlsx\", \n  destfile = \"macro_predictors.xlsx\", \n  mode = \"wb\"\n)\n\nmacro_predictors &lt;- \n  read_xlsx(\"macro_predictors.xlsx\", sheet = \"Monthly\") |&gt; \n  mutate(\n    # Several cleaning steps & variable transformations...\n  )\n#&gt;   # A tibble: 1,152 × 15\n#&gt;   month        rp_div    dp    dy    ep     de     svar    bm   ntis    tbl\n#&gt;   &lt;date&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 1926-12-01 -0.0220  -2.97 -2.96 -2.39 -0.586 0.000465 0.441 0.0509 0.0307\n#&gt; 2 1927-01-01  0.0422  -2.94 -2.96 -2.37 -0.568 0.000470 0.444 0.0508 0.0323\n#&gt; 3 1927-02-01  0.00363 -2.98 -2.93 -2.43 -0.549 0.000287 0.429 0.0517 0.0329\n#&gt; 4 1927-03-01  0.0142  -2.98 -2.97 -2.45 -0.531 0.000924 0.470 0.0464 0.032 \n#&gt; 5 1927-04-01  0.0459  -2.98 -2.97 -2.47 -0.513 0.000603 0.457 0.0505 0.0339\n#&gt; # ℹ 5 more variables: lty &lt;dbl&gt;, ltr &lt;dbl&gt;, tms &lt;dbl&gt;, dfy &lt;dbl&gt;, infl &lt;dbl&gt;\nExample: FRED CPI data using {tidyquant}\nlibrary(tidyquant)\n\ncpi_monthly &lt;- \n  tq_get(\"CPIAUCNS\", get = \"economic.data\") |&gt;\n  mutate(\n    month = floor_date(date, \"month\"),\n    cpi = price / price[month == max(month)],\n    .keep = \"none\"\n  )\nprint(cpi_monthly, n = 5)\n#&gt; # A tibble: 121 × 2\n#&gt;   month        cpi\n#&gt;   &lt;date&gt;     &lt;dbl&gt;\n#&gt; 1 2014-01-01 0.758\n#&gt; 2 2014-02-01 0.761\n#&gt; 3 2014-03-01 0.766\n#&gt; 4 2014-04-01 0.769\n#&gt; 5 2014-05-01 0.771",
    "crumbs": [
      "Finance",
      "Snippets"
    ]
  },
  {
    "objectID": "qmd/finance-valuation.html",
    "href": "qmd/finance-valuation.html",
    "title": "Valuation",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Finance",
      "Valuation"
    ]
  },
  {
    "objectID": "qmd/finance-valuation.html#misc",
    "href": "qmd/finance-valuation.html#misc",
    "title": "Valuation",
    "section": "",
    "text": "TL;DR;\n\nIn order to be considered, a project’s Rate of Return must exceed the company’s Cost of Capital which deems it as a good investment. Then, out of the projects that exceed the company’s Cost of Capital, the project with the largest Rate of Return is, in general, considered the best project to invest in.\n\nOther Factors\n\nNet Present Value (NPV): While the rate of return for a project may be less than another, its NPV might be greater due its duration being longer. Therefore, its total value might be greater.\nPayment Schedule: Due to the Time Value of Money, a project that pays earlier rather than later, with all else being equal, is more valuable.\nPayout duration: A project with payouts over a longer time period may be more desirable for a given business environment.\n\n\nRecommended\n\nRates of Return: AIRR,MIRR &gt; IRR\n\nComparison\n\n\n\n\n\n\n\n\nFeature\nAIRR\nMIRR\n\n\n\n\nCalculation method\nAverage return on each invested dollar\nIRR of the terminal value of cash flows\n\n\nMultiple solutions\nNo\nPossible\n\n\nCompatibility with NPV\nConsistent\nMay not be\n\n\nReinvestment rate\nAssumes constant rate\nExplicitly factors in\n\n\nEase of understanding\nSimple and intuitive\nComplex and less intuitive\n\n\nComputational complexity\nCan be expensive for complex cash flows\nLess computationally intensive\n\n\n\n\nCost of Capital: WACC\n\nFor small businesses, I haven’t seen anything to use in place of Cost of Capital (ie. WACC), but AIRR and MIRR is still available to compare projects.",
    "crumbs": [
      "Finance",
      "Valuation"
    ]
  },
  {
    "objectID": "qmd/finance-valuation.html#sec-finance-val-ror",
    "href": "qmd/finance-valuation.html#sec-finance-val-ror",
    "title": "Valuation",
    "section": "Rates of Return",
    "text": "Rates of Return\n\nRate of Return (RoR)\n\\[\n\\text{RoR} = \\frac{\\operatorname{current\\_value - \\operatorname{initial\\_value}}}{\\operatorname{initial\\_value}} \\times 100\n\\]\n\nThe net gain or loss of an investment over a specified time period, expressed as a percentage of the investment’s initial cost. The rate of return disregards some key factors in an investment, like the time value of money, the timing and size of cash flows, and the risk and uncertainty associated with any investment or in the case of stocks — taxes and investing fees.\nTo adjust for inflation, just subtract the inflation rate (aka Real Rate of Return).\nTypes\n\nArithmetic: \\(\\sum_{i=1}^T r_i\\) where \\(r_i\\) is the return for the time period, \\(i\\).\nGeometric (aka Compound Annual Growth Rate): \\([(1+r_1) \\times (1+r_2) \\times \\cdots \\times (1+r_T)]^{1/T} - 1\\)\n\nIncludes compounding effect.\nAt most is the Arithmetic average but usually less\n\nAnnual Percentage Rate (APR): \\(m \\times k_m\\) where \\(m\\) is the number of compounding periods in a year and \\(k_m\\) is the periodic rate\n\nConventional method of quoting interest rates\nIgnores compounding effect\ne.g. a credit card with a 1% monthly interest rate has an \\(\\text{APR} = 0.01 \\times 12 = 0.12 \\;\\operatorname{or}\\; 12\\%\\)\n\nEffective Annual Rate (EAR): \\((1+k_m)^m - 1\\)\n\nAPR that takes into account the compounding effect.\ne.g. a credit card with a monthly interest rate of 1.25%, what is the EAR? \\(\\text{EAR} = (1 + 0.0125)^{12}-1 = 0.1608 \\;\\text{or}\\; 16.08\\%\\)\n\n\n\n\n\nInternal Rate of Return (IRR)\n\nA flawed indicator of strength for capital projects. It should only be used when a project has no interim cashflows or is somehow able reinvest those interim cashflows at the same IRR for the duration of the project (See disadvantages).\nThe internal rate of return is a discount rate that makes the net present value (NPV) of all cash flows from a particular project or investment equal to zero. In general, projects with higher IRRs are more favorable than projects with lower IRRs, as the expected rate of return on these projects is greater.\nFormula\n\\[\nNPV = \\sum_{t=0}^T \\frac{C_t}{(1+r)^t} = 0\n\\]\n\nSolve for \\(r\\) to get IRR\nNote that \\(t=0\\) is the initial investment which makes the denominator 1 and therefore that particular \\(\\text{PV}\\) (Present Value) is just \\(C_0\\). Since it’s a cash outlay (i.e. cost), it will be a negative value.\nVariables\n\n\\(T\\): Total number of time periods\n\\(t\\): Time Period\n\\(C_t\\): Net cash inflow (or outflow) for time period, t\n\\(r\\): Internal Rate of Return\n\n\nDisadvantages\n\nLarger projects with lower yields but higher net cash proceeds may be put at an analytical disadvantage when using IRR since IRR is a percentage and not in dollars.\nAssumes cash flows will be reinvested into projects with same IRR for each compounding year of the project. Business enviroments change, so it’s not reasonable to assume investment opportuniities with the samee IRR will be available every year. This leads to a more optimistic projection than there should be. (Video)\n\ne.g. the cash flow recieved in year 1 is automatically invested into another project with same IRR.\nUsing MIRR instead of IRR solves this issue.\n\n\nShows how IRR gets inflated as the cost of capital increases.\n\n\nDoes not consider differences in the duration of projects. A longer term project may be more favorable even if it has a lower IRR, because it is generating a substantial cash flow for a longer period that would be difficult to replace if the shorter term project was chosen.\n\n\n\n\nModified Internal Rate of Return (MIRR)\n\nBetter alternative to IRR that assumes the cash inflows are reinvested at cost of capital of the company instead of at the IRR. (video1, video2)\nWeaknesses\n\nInvolves calculating the IRR of the terminal value of the cash flows,making it less transparent than AIRR.\nSimilar to IRR, MIRR can sometimes have multiple solutions, leading to confusion.\n\nSituations:\n\nSign changes in non-periodic cash flows:\n\nIf a project has non-periodic cash flows that switch between positive and negative, it can create multiple points where the present value of the terminal value equals the initial investment. Each of these points corresponds to a potential MIRR solution.\n\nLarge initial investment followed by small positive cash flows:\n\nWhen a project has a significant initial investment followed by a series of small positive cash flows, there might be multiple interest rates at which the present value of the terminal value (discounted future cash flows) equals the initial investment.\n\nHigh discount rates and long-term negative cash flows:\n\nIf a project has a high discount rate and experiences prolonged negative cash flows, there might be multiple points where the present value of the terminal value (discounted future cash flows) equals the initial investment.\n\nProjects with high variability in cash flows:\n\nProjects with highly variable and unpredictable cash flows, especially those with large spikes or dips, can be more susceptible to having multiple MIRR solutions due to the difficulty in accurately estimating the terminal value.\n\nRounding errors in calculations\n\nSolutions:\n\nAnalyze the cash flow pattern: Understand the underlying reasons for multiple solutions and whether they represent realistic scenarios.\nPerform sensitivity analysis on how changing the discount rate or reinvestment rate affects the number and value of MIRR solutions.\nConsider alternative metrics\n\n\nIn some cases, MIRR can lead to different decisions than NPV, creating inconsistencies.\n\nFormula\n\\[\n\\text{MIRR} = \\left(\\frac{\\text{FV}}{\\text{PV}} \\right)^{\\LARGE{\\frac{1}{n}}} - 1\n\\]\n\nVariables\n\n\\(n\\): Project horizon, i.e. number of periods where cashflows occur\n\\(\\text{FV}\\): The sum of the future values of each cash inflow (i.e. positive cash flow), aka Terminal Cash Flow. Using an assumed reinvestment rate which is realistically different than the company’s cost of capital, each positive cash inflow is projeced forward (i.e. compounded) to the value at the last year of the project. (See Time Value of Money for the formula)\n\ne.g. For a project with a 5yr horizon, a future value for the cash inflow of year 2 is its projected value at year 5.\n\n\\(\\text{PV}\\): The absolute sum of the present values of each cash outflow (i.e. negative cash flow), aka Cash Outlay (e.g. costs, investments). Each cash outlay is projected (i.e. discounted) back to year 0 value using the company’s borrowing rate. (See IRR for the formula, NPV is the sum of PVs)\n\n\nExample: 7 year Project (link)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTimeline\nYear 0\nYear 1\nYear 2\nYear 3\nYear 4\nYear 5\nYear 6\nYear 7\n\n\n\n\nCash Outlay\n10000.00\n4500.00\n950.00\n\n\n\n\n\n\n\nCash Inflow\n\n\n\n6000.00\n7500.00\n1250.00\n8010.00\n9000.00\n\n\nFV,PV at 12.5%\n\n4000.00\n750.62\n9610.84\n10678.71\n1582.03\n9011.25\n\n\n\nTotal CF\n\n\n\n\n\n\n\n39882.83\n\n\nTotal Outlay\n\n\n\n\n\n\n\n14750.62\n\n\n\nreinvestment_rate &lt;- 0.125\nborrowing_rate &lt;- 0.08\nfirst_co &lt;- 10000.00\ncash_outlays &lt;- c(4500.00, 950.00)\ncash_flows &lt;- c(6000.00, 7500.00, 1250.00, 8010.00)\nfinal_cf &lt;- 9000.00\nproject_horizon &lt;- length(c(cash_outlays, cash_flows, final_cf)) # 7yrs\n\ncalc_pv &lt;- function(c, disc_t, r) {\n  c / ((1 + r)^disc_t)\n}\n\ncalc_fv &lt;- function(c, comp_t, r) {\n  c * ((1 + r)^comp_t)\n}\n\npv_vec &lt;- purrr::map2_dbl(\n  cash_outlays,\n  seq(length(cash_outlays)), # 1, 2\n  calc_pv,\n  r = borrowing_rate\n)\n\nfv_vec &lt;- purrr::map2_dbl(\n  cash_flows,\n  rev(seq(length(cash_flows))), # 4, 3, 2, 1\n  calc_fv,\n  r = reinvestment_rate\n)\n\ntotal_cf_adj &lt;- sum(fv_vec) + final_cf\ntotal_co_adj &lt;- sum(pv_vec) + first_co\n\nmirr &lt;- function(fv, pv, T) {\n  ((fv / pv)^(1/T)) - 1\n}\n\nmirr(total_cf_adj, total_co_adj, project_horizon)\n#&gt; [1] 0.1501348\n\nIf the cost of capital is less the 15%, then this is a good investment.\nNote the opposite order used for disc_t and comp_t when iterating through the cash outlays and cash flows.\n\ne.g. 4500 needs to be discounted 1 year to Year 0 dollars and 6000 needs to be compounded 4 years to Year 7 dollars\n\n\nExample:\n\nDescription:\n\nThe US Auto Company, maker of the AerTron hovercar, bought a new die press for the lift propeller housing of their new model. The die press, installed, costs $24 M and a set of dies costs an additional $12 M and has a life of 4 years. The net revenue for this machine is $8 M annually. USAC plans to replace the die set during year 4 for the same $12 M that the original cost, and then run the system for another 4 years. USAC pays 9.5% for borrowed money and expects to earn 16.5% on invested money. What is the MIRR for this system?\n\nData and Code:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear 0\nYear 1\nYear 2\nYear 3\nYear 4\nYear 5\nYear 6\nYear 7\nYear 8\n\n\n\n\nCash Outlay\n36\n\n\n\n4\n\n\n\n\n\n\nCash Inflow\n\n8\n8\n8\n\n8\n8\n8\n8\n\n\n\nreinvestment_rate &lt;- 0.165\nborrowing_rate &lt;- 0.095\nfirst_co &lt;- 36\ncash_outlays &lt;- 4\ncash_flows &lt;- c(8, 8, 8, 8, 8, 8)\nfinal_cf &lt;- 8\nproject_horizon &lt;- length(c(cash_outlays, cash_flows, final_cf)) # 8yrs\n\ncalc_pv &lt;- function(c, disc_t, r) {\n  c / ((1 + r)^disc_t)\n}\n\ncalc_fv &lt;- function(c, comp_t, r) {\n  c * ((1 + r)^comp_t)\n}\n\npv_vec &lt;- purrr::map2_dbl(\n  cash_outlays,\n  4,\n  calc_pv,\n  r = borrowing_rate\n)\n\nfv_vec &lt;- purrr::map2_dbl(\n  cash_flows,\n  c(7,6,5,3,2,1),\n  calc_fv,\n  r = reinvestment_rate\n)\n\ntotal_cf_adj &lt;- sum(fv_vec) + final_cf\ntotal_co_adj &lt;- sum(pv_vec) + first_co\n\nmirr &lt;- function(fv, pv, T) {\n  ((fv / pv)^(1/T)) - 1\n}\n\nmirr(total_cf_adj, total_co_adj, project_horizon)\n#&gt; [1] 0.1275095\n\nIn Year 0, there is an initial investment of $36M since the cost of the die press plus installation cost is $24M and set of dies cost $12M.\nIn Year 4, the die set is replaced. The question says the die set is replaced “during year 4.” Since the answer is what it is, this evidently means the company gets the positive cash flow of the $8M and incurs the cost of the new die set of $12M which leads to an outflow of $4M.\n\nWould’ve made more sense if they would’ve said the die set was replaced at the end of year 4 IMO. My first attempt assumed they didn’t receive the cash inflow $8M for that year.\n\nThe Year 4 outflow is discounted 4 years, and each cash inflow (except the last one) is compounded to Year 8. Note that in the code, Year 4 is skipped since it’s an outflow year.\n\n\n\n\n\nAverage Internal Rate of Return (AIRR)\n\nAIRR overcomes many of the limitations of IRR by calculating the average rate of return on each invested dollar at each point in time, resulting in a single, unambiguous measure of project profitability. It addresses the issues with multiplicity, complex numbers, and incompatibility with NPV, providing a more reliable and interpretable measure for investment decisions. (Magni, 2010)\n\nMagni listed 21 flaws of IRR, see his paper “Average Internal Rate of Return and Investment Decisions: A New Perspective” for details.\nI didn’t include the formulation here, but AIRR can be represented as weighted average. See video for details.\nWeaknesses:\n\nAssumes a constant reinvestment rate, which may not be realistic for all situations.\nCan be computationally expensive for complex cash flows\n\n\nNotes from Videos: AIRR: A Comprehensive Approach to Rate of Return and Investment Decisions\nFormula Using Income and Capital\n\n\\[\n\\begin{align}\n\\text{AIRR} &= \\frac{\\text{Total Income}}{\\text{Total Capital}}\\\\\n&= \\frac{I_1 + \\frac{I_2}{1+r} + \\frac{I_2}{(1+r)^2} + \\cdots + \\frac{I_n}{(1+r)^{n-1}}}{C_0 + \\frac{C_1}{1+r} + \\frac{C_2}{(1+r)^2} + \\cdots + \\frac{C_{n-1}}{(1+r)^{n-1}}} \\\\\n&= \\frac{\\sum_{i = 1}^n\n\\frac{I_i}{(1+r)^{i-1}}}{\\sum_{j = 1}^{n} \\frac{C_{j-1}}{(1+r)^{j-1}}}\n\\end{align}\n\\]\n\n\\(I\\): The income of the project\n\\(C\\): The capital invested in the project where \\(C_0\\) is the initial investment. While the final period of the project will have a cashflow, it will not have a capital investment.\n\\(r\\): The cost of capital\n\\(n\\): The total number of periods of the project not including the initial investment period.\n\nFormula Using Net Present Value and Cash Flows\n\\[\n\\text{AIRR} = r + \\frac{\\text{NPV}(1+r)}{\\text{PV[C]}}\n\\]\n\n\\(r\\): The cost of capital\n\\(NPV\\): Net Present Value of the cash flows\n\\(PV[C]\\): The total cost of capital discounted the present value (i.e. Present Value of Total Capital)\n\\(\\frac{\\text{NPV}}{\\text{PV[C]}}\\): This is the net present value return per unit of capital\n\\(\\frac{\\text{NPV}(1+r)}{\\text{PV[C]}}\\): This is the Excess Return Rate. It tells investors how much more they are making beyond the cost of capital rate.\n\nFormula Using Time-Varying Cost of Capital\n\\[\n\\begin{align}\n\\text{AIRR} &= \\frac{I_1 + \\frac{I_2}{1+r_2} + \\cdots + \\frac{I_n}{(1+r_2)\\cdots(1+r_n)}}{C_0 + \\frac{C_1}{1+r_1} + \\cdots + \\frac{C_{n-1}}{(1+r_1)\\cdots(1+r_{n-1})}} \\\\\n&= (1+r_1) \\cdot \\frac{\\frac{I_1}{1+r_1} + \\cdots + \\frac{I_n}{(1+r_1) \\cdots (1+r_n)}}{C_0 + \\frac{C_1}{1+r_1} + \\frac{C_{n-1}}{(1+r_1)\\cdots(1+r_{n-1})}}\\\\\n&= (1+r_1) \\cdot \\frac{\\text{PV[I]}}{\\text{PV[C]}}\n\\end{align}\n\\]\n\n\\(\\text{PV[I]}\\): Total income of the project that has been discounted to the present value (i.e. Present Value of Total Income)\n\\(r_i\\): The time-varying cost of capital where \\(i\\) represents the period of the project.\n\\(n\\): The total number of periods of the project not including the initial investment period.\nThe Minimal Attractive Cost of Capital (MARR) replaces the cost of capital (since we now have multiple CoCs) as the threshold for an acceptable investment\n\\[\n\\begin{align}\n\\text{MARR} &= (1+r_1) \\cdot \\frac{\\frac{r_1C_0}{1+r_1}+\\cdots+\\frac{r_n C_{n-1}}{(1+r_1)\\cdots(1+r_n)}}{C_0 + \\frac{C_1}{1+r_1}+\\cdots+\\frac{C_{n-1}}{(1+r_1)\\cdots(1+r_{n-1})}} \\\\\n&= (1+r_1) \\cdot \\frac{\\text{PV[R]}}{\\text{PV[C]}} \\\\\n&= \\text{AIRR} - \\frac{\\text{NPV}(1+r_1)}{\\text{PV[C]}}\n\\end{align}\n\\]\n\n\\(\\text{PV[R]}\\): The time-varying market return discounted to the present value. The amount of return you’d expect from a minimally-acceptable, alternative investment using these capitals and these costs of capital over the same length of time (e.g. treasury bonds).\n\nNet Present Value using a Time-Varying CoC\n\\[\n\\begin{align}\n\\text{NPV} &= F_0 + \\frac{F_1}{1+r_1} + \\cdots + \\frac{F_n}{(1+r_1)\\cdots(1+r_n)} \\\\\n&= \\text{PV[C]} \\times \\frac{\\text{AIRR}-\\text{MARR}}{1+r_1}\n\\end{align}\n\\]\n\nThe amount of value that exceeds the minimally acceptable investment’s return.\nThis is a reformulation of the “Formula using Net Present Value and Cash Flows”\n\\(\\text{PV[C]}\\) represents the size of the investment\n\\(\\frac{\\text{AIRR}-\\text{MARR}}{1+r_1}\\) represents the efficiency of the project\n\n\nExample: Net Present Value and Cash Flows Formula for AIRR\n\nCalculate Cash Flows\n\n\n\nTime\n0\n1\n2\n\n\nIncome\n\n375\n225\n\n\nCapital\n600\n400\n\n\n\nDepreciation\n\n200\n400\n\n\nCash Flow\n-600\n575\n625\n\n\n\n\nThere is an initial capital investment of $600 that depreciates each year of the project. Capital decreases from 600 to 400 which is a depreciation of $200 and then from $400 to 0 which is a depreciation of $400.\nDepreciation is added to Income (aka earnings) to get Cash Flow (See Discount Cash Flow Model &gt;&gt; Calculate FCF from projected variables for each year to the horizon)\n\nCalculate NPV\n\\[\n\\text{NPV} = -600 + \\frac{575}{1+0.15} + \\frac{625}{(1 + 0.15)^2} = 372.60\n\\]\n\nWhere the cost of capital is 15%\n\nCalculate AIRR\n\\[\n\\text{AIRR} = 0.15 + \\frac{372.60 \\times (1+0.15)}{600 + \\frac{400}{1 + 0.15}}= 0.602 = 60.2\\%\n\\]",
    "crumbs": [
      "Finance",
      "Valuation"
    ]
  },
  {
    "objectID": "qmd/finance-valuation.html#sec-finance-val-coc",
    "href": "qmd/finance-valuation.html#sec-finance-val-coc",
    "title": "Valuation",
    "section": "Cost of Capital",
    "text": "Cost of Capital\n\nThe purpose of calculating Cost of Capital is to determine the minimum rate of return that a company should generate to satisfy its investors and creditors. It is commonly used in capital budgeting and valuation processes. When a company is considering new investment projects, the Cost of Capital is often used as the discount rate to evaluate the project’s feasibility. If the project’s expected rate of return is higher than the Cost of Capital, it may be considered a worthwhile investment.\nFor DCF analysis\n\nFor diverse companies (e.g. GE), it’s better to calculate a CoC for each division.\nThe lower the CoC, the more valuable the business.\n\n\n\nWeighted Average Cost of Capital (WACC)\n\nThe weighted cost of a company’s invested capital (both debt and equity). Recommend method to calculate Cost of Capital.\n\\[\\text{WACC} = w_e \\cdot K_e + w_d \\cdot K_d\\]\n\n\\(w_e\\): The proportion of equity to the company’s market value of capital, \\(\\frac{E}{E+D}\\)\n\n\\(D\\) is the market value of Debt\n\\(E\\) is the market value of Equity\n\n\\(w_d\\): The proportion of debt to the company’s market value of capital, \\(\\frac{D}{D + E}\\)\n\\(K_e\\): Cost of Equity - The required annual rate of return that a company’s equity investors expect to receive (includes dividends).\n\\(K_d\\): After-Tax Cost of Debt\n\nThe current proportions, \\(w_e\\) and \\(w_d\\), should be compared with historical weight values of the company and a decision should be made on whether these proportions accurately represent the future capital structure (i.e the proportions) of the company over the next 5-10 years.\n\nExample: A company has recently made an acquisition of another company, therefore, temporarily inflating it’s debt proportion. This debt might be payed off within a few years which will reduced its debt proportion of capital. Using the debt proportion prior to the debt being paid would bias the WACC.\n\n\n\nCost of Equity\n\nNotes from Video: Estimating Cost Of Equity For WACC - DCF Model Insights\nAlso see Forcasting, Statistical &gt;&gt; Interval Forecasting\n(Basic) Capital Asset Pricing Model (CAPM)\n\\[\nK_e = R_f + \\beta_L\\; (R_m - R_f)\n\\]\n\n\\(R_f\\): Risk-free rate of return (Time Value of Money). A rate of a U.S. T-Bill, T-Note, or T-Bond is typically used.\n\\(R_m\\): Expected Market rate of return\n\\(\\beta_L\\): (Levered Beta) Risk estimate or a company’s stock beta or a similar public company’s stock beta (Reward for bearing systematic risk). Companies with higher betas will have higher values of \\(K_e\\)\n\nFor private companies, the beta from a peer publicly traded company or the average beta of a group of peer publicly traded companies can be used. Private companies may have differenct capital structures (see WACC weights), so each peer company’s beta from the peer group of companies needs be unlevered (i.e. remove influence of leverage) to obtain their Asset Betas.\n\nBeta on places like Yahoo Finance is a Levered Beta. That beta is affected by that companies capital structure and its stock price’s day-to-day fluctuations. The asset beta is the beta of the industry or sector.\nUnlevering Beta\n\\[\n\\beta_U = \\frac{\\beta_L}{1 + \\frac{D}{E}(1-\\text{tax rate})}\n\\]\n\n\\(\\beta_U\\): Unlevered Beta\n\\(\\beta_L\\): Levered Beta\n\\(D/E\\): Debt to Equity Ratio\n\\(\\text{tax rate}\\): The marginal tax rate\n\nAfter unlevering the beta for each company in the peer group, the average is taken to get the peer group’s asset beta.\nFinally the peer group’s asset beta is relevered using the company’s target capital structure\n\\[\n\\beta_L = \\beta_U(1+\\frac{D}{E}(1-\\text{tax rate}))\n\\]\n\nThe company’s target capital structure is oftentimes its current capital structure\n\n\nThe middle yellow line shows how the WACC goes down as the company takes on a small amount debt which increases its value (lower WACC) since the interest on the debt is tax deductible. But, if the company takes on too much debt, its WACC increases which decreases its value since its risk of default gets higher.\nThe optimal amount of debt is difficult to determine by the analyst, so it’s assumed that management’s goal is to optimize value. Therefore, unless under unusual circumstances (e.g. post-acquisition period), the company should be at its target capital structure (i.e. optimal \\(w_d\\)) or moving towards it.\n\n\n\n\n\\(R_m - R_f\\) The market risk premium is the amount of systemic risk.\n\nSystematic Risk (aka Non-Diversifiable Risk): The market risk that all companies are exposed to (i.e. economy).\nFrom 1926 to 2011, Ibbotson calculate the market risk premium to be 6.62%. Various Wall Street firms calculated it to be between 5-8%.\n\n\n(Advanced) Capital Asset Pricing Model (CAPM)\n\\[\nK_e = R_f + \\beta_L\\; (R_m - R_f) + R_s + \\text{CRP} + R_z\n\\]\n\n\\(R_s\\): Size Risk Premium - The smaller the company, the riskier it is, and the higher the risk size premium. Larger companies are generally more diversified (e.g. more stores to offset local busines conditions)\n\nUse Ibbotson/Kroll SBBI yearbook’s “Size Premium” value. Provides consistency among analysts. (See R &gt;&gt; Documents &gt;&gt; Finance &gt;&gt; 2016 SBBI, Ch.7, pg 7-16 (pg140)) Kroll now puts out a monthly version for $1200 🙄.\n\n\\(\\text{CRP}\\): Country Risk Premium - Risk associated with investing in an international company. (e.g. political instability, volatile exchange rates, economic turmoil)\n\nUse Default Spread from NYU Stern’s country spread page\n\n\\(R_z\\): Specific Company Risk Premium (SCRP) (Reward for bearing unsystematic risk)\n\nUnsystematic Risk: The risk that affects a single company or a small group of companies. Unsystematic risk is lowered through diversification. Larger companies are typically more diversified than smaller companies.\nNo standardized formula for calculating this risk and is usually based on the analyst’s judgement. Factors include the financial statement, comparative ratio analysis, and qualitative matters such as a site visit and management interviews.\nHighland Global method for estimating SCRP\n\n\nLink to white paper (Also in R &gt;&gt; Documents &gt;&gt; Finance)\nMore appropiate for older industrial companies than new companies. (See Issues section)\nProcess\n\nFor each factor’s value of the company, get the appropriate Assignment Rating in the left-most column\n\nExample shows this value at the bottom of each factor column where the highlighted cell’s are the company’s factor values.\n\nAverage all the Assignment Ratings to get the SCRP estimate\n\nFor this example, estimated SCRP = 3.86%\n\n\nFactors\n\nRevenue Growth measured by either Compound Annual Growth Rate over the last 3 to 5yrs or Forecasted Growth Rate\nFinancial Risk measured by Total Debt Ratio\n\nUse the most recent fiscal year-end balance sheet, unless there’s significant interim or projected change in teh firm’s capital structure\n\nOperation Risk measured by Fixed Costs/Sales\n\nIn absence of a major cost cutting intitative to be implemented in the foreseeable future, use the most recent fiscal year data.\n\nProfitability measured by Net Profit Margin\n\nFor stable firms, use the most recent fiscal year net profit margin on an adjusted basis.\nFor firms with erratic earnings or profitability, use a 3 to 4yr average of the net profit margin or the projected net profit margin if the erratic behavior is expected to subside.\nIf earnings are not adjusted for the previous 3 to 5yrs, use the most recent fiscal year.\n\nIndustry Risk measured by Firm Return on Asset (ROA)/ Industry ROA\n\nReturn on Asset measures the ability of the firm to generate revenues with its asset base.\n\nEconomic Risk measured by Firm ROA/GDP Change\n\nRatio suggests that economic risk for the firm is a function of its ability to generate a return on its asset bas relative to the overall economy’s ability.\nUse most recent GDP change (i.e growth or contraction %)\n\nCustomer Concentration measured by Total Sales to Top 5 Customers/Firm’s Total Sales\n\nUse most recent fiscal year’s sales figures unless there’s an anticipated change for the future (e.g. loss of the one of the top five customers in the next year)\n\n\nIssues\n\nNeeds adjustment for Tech companies.\n\nTech companies don’t have many fixed costs because they don’t have a lot of tangible assets. So, there would need to be some sort of replacement measure for Operational Risk\nReturn on Equity would be more appropriate for estimating Industry Risk (and Economic?)\n\nEconoomic Risk is essentially Country Risk, so using this measure in the Advanced CAPM is potentially double counting.\n\n\n\n\nBuild-Up Approach Model (BAM)\n\\[\nK_e = R_f + R_m + \\text{IRP}_i + R_s + CRP + R_z\n\\]\n\n\\(\\text{IRP}_i\\): Industry Risk Premium - The amount investors expect the future return of this industry to exceed the market as a whole.\n\\[\n\\text{IRP}_i = (Ri_i \\times \\text{ERP}) - \\text{ERP}\n\\]\n\n\\(Ri_i\\): The risk index for the industry.\n\\(\\text{ERP}\\): The expected equity risk premium\nReplaces the \\(\\beta_a\\) in the CAPM to take into account systemic risk.\nUse Valuation Handbook – Guide to Cost of Capital’s “Industry Risk Premia” value. (See R &gt;&gt; Documents &gt;&gt; Finance &gt;&gt; 2016 Valuation Handbook, Appendix 3a) Kroll now puts out a monthly version for $1200 🙄.\n\n\n\n\n\nAfter-Tax Cost of Debt\n\nNotes from Video: Estimating The Cost Of Debt For WACC - DCF Model Insights\nList of instruments that should be considered as Debt in the calculations below:\n\nLong and Short-Term interest-bearing obligations (e.g Loans, Bonds)\nOperating Leases (e.g. An airline that leases out its planes.) (Video)\n\nMethods\n\nYield-to-Maturity (Most Accurate)\n\nVariables required for each bond:\n\nMarket Price: The Price the bond is being traded at\nAmount: The amount the bond was issued for\nYield to Maturity: Rate at which the current market price of the bond is equal to the present value of all the cash flows from the bond\n\nAmount was difficult to find. I found it along with the other variables by using a combination of websites, finra.org and bondsupermart.com. Each bond has a few different ids, so I suspect the amount can obtained through the SEC or somewhere using them if those websites die.\nOverall Process\n\nCalculate Yield-to-Maturity (YTM) of all publicly traded company debt\nCalculate the weighted average of all debt instruments\nMultiply result by (1 - Tax Rate) to get \\(K_d\\)\n\nSteps\n\nCalculate each bond’s discount rate (DR)\n\\[\n\\text{DR}_i = \\frac{\\text{Price}_i}{100}\n\\]\n\n\\(i\\): The index for each bond the company has issued\nPrices below 100 are trading below market value\nPrices above 100 are trading above market value (aka at a premium)\n\nCalculate the Market Value of Debt (MVD)\n\\[\n\\text{MVD} = \\sum_{i=1}^N \\text{MVB}_i = \\sum_{i=1}^N \\text{DR}_i \\times \\text{A}_i\n\\]\n\n\\(A\\): The amount of the bond\nThe market value of a bond (MVB) is the discount rate (DR) \\(\\times\\) the amount of the bond (A). Therefore, the sum is the market value of the company’s debt.\n\nFor each bond, calculate Proportion of MVD (pMVD)\n\\[\n\\text{pMVD}_i = \\frac{\\text{MVB}_i}{\\text{MVD}}\n\\]\n\nCalculates each bond’s proportion of the total debt\n\nCalculate After-Tax Cost of Debt\n\\[\nK_{d} = (1- \\text{Tax Rate}) \\times \\sum_{i = 1}^N \\text{pMVD}_i \\times \\text{YTM}_i\n\\]\n\n\\(\\text{YTM}\\): Yield to Maturity\n\\(\\text{Tax Rate}\\): The company’s effective tax rate in its most recent quarter.\nThe proportion (pMVD) \\(\\times\\) the bond’s yield to maturity (YTM) is its weighted YTM.\n\n\n\nDebt Rating (Best of the Rest)\n\\[\nK_d = (\\operatorname{Risk Free Rate} + \\operatorname{Default Spread}) \\times (1 - \\operatorname{Tax Rate})\n\\]\n\nFor cases where the company’s debt may not have market prices readily available. (e.g a mix of bonds and bank debt)\nNot sure how this works if a company has multiple debt instruments that get payed off at different time intervals.\n\nMaybe compute this value for each debt instrument. Then, multiply each \\(K_{d,i}\\) by its instrument’s proportion of overall debt as in YTD method above. Then, add them all together.\n\n\\(\\text{Default Spread}\\): Associated with the company’s credit rating (e.g. AAA, BB+, etc.). Get the company’s rating and look-up the spread for that rating. The company credit ratings seem readily availabe just by googling, but the spreads are a little harder to find. NYU Stern Business School has a page.\n\nFor foreign corporations or corporations that get much of their revenue from other countries (adds in geopolitical risk), you need to add the add the “country spread” to the default spread. NYU Stern’s country spread page.\n\n\\(\\text{Risk Free Rate}\\): Value that’s used depends upon length of bank loan or maturity date of the bond. (e.g for a 12-year bond or a 12-year loan, use the 10-year treasury yield since it is the closest benchmark)\n\\(\\text{Tax Rate}\\): The company’s effective tax rate in its most recent quarter.\n\nSynthetic Rating\n\nEstimate the credit rating using a company’s financial characteristics\n\\[\n\\text{Interest Coverage Ratio} = \\frac{\\text{EBIT}}{\\text{Interest Expenses}}\n\\]\n\n\\(\\text{EBIT}\\): Typically reported in a company’s income statement\n\\(\\text{Interest Expense}\\): How much interest was payed on its debt (long and short term). Most likely also reported in the company’s financials.\n\\(\\text{Interest Coverage Ratio}\\): The relationship to the rating and therefore the default spread is also on the NYT Stern page\n\nThen, use Debt Rating method to get \\(K_d\\)\n\nInterest Rate on Bank Debt\n\\[K_d = \\frac{\\text{interest expense}\\times (1-\\text{tax rate})}{\\text{total debt}}\\]\n\nMethod of last resort if the other methods are not available. Since bank debt isn’t traded, the market value of this debt is unknown. The WACC formula weights are market value based, so a method that take into account the market value would be preferrable.\n\\(\\text{interest expense}\\): Interest paid on company’s current debt\n\nSince interest expense is tax-deductible, the debt is calculated on an after-tax basis\n\n\\(\\text{tax rate}\\): Company’s effective tax rate",
    "crumbs": [
      "Finance",
      "Valuation"
    ]
  },
  {
    "objectID": "qmd/finance-valuation.html#sec-finance-val-dcf",
    "href": "qmd/finance-valuation.html#sec-finance-val-dcf",
    "title": "Valuation",
    "section": "Discount Cash Flow Model",
    "text": "Discount Cash Flow Model\n\nFundamental valuation methodology that’s based on the principal that the value of a company can be derived from the Present Value (PV) of it’s projected Free Cash Flow (FCF)\n\nThe valuation implied for a target by a DCF is known as it’s Intrinsic Value, as opposed to its market value.\nFree Cash Flow (FCF) - The cash generated by a company after paying all cash operating expenses and associated taxes, as well as the funding of capex and working capital, but prior to the payment of any interest expense.\n\nMisc\n\nNotes from\n\nVideo: Discounted Cash Flow (DCF) Model – CH 3 Investment Banking Valuation, Leveraged Buyouts, and M&A - Rosenbaum, Pearl\nVideo: Why Is My DCF Model Incorrect? Common Errors\n\nAdvantages\n\nCash Flow based: A more fundamental approach to valuation than using multiple-based methodologies\nMarket Independent: More insulated from market aberrations such as bubbles and distressed periods\nSelf-Sufficient: Doesn’t rely entirely upon truly comparable companies which is important when there are no “pure play” comparables. (e.g. precedent transactions analysis and comparable companies analysis)\nFlexibility: Allows the you to run multiple scenarios (See Sensitivity Analysis)\n\nDisadvantages\n\nDepends on Projections: Accurate forecasting is a challenge especially as the forecast horizon lengthens\nSensitivity to Assumptions: Relatively small changes in variable values can (but not always) result in significant changes to results\nTerminal Value: Can represent 2/3 or more of the valuation, and this value depends on the final period of the forecast horizon which is the most uncertain.\nAssumes a Constant Capital Structure: Capital structure is not allowed to change over the entire entire horizon. (See WACC)\n\nPitfalls\n\nForecast no less than 7yr (recommended 10+) of cash flows\n\nSome contracting companies or resource companies like mining corporations can project cash flows out a decade or more.\nForecasted cash flows should represent no less than 1/3. By extending the horizon as far out as you reasonably can, you lessen the influence of the terminal value in the enterpise value calculation.\n\nUnder-projecting CAPEX\n\nFor companies with a recent acquisition, sometimes analysts under-project CAPEX values which inflates FCF projections. Add an ROI line and see if there’s a lot of volatility (I think this supposed to be projected ROI that you get from analyst reports). If there is a volatility, then your model is unreliable. I guess you’re then supposed to choose another model.\n\n\n\nProcess\n\nStudy the target and determine key performance drivers\nProject FCF\nCalculate WACC\nDetermine the terminal value\nCalculate PV and determine valuation\n\nDetermine Key Performance Drivers\n\nStudy the target and its sector which includes its business model, financial profie, value proposition for customers, end markets, competitors, and key risks.\nAnalyze drivers of sales growth, profitability, and FCF generation with the goal of crafting FCF projections.\n\nInternal Drivers: new facilities/stores, developing new products, securing new customer contracts, improving operational and/or working capital efficiency, etc.\nExternal Drivers: acquisitions, end market trends, consumer buying pattern, macroeconomic factors, legislative/regulatory changes, etc.\n\n\nProject Necessary Variables\n\nHistorical Performance such as past growth rates, profit margins, and other ratios are good indicators of future performance especially for mature companies in non-cyclical sectors.\n\n3 years of historical data for growing or mid-size companies or 5 years for blue chip companies is typically representative enough to be predictive of future performance.\n\nFCF forecast horizon is typically 5yrs but may be longer depending on its sector, stage of development, and performance predictability.\n\nThe chosen horizon should reach to where the target’s performance reaches a stead-state or normalized level.\nThe terminal value depends heavily on the last forecasted value of FCF, so that value needs to be as accurate as possible and representative of the steady-state performance of the company (i.e not much volatility in its earnings)\nFor a cyclical company, the horizon should be at a period that’s between business cycles, so the company is not likely to be performing at a peak or valley (i.e. steady-state).\nHorizon Guidelines\n\n\n\n\n\n\n\nCompany’s Competitive Position\nExcess Returns/Forecast Horizon\n\n\n\n\nSlow Growing; Operates in a highly competitive, low margin industry\n1 Year\n\n\nSolid company that has advantages such as strong marketing channels, brand recognition, regulatory advantages, etc.\n5 Years\n\n\nEarly stages of rapid growth; Operates with high barriers to entry; dominant market position or prospects\n10 Years\n\n\n\n\nCompanies with long-term contracted revenue streams such as natural resources, satellite communications, or utilites should be forecasted out to 10+ years.\n\n\nExample: Percentage of Sales Method for COGS, Gross Margin, and SG&A\n\nGet historical data for Sales (aka Revenue), COGS, Gross Margin, SG&A, EBITDA, D&A, and EBIT for the last 3 to 5 years. Get a consensus of projected estimates for Sales, EBITDA, and EBIT.\n\n\nFor public companies, projected estimates of Sales, EBITDA, and EBIT for the first 2 or 3 years can obtained from reports generated by other analysts who specialize in the sector. Use the consensus or average of these estimates.\n\nFor private companies, estimates of peer companies in the public sector can used as a proxy\nSales projections must be consistent with other DCV assumptions, such CAPEX and Working Capital. (e.g. higher top line growth typically requires the support of higher levels of CAPEX and Working Capital)\n\n\nTransform variable amounts to a percentage of sales for each year\n\nFor COGS, Gross Margin, and SG&A, calculate the average percentage (last column). Then, multiply each variable’s average percentage times each projected Sales estimate to get each variable projected estimate for each period of the horizon.\n\n\nFor projecting COGS and SG&A further, it’s typical to hold Gross Margin and SG&A (as a percentage of Sales) constant. Although, you may want to make a slight adjustment due to company trends or the outlook of the market.\n\nFor D&A projected estimates, subtract EBIT from EBITDA for each period of the horizon to get D&A. The projected D&A values should be consistent with CAPEX projections and D&A historical values (i.e. compare as a percentage of Sales or as a percentage of CAPEX).\n\nProjecting Other Variables\n\nCAPEX can be projected using consensus estimates from analysts’ reports or in the same manner as COGS, Gross Margin, and SG&A by the average percentage of sales methodology.\n\nIf a business is going through unusual circumstances (e.g. merger) analyst reports are recommended.\n\nYear-over-Year (YoY) Changes in Net Working Capital can be projected in a couple different ways:\n\nThe same manner as COGS, Gross Margin, and SG&A by the average percentage of sales methodology\n(Recommended) By projecting individual components of both current assets and current liabilities for each year in the horizon and calculating YoY NWC from them.\n\nAccounts Receivable (A/R): \\(\\text{DSO Ratio} = \\frac{\\text{AR}}{\\text{Sales}} \\times 365\\)\n\nCalculate the DSO Ratio for the previous years using the historical data\nCalculate the average DSO Ratio over the previous years.\nSolve for AR to get: \\(AR = \\frac{\\text{DSO Ratio}}{365} \\times \\text{Sales}\\)\nFor each year in the horizon, input the average DSO Ratio for \\(\\text{DSO Ratio}\\) and the consensus projected estimate of sales for \\(\\text{Sales}\\). Then, calculate the projected \\(AR\\).\n\nUse the same method for:\n\nInventory:\n\\[\n\\text{DIH Ratio} = \\left(\\frac{\\text{Inventory}}{COGS}\\right) \\times 365\n\\]\n\nOr\n\n\\[\n\\text{Inventory Turns} = \\left(\\frac{\\text{COGS}}{\\text{Inventory}}\\right)\n\\]\nAccounts Payable (A/P): \\(\\text{DPO Ratio} = \\left(\\frac{\\text{AP}}{\\text{COGS}}\\right) \\times 365\\)\n\nUse the Percentage of Sales method for:\n\nPrepaid Expenses and Other Current Assets\nAccrued Liabilities and Other Liabilities\n\nUse these projected values to calculate projected Current Assets and projected Current Liabilities. Then the difference is the projected YoY NWC for each period in the horizon\n\n\n\n\nCalculate FCF from projected variables for each year to the horizon\n\\[\n\\text{FCF}_i = \\text{NOPAT}_i - \\text{CAPEX}_i - \\text{Net Working Capital}_i + \\text{D\\&A}_i\n\\]\n\nNet Operating Profit After Taxes (NOPAT): \\(\\text{NOPAT} = \\text{EBIT} - \\text{Taxes}\\)\n\nIf the company had been losing money but is expected to become more profitable, then effective tax rate will be too low. Therefore the marginal tax rate should be used\nUse effective tax rate if the company is large and stable or already growing and doesn’t use tax credits, nondeductible expenses (e.g. gov’t fines), deferred tax asset valuation allowances, or other company-specific tax policies. Otherwise, you should use the marginal tax rate.\n\n\nDetermining the Terminal Value\n\nA Terminal Value is used to capture the remaining value of the FCF beyond the projection period. Typically, around 2/3 of the overall valuation depends on this terminal value.\nIt’s based on the value of FCF or EBITDA in the final year of the projection period (i.e. final year in the horizon). Therefore, the projected values in the final year are very important\nEnd-of-Year vs. Mid-Year calculations\n\nFormulas for both End-of-Year and Mid-Year are given in these last two sections, but FCF are usually generated throughout the year rather than at year-end. So, it’s typically discounted using the mid-year convention.\n\nThe exception is the EMM calculation, since it uses LTM trading multiples for a calendar year end EBITDA (or EBIT). So it’s recommended to use the End-of-Year convention.\n\nUsing the Mid-Year convention results in a slightly higher valuation due to the fact that FCF is received sooner.\nIf a company does only receive its cash flows at the end of the year, then the End-of-Year convention should be used.\n\nExit Multiple Method (EMM)\n\\[\n\\text{Terminal Value} = \\text{EBITDA}_T \\times \\text{Exit Multiple}\n\\]\n\n\\(T\\): The final year of the projected period (aka horizon)\nDetermines the remaining FCF by means of a multiple. This multiple is typically the average of the Last-Twelve-Month (LTM) EBITDA multiples of comparable companies.\n\nAs current multiples may be affected by sector or economic cycles, it is important to use both a steady-state trading multiple. (e.g. Don’t use a LLM mulitple from year where the company was at a cyclical high)\n\nPerform a sanity check by comparing EMM’s implied growth rate to the PGM implied growth rate\n\nEnd-of-Year Discounting\n\\[\nr_{\\text{emm}} = \\frac{(\\text{Terminal Value} \\times \\text{WACC}) - \\text{FCF}_T}{\\text{Terminal Value} + \\text{FCF}_T}\n\\]\nMid-Year Discounting\n\\[\nr_{\\text{emm}} = \\frac{(\\text{Terminal Value} \\times \\text{WACC}) - \\text{FCF}_T \\sqrt{1 + \\text{WACC}}}{\\text{Terminal Value} + \\text{FCF}_T \\sqrt{1 + \\text{WACC}}}\n\\]\nIf \\(r_{\\text{emm}}\\) is not close to the PGM growth rate, \\(g\\), then this could indicate that the EMM assumptions are not realistic\n\n\nGordon Growth Method aka Perpetuity Growth Method (PGM)\n\\[\n\\text{Terminal Value} = \\frac{\\text{FCF}_T (1 + g)}{\\text{WACC - g}}\n\\]\n\n\\(T\\): The final year of the projected period (aka horizon)\n\\(g\\): The perpetuity growth rate\nTreats final projected year as a perpetuity that grows as some assumed rate.\nThe perpetuity growth rate is typically chosen on theh basis of the company’s expected long-term industry growth rate which generally tends to be around the nominal GDP growth (2% - 4%)\nAssumes the company’s rate of return on capital will always be more than the cost of capital which doesn’t jive with microeconomic Life Cycle theory. Life Cycle theory says that eventually the rate of return will equal the cost of capital due to other competitors entering the market to capture the growth rate of the industry.\nIf \\(g \\ge \\text{WACC}\\), then you must use the EMM method instead.\nPerform a sanity check by comparing PGM’s implied exit multiple with EMM’s exit multiple\n\nEnd-of-Year Discounting\n\\[\n\\text{Exit Multiple}_{\\text{PGM}} = \\frac{\\text{Terminal Value}}{\\text{EBITDA}_T}\n\\]\nMid-Year Discounting\n\\[\n\\text{Exit Multiple}_{\\text{PGM}} = \\frac{\\text{Terminal Value}\\times \\sqrt{1+\\text{WACC}}}{\\text{EBITDA}_T}\n\\]\nIf the \\(\\text{Exit Multiple}_{\\text{PGM}}\\) is not close to EMM’s \\(\\text{Exit Multiple}\\), then could indicate that the PGM assumptions are not realistic.\n\n\nPGM with correction\n\\[\n\\text{Terminal Value} = \\frac{\\text{NOPAT}_T}{\\text{WACC}}\n\\]\n\nFollows Life Cycle theory that the rate of return on capital will eventually equal the cost of capital due competitors entering the market. It more closely represents the reality that net profit on these cash flows will eventually be zero.\n\n\nExplicit Forecast represents the projected FCFs, and after the vertical line, the downward sloping diagonal represents the corrected terminal value.\n\nIn uncorrected pgm, the diagonal line for terminal value would be sloping upwards.\n\n\n\n\nDetermine the Valuation\n\nCalculate the Discount Factor:\n\nSee “End-of-Year vs. Mid-Year calculations” in the Terminal Value section for further details\nEnd-of-Year: \\(\\frac{1}{(1+\\text{WACC})^i}\\)\n\n\\(i\\) is the projected period\nSo later periods have a smaller discount factor\n\nMid-Year: \\(\\frac{1}{(1+\\text{WACC})^{i-0.5}}\\)\n\nCalculate the Present Value (PV) of FCFi: \\(\\text{FCF}_i \\times \\text{Discount Factor}_i\\)\nCalculate the Enterpise Value\n\nExample: 5-Year Horizon with Mid-Year Discounting\n\\[\n\\text{Enterprise Value} = \\frac{\\text{FCF}_1}{(1+\\text{WACC})^{0.5}} + \\frac{\\text{FCF}_2}{(1+\\text{WACC})^{1.5}} + \\frac{\\text{FCF}_3}{(1+\\text{WACC})^{2.5}} + \\frac{\\text{FCF}_4}{(1+\\text{WACC})^{3.5}} + \\frac{\\text{FCF}_5}{(1+\\text{WACC})^{4.5}} + \\frac{\\text{EBTDA}_5 \\times \\text{Exit Multiple}}{(1+\\text{WACC})^{5}}\n\\]\n\nAll FCF values and the Terminal Value is discounted to present and summed to get the Enterprise Value.\n\n\nDerive Implied Share Price\n\nCalculate Implied Equity Value (IEV): \\(\\text{IEV} = \\text{Enterprise Value} - \\text{Net Debt} + \\text{Preferred Stock} + \\text{Noncontrolling Interest}\\)\nCalculate Implied Share Price (ISP): \\(\\text{ISP} = \\frac{\\text{IEV}}{\\text{Fully Diluted Shares Outstanding}}\\)\nThe implied share price can be compared to the market price to determine whether a stock is being undervalued or overvalued\n\n\nPerform Sensitivity Analysis\n\nThere are various assumptions is DCF analysis, therefore it’s prudent to select a range of values at various stages and see how much the output is changed. This should result in a range of Enterprise Values.\nIf slight changes in varibles result in large changes in enterprise value, then something is wrong with your model\nMake sure to include variables that are directly related to business drivers such as sales, operating costs, and investment (e.g. Gross Margin, CAPEX, and other financial statment line items). These are the variables that stakeholders will want to see when they look at your analysis. They want to know what levers they can pull to increase the valuation of their business.\n\nSimple Solutions: Lengthen horizon; adjust values in terminal value calculation\nExample: Trying Different WACC and Exit Multiple Values\n\n\n6000 is the result of the DCF analysis and the rest of cells are the results of the sensitivity analysis.\nRelatively minor effects result from changes in WACC and the Exit Multiple. Therefore there is no indication that the model is flawed is aspects regarding these two variables.\n\nExample: Trying different Terminal Value Growth Rates and Gross Margins\n\n\nShows a decrease in Gross Margin by 0.5% results in around a 40% decrease of enterpise value ($15.39 \\(\\rightarrow\\) $9.32)! Indicates something is wrong with the model.\nMaybe the model is being overly influenced by Gross Margin or maybe the other variables aren’t influencial enough.\nAlso terminal growth rate is from the PGM method for calculating Terminal Value. The PGM-corrected version of that model should be used which doesn’t have a growth rate.",
    "crumbs": [
      "Finance",
      "Valuation"
    ]
  },
  {
    "objectID": "qmd/healthcare.html",
    "href": "qmd/healthcare.html",
    "title": "Healthcare",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Healthcare"
    ]
  },
  {
    "objectID": "qmd/healthcare.html#sec-health-misc",
    "href": "qmd/healthcare.html#sec-health-misc",
    "title": "Healthcare",
    "section": "",
    "text": "Notes from\n\nPaper: Scalable and accurate deep learning with electronic health records by Google AI’s team\nHow to Encode Medical Records for Deep Learning\n\nMakes data suitable for a RNN (see Processing for details)\n\n\nAlso see\n\nEpidemiology &gt;&gt; Disease Mapping\n\nFalse negatives are often more important than false positives. (e.g. detecting diseases)\nPopulations within Healthcare Claims Data\n\n\nLines of business (LOB) such as patients with coverage from a government payer (Medicaid, Medicare); commercial lines (employer groups, retail or purchased via the exchange); self-insured (self-funded groups, usually large employers paying for their own healthcare claims of employees), etc.\n\nSub-lines of business, or groups. For instance, Medicaid may consist of multiple sub-groups representing different levels of eligibility, coverage, benefits, or types of people/why they qualified for the program (Temporary Assistance for Needy Families (TANF) vs. Medicaid expansion for adults)\n\nDemographics (certain groups, males or females, certain regions, etc.)\n\nConditions (examining certain chronic conditions, top disease states of interest or those driving the highest costs to the system, those that Medicare focuses on, etc.)\n\nSub-groups of some combination of the above, for instance: Medicaid, TANF, looking at mothers vs. newborns\n\nCross-group comparison of some combination of the above, for instance: Medicaid, TANF, new mothers and their cost/utilization/outcomes trends vs. Commercial, self-insured, new mothers and their cost/utilization/outcomes\n\n\nICU’s data is usually the most complete and available for research compared to other healthcare data\nRecognized cutpoints - 25 kg/m2 to define “overweight” based on body mass index.",
    "crumbs": [
      "Healthcare"
    ]
  },
  {
    "objectID": "qmd/healthcare.html#sec-health-terms",
    "href": "qmd/healthcare.html#sec-health-terms",
    "title": "Healthcare",
    "section": "Terms",
    "text": "Terms\n\nBiomedical Informatics - involves carrying out analysis on large-scale biological datasets in order to understand and profer solutions to health-related problems. Focuses on the optimal use of biomedical information, data, and knowledge for problem-solving and decision-making by employing computational and traditional approaches\nCalibration - the agreement between the estimated and the “true” risk of an outcome. A well-calibrated model is one that minimizes residuals, which is equivalent to saying that the model fits the test data well. (This is just GoF. How well the model generalizes.)\nClinical Data Science - focuses on applying data science to healthcare with the goal of improving the overall well-being of patients and the healthcare system\nDiscrimination - the ability of a model to rank patients according to risk (Often measured by AUROC)\nElectronic Health Record (EHR) -  comprehensive collection of all information by the individuals involved in patient care. This includes records from clinicians, laboratories, radiology imaging, health insurance, socio-demographics, genetic sequencing data, etc.\nElectronic Medical Record (EMR) - patient medical and treatment history within a single practice. EMR is the electronic version of the traditional paper records found in clinicians’ offices.\nEndpoint - Outcome variable measured in a medical study. e.g. Death, stroke, or quality of life are good endpoints. Blood tests and images on scans are not good endpoints.\n\nA composite endpoint is one that consists of two or more events\n\nExample: death due to cardiovascular causes or hospitalization due to heart failure\n\nSo the binary outcome would be a 1 if either of those events took place or a 0 if they did not. Or in a survival model, time until either of those events.\nIssues\n\nSee The All-Important Endpoint of a Medical Study for details\n\nSee Harrell in the comments for the solution\n\n\n\n\n\nFHIR - Fast Healthcare Interoperability Resources - open healthcare data standard (also sets standards for other things)\n\nOne JSON data schema per healthcare concept — e.g. Patient, Observation, Condition, MedicationRequest, etc.\n\nHealthcare Analytics - analytics activities that can be undertaken as a result of data generated from core areas of healthcare including claims and cost data, pharmaceutical and research & development data, clinical data, patient behavior & sentiment data (narrower in scope compared to clinical data science)\nIn Silico Modeling - Widely used in pharmaceutical research to simulate the interactions between drugs and their target molecules (e.g., proteins or enzymes). Researchers use computer programs to predict how potential drug compounds will interact with their targets and to identify potential drug candidates more efficiently.\n\nA Latin phrase that means “in silicon,” and it’s used to describe processes that are carried out on a computer or using computer simulations.\nCan be used to predict the toxicity of chemicals or compounds without the need for extensive animal testing. This is important for assessing the safety of new drugs or chemicals.\nUsed to understand the function of molecular networks in metabolism, gene regulation, or signal transduction\n{sismonr}- “Aims at simulating random in silico biological systems, containing genes linked through networks of regulatory interactions. The regulatory interactions can be of different type, i.e. we account for regulation of transcription, translation, RNA decay, protein decay and protein post-translational modification. sismonr then simulates the stochastic expression profiles of the different genes in these systems over time for different in silico individuals, where the individuals are distinguised by the genetic mutations they carry for each gene, affecting the properties of the genes. The user has control over the different parameters used in the generation of the genes, the regulatory network and the individuals.” (Paper)\n\nLife Expectancy - a snapshot of the current mortality (“expenctancy” comes from “expected value”)\n\nAssumes that assumes that the observed age-specific death rates at the time of birth for a cohort stay unchanged for their entire lifetimes.\n\nLine of Business (LOB) - a statutory set of heath insurance policies\nProgression-Free Survival (PFS) - The length of time during and after the treatment of a disease, such as cancer, that a patient lives with the disease but it does not get worse. In a clinical trial, measuring the PFS is one way to see how well a new treatment works.\nProgression-Free Survival Rate - The percentage of people who did not have new tumor growth or cancer spread during or after treatment. The disease may have responded to treatment completely or partially, or the disease may be stable. This means the cancer is still there but not growing or spreading.\nSocial Determinants of Health (SDOH) - conditions in the environments where people are born, live, learn, work, play, worship, and age that affect a wide range of health, functioning, and quality-of-life outcomes and risks. (US Health and Human Services article)\n\nFive Categories: Economic Stability, Education Access and Quality, Health Care Access and Quality, Neighborhood and Built Environment, Social and Community Context",
    "crumbs": [
      "Healthcare"
    ]
  },
  {
    "objectID": "qmd/healthcare.html#use-cases",
    "href": "qmd/healthcare.html#use-cases",
    "title": "Healthcare",
    "section": "Use Cases",
    "text": "Use Cases\n\nMisc\n\nIn most cases, these tools are not meant to replace a doctor in a particular situation. They’re meant supplement the doctor’s toolbox.\nBenefits: cost-effectiveness, first-line screening, longer term monitoring, speeds up detection and therefore referral process\n\nClinical Outcomes\n\nMortality (death) events\nEarly warning score (EWS) - likelihood of death\n\nFeatures: respiratory rate, oxygen saturation, temperature, blood pressure, heart rate, and consciousness rating\nEach variable has a normal range as established by common medical knowledge. A score is computed based on a lookup table to characterize how far away the variable is from its normal range. If the sum of all scores surpasses a threshold, it means a high likelihood of death\n\nEarly warning system for patient deterioration (CHARTwatch at Toronto hospital system, video)\n\nDeployed to General Internal Medicine ward (GIM)\nSlides with links to papers that discuss the model on the youtube website\nPredicts risk score of a patient “at risk”\n\n“At Risk” - transfer to ICU or transfer to Palliative Care unit or Death (composite endpoint, see Terms)\nProbability score then thresholded into “high,” “medium,” and “low” risk (color coded) labels\n\nFeatures: laboratory values, vital measurements, and demographics\nPrediction delivery\n\nEmail to nurses, palliative care unit, etc.\n\nTable with name, bed #,… , Status (aka prediction) for each patient\n\n“Front-End Tool” - software for people that sign out patients, has general patient information, includes column for Status\nPhone alerts for patients with High risk labels get sent to clinicians\n\n\nDiagnostic Tool\n\nExample: Heart Murmur Detection (video, slides)\n\nTakes phonocardiographic data (audio sensor/time series) to predict heart murmur (binary) outcome.\n\n\n\nResource Utilization\n\nPlanning for hospital bed capacity\nScore for long length of stay (i.e. larger than 7 days)\n\nCommonly computed 24 hours after admission\nLogistic regression model (with proper regularization techniques)\nFeatures: age, gender, condition category, diagnosis code, hospital service received, and lab tests of vital signs to produce a probability number for long length of stay.\n\nAssignment/Scheduling\nForecast daily Emergency Department arrivals\n\nQuality of Care\n\n30 days readmission after discharge\nHospital score for readmission\n\nTypically calculated at discharge time\nFeatures: hemoglobin level, sodium level, type of the admission, number of previous admissions, length of stay, whether the hospital stay is cancer related, and whether medical procedures were performed during the stay\nBased on established medical knowledge, the values of each factor is translated to a risk score, and the sum of which depicts the overall risk of readmission\n\n\nMedical Imaging AI tools\nDashboard to compute dosage (e.g. blood thinner) based on patient health factors",
    "crumbs": [
      "Healthcare"
    ]
  },
  {
    "objectID": "qmd/healthcare.html#sec-health-preproc",
    "href": "qmd/healthcare.html#sec-health-preproc",
    "title": "Healthcare",
    "section": "Prepocessing",
    "text": "Prepocessing\n\nCategoricals and Codes\n -\n\nReconciling codes from various sources of data can be challenging because some coding systems are proprietary\n\ne.g. “Heart failure” may be “123” in one coding system and “1a2b3c” in another\nSolution: Tokenize data and create embeddings\n\nWorks as long as each healthcare dataset uses a consistent set of coding systems for itself\n\n\nTokenization\n\nText\n\nJust split it by whitespaces.\nExample: “high glucose” becomes [“high”, “glucose”]\n\nCodes\n\nEmbedding\n\nFor every field in every healthcare concept, we build a vocabulary of a predefined size\n\nCreating a global vocabulary doesn’t work because different fields carries distinct healthcare semantics\n\nTrain an embedding for each token\nEmbeddings are learnt jointly with the prediction tasks\nChoose the same embedding dimension for all fields within a given healthcare concept (easier for aggregating)\n\nAggregate to a common time-step\n\ne.g. 1 hour or a few hours or can be tuned as a hyperparameter\nTake the average of all field embeddings in an instance (embeddings for a time step) to form the aggregated embedding for that instance\n\nCan also use median, etc. instead of mean\n\nIf there are multiple instances of a concept, we can further average the instance embeddings to form the embedding for that concept\nSince the healthcare concepts are a predefined enumerated list, we can concatenate the concept embeddings together to form a fixed sized example.\nIf a concept does not appear in the time-step, we just set its embedding to all 0s.\n\nAppend timestamp feature to embedding\n\nThe timestamp of the instances / training examples are not evenly spaced. When a particular event occurs and has significant clinical meaning, it’s not captured by the embeddings.\nTake the average of the timestamps of all instances in the time-stamp, and append it to the end of the fixed size embedding we obtained via fields -&gt; instance, and instances -&gt; concept aggregation\n\nThis part isn’t completely clear to me. Need to check the Google AI paper\nI think it means average all the timestamps within each aggregated embedding and append it to the embedding\n\nSeems like the averaged timestame would need to be transformed into a numerical before being appended.",
    "crumbs": [
      "Healthcare"
    ]
  },
  {
    "objectID": "qmd/healthcare.html#bias-in-healthcare-claims-data",
    "href": "qmd/healthcare.html#bias-in-healthcare-claims-data",
    "title": "Healthcare",
    "section": "Bias in Healthcare Claims Data",
    "text": "Bias in Healthcare Claims Data\n\nNotes from article, (mini) paper\nPackages\n\n{fairmodels} - From Dalex group\n{{biaslyze}} - NLP Bias Identification Toolkit\n\nImbalance of patients or members represented in large healthcare datasets can make your results non-generalizable or (in some cases) flat out invalid.\n\nGoals should be: investigate the biases, mitigate as well as possible, and decide which insights are still valuable or meaningful despite these nuances\n\nCharacteristics of data that lead to biases\n\n\nOnly includes any members/patients who actually had an incident/event for which the insurer processed a subsequent claim\n\nTends to over/underrepresent certain groups who are more likely to have chronic health problems, adverse social determinants of health, seek care,\n\nGroups are reflective of the type of demographics your organization tends to serve or that you have a larger book of business in\n\n\nCan cause over-and-under-representation of certain diseases\n\nEven companies within the same insurance sector may not have similar populations. Company business practices affect the types of populations within their data.\n\n\nOur [patients / members / employees / residents / etc.] may not act similarly or even represent [all patients / other groups / other types of employer s / other regions / etc.]\n\n\nData is years old. Effects sizes in data that’s 5 or 6yrs old may not be valid now.\n\n\nTypes\n\n\nUndercoverage bias - occurs when a part of the population is excluded from your sample.\n\nHistorical bias - occurs when socio-cultural prejudices and beliefs are mirrored into systematic processes [that are then reflected in the data].\n\nSystemic biases - result from institutions operating in ways that disadvantage certain groups.\n\n\nSolutions\n\n\nAugment or compare with outside data:  data sharing/collaboration or tapping into additional feeds such as health information exchange (HIE)\n\nSocial Determinants of Health (SDOH) - (see definition below) compare socio-economics and demographics of your dataset with  Census estimates and the SDOH variability seen for said zip codes.\n\n\nWill determine the amount of reweighting/normalizing the data requires",
    "crumbs": [
      "Healthcare"
    ]
  },
  {
    "objectID": "qmd/healthcare.html#sec-health-fairpriv",
    "href": "qmd/healthcare.html#sec-health-fairpriv",
    "title": "Healthcare",
    "section": "Fairness and Privacy",
    "text": "Fairness and Privacy\n\nMisc\n\nNotes from - Algorithmic Bias in Healthcare and Some Strategies for Mitigating It\nBias - Systematic error due to incorrect assumptions\nUnder-represented groups are most susceptible to the impacts of bias\nDeidentified clinical data sets are collections of observational patient data that have been stripped of all direct Patient Health Information (PHI) components. IRB permission is not necessary for access to deidentified clinical data sets.\nClinical data sets with HIPAA restrictions include observational patient information such as dates of admission, discharge, service, and birth and death as well as city, state, zip codes with five digits or more, and ages expressed in years, months, days, or hours. Without a patient’s consent or a HIPAA waiver, HIPAA-restricted clinical data sets may be used or shared for research, public health, or healthcare operations.\n\nBiases to fairness\n\nHistorical bias - when the data collected to train an AI system no longer reflects the current reality\n\ne.g. even though the gender pay gap is still an issue, it was worse in the past.\n\nRepresentation bias - depends on how the training data is defined and sampled from the population.\n\ne.g. the data used for training the first facial recognition system, mostly relying on white faces, which lead the model to have a hard time recognizing black faces and other dark-skinned faces.\n\nMeasurement bias - occurs when training data features or measurements differ from real-world data\n\ne.g. where the data for image recognition is mainly collected from one type of camera while real-world data is from multiple types of cameras.\n\nCoding/human bias - this happens mostly when scientists dive deep into a project with their subjective thoughts about their study\n\ne.g. “non-white patients receive fewer cardiovascular interventions and fewer renal transplants”, and “Black women are more likely to die after being diagnosed with breast cancer”. Source\n\n\nStrategies to mitigate bias\n\nCollecting and using diverse training data:\n\nData non-representative of the real-world population is a common cause of bias.\nCollect and use diverse training data that accurately reflects the demographics, backgrounds, and characteristics of the population the algorithm will be used on.\n\nTest the algorithm for bias:\n\nCan be done using a variety of methods, including conducting bias audits and using fairness metrics to measure the algorithm’s performance.\nEvaluate performance of model across demographic groups and not just the whole population.\n\nUse algorithmic fairness techniques:\n\nPre-processing algorithms that adjust the data to reduce bias\nIn-processing algorithms that make adjustments during the training process\nPost-processing algorithms that adjust the output of the algorithm to make it fairer\n\nEnsure transparency and accountability:\n\nProvide clear explanations of how the algorithm works\nRegularly review and update the algorithm to remove any biases that may have been introduced\nProvide mechanisms for individuals to challenge the decisions made by the algorithm.\n\nEngaging with diverse stakeholders\n\ne.g. individuals and communities that may be affected by the algorithm\nUnderstand their perspectives and incorporate their feedback into the design and implementation of the algorithm.\nCan help ensure the algorith accurately reflects the needs and concerns of the population it will be used on.\n\n\nExamples of Algorithmic Bias\n\nUnitedHealth Group\n\nDeveloped a commercial algorithm in order to determine which patients would require extra medical care (patients with the greatest medical need).\nA bias in the algorithm reduced the number of black patients identified for extra care by more than half, and falsely concluded that black patients are healthier than equally sick White patients.\nRace correlated with other factors such as historical healthcare expenditures to evaluate future healthcare needs, which made it reflect economic inequality rather than the true medical needs of patients.\n\nDrug discovery for Covid-19\n\nAn AI system was developed to triage patients and expedite the discovery of a new vaccine\nThe AI system was able to predict with 70 to 80% accuracy which patients are likely to develop severe lung disease\nThe triage process was solely based on the symptoms and preexisting conditions of patients, which can be biased because of the disparities based on race and social economic status.",
    "crumbs": [
      "Healthcare"
    ]
  },
  {
    "objectID": "qmd/insurance.html",
    "href": "qmd/insurance.html",
    "title": "Insurance",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Insurance"
    ]
  },
  {
    "objectID": "qmd/insurance.html#sec-insur-misc",
    "href": "qmd/insurance.html#sec-insur-misc",
    "title": "Insurance",
    "section": "",
    "text": "Packages\n\nCRAN Task View\n\nAlso see\n\nRegression, Other &gt;&gt; Censored and Truncated Data &gt;&gt; Examples\n\nDS Use Cases\n\nPredict insurance claims frequency (see bkmk)",
    "crumbs": [
      "Insurance"
    ]
  },
  {
    "objectID": "qmd/insurance.html#sec-insur-risk",
    "href": "qmd/insurance.html#sec-insur-risk",
    "title": "Insurance",
    "section": "Risk",
    "text": "Risk\n\nWhen analyzing data, beware of survivorship bias\n\nExample: Real Estate Investment\n\nAn analyst is studying housing prices over time in a certain region. They use a current map and so only consider neighborhoods that have survived without major incidents (like natural disasters, economic decline, etc.). They will probably underestimate the risk and overestimate the return of real estate investment in that region.\n\n\nLimiting exposure\n\nFrom http://ronaldrichman.co.za/2021/02/24/x-is-not-fx-insurance-edition/\nSeverity\n\nCapping the payout of a policy\n\ne.g. only paying a maximum amount if tragedy strikes\n\n\nFrequency\n\nSetting a threshold to which the policy only pays out after the threshold has been passed\n\nKeeps the insurance company from being needled to death by administrative costs of frequent policy payouts\ne.g. minor doctor appointments\n\n\nReinsurance\n\nPolicies that produce an option-like exposure, where one can pass risk above a fixed level of losses to the counterparty for a fixed premium (excess of loss). Other options are to share risks in more or less equal proportions.\n\nAllows insurers take on risky (and potentially more profitable) policies by taking on an insurance policy themselves for the excess risk\n\nairplanes, volatile manufacturing, etc.\n\n\n\n\nAnalysis\n\nFit one distribution to the smaller and more frequent attritional losses, and another disruption to the extreme losses, with the latter distribution often motivated by extreme value theory\n\nThis approach ignores the fact the each loss has an upper bound determined by the limits on the policy generating the loss. Also, since these extreme losses follow a very heavy tailed distribution, naïve estimators of the statistical properties of these losses are likely to be biased\n\nShadow Moments\n\nTransform the data to a new domain that is unbounded, parameterizing EVT distributions in this domain, and then translating the implications of these models back to the original bounded domain\nCirillo, P., & Taleb, N. N. (2016). On the statistical properties and tail risk of violent conflicts. Physica A: Statistical Mechanics and Its Applications, 452, 29–45. https://doi.org/10.1016/j.physa.2016.01.050\nCirillo, P., & Taleb, N. N. (2020, June 1). Tail risk of contagious diseases. Nature Physics, Vol. 16, pp. 606–613. https://doi.org/10.1038/s41567-020-0921-x",
    "crumbs": [
      "Insurance"
    ]
  },
  {
    "objectID": "qmd/insurance.html#sec-insur-mba",
    "href": "qmd/insurance.html#sec-insur-mba",
    "title": "Insurance",
    "section": "Market Basket Analysis",
    "text": "Market Basket Analysis\n\nSupport: What percent of patients have disease 1 and disease 2?\nConfidence: Of the people w/disease1, what percent also have disease 2?\nLift: How much more likely are you to have disease 2 if you already had disease 1 (and vice versa)",
    "crumbs": [
      "Insurance"
    ]
  },
  {
    "objectID": "qmd/kpis.html",
    "href": "qmd/kpis.html",
    "title": "KPIs",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "KPIs"
    ]
  },
  {
    "objectID": "qmd/kpis.html#sec-kpis-misc",
    "href": "qmd/kpis.html#sec-kpis-misc",
    "title": "KPIs",
    "section": "",
    "text": "Also see\n\nJob, Organizational and Team Development &gt;&gt; Developing a Data Strategy &gt;&gt; Objectives and Key Results (OKRs)\n\nLeaders should focus on 3 to 6 KPIs that will drive growth.\nWhen a KPI’s growth begins to stagnate, break up the KPI monolith into smaller, more meaningful (and hopefully, easier to optimize) segments",
    "crumbs": [
      "KPIs"
    ]
  },
  {
    "objectID": "qmd/kpis.html#sec-kpis-terms",
    "href": "qmd/kpis.html#sec-kpis-terms",
    "title": "KPIs",
    "section": "Terms",
    "text": "Terms\n\nOperationalization - A process of defining the measurement of a phenomenon that is not directly measurable (e.g. happiness), though its existence is indicated by other phenomena. It is the process of defining a fuzzy concept so as to make the theoretical concept clearly distinguishable or measurable, and to understand it in terms of empirical observations.",
    "crumbs": [
      "KPIs"
    ]
  },
  {
    "objectID": "qmd/kpis.html#sec-kpis-gen",
    "href": "qmd/kpis.html#sec-kpis-gen",
    "title": "KPIs",
    "section": "General",
    "text": "General\n\nKPI characteristics\n\nMonotonic: you always want to drive them higher or lower\nTypically a summary statistic or rate\n\nQuestion that you should ask when determining which metrics should be your KPIs\n\nWhat do you want to know?\nHow much do you need to know?\nWhy do you want to know this?\nWhat is the value or impact between not knowing and knowing?\n\nInput \\(\\rightarrow\\) Output \\(\\rightarrow\\) Outcome Framework\n\nInput Metrics\n\nProvide information about the resources used to create a system or process\nUseful for evaluating the efficiency of a system or process\nWhat you control: amount of time you spend on a task, the quantity of materials used to produce something, etc.\nOutputs should be highly responsive to your inputs\n\nOutput Metrics\n\nProvide information about the immediate results of a system or process\nUseful for evaluating the performance of a system or process\n\nAnswers whether the system or process is producing the desired results.\n\nIt should be clear how much your output would change for one additional unit of input.\nVery actionable but not fully under your control\nThere is a causal link between your output and your outcome, so difficult to discover\n\nMay require experimentation to determine causality\n\nTypically tracked daily\n\nOutcome Metrics\n\nNorth Star metrics (See Product Metrics &gt;&gt; Types)\nWhat you are aiming to move with all your activities.\nTypically requires multiple outputs to move\nTypically tracked monthly or quarterly\n\nExample: Educational Program\n\nThe number of teachers, their average seniority, the funding of the school (inputs) help drive the grades of the students and their consistency over time higher (output) which ultimately lead to more students graduating high school (outcomes).\n\n\nIndustry Examples\n\nFinancial — Revenue growth, cash flow, burn rate, gross profit\nCustomer — Engagement rates, net promoter scores, acquisition costs, conversion rates\nSupport & Service — Turnaround time, mean time to resolve, SLA compliance, quality\nEmployee — Attrition and retention, satisfaction, engagement\nGovernance, Risk, & Compliance — Percent compliance to process, audit compliance, non-security incidents Issues\n\nAvoid incentivizing the wrong behavior\n\nAs soon as someone’s performance is linked to a metric — it is fair game to expect them to try to move the metric, so it is up to the metric designer to make sure the ‘rules of the game’ are clearly articulated.\nExample: Reducing the number of support tickets opened via email\n\nPotential Solution: Engineer makes it as hard as possible to contact the email support\nUnintended Consequence: Increasing the number of ‘negative’ social media interactions\n\nExample: You don’t want to push your salespeople to sell without caring about the retention rate.\n\ne.g Car salesmen selling a lemon to a customer. Customer unlikely to return to buy another car.\n\nSolutions\n\nPair metric with a guardrail metric (see Product Metrics &gt;&gt; Types)\n\nExamples\n\nQuantity (sales) + quality (retention rate)\nShort term metric (inventory level) + long term metric (# of shortages)\n\n\nDesign a compound metric\n\nBy taking into account multiple measurement, it cannot be easily gamed\nCan also include metrics you don’t want move negatively (e.g. cost metrics, guardrail metrics, health metrics)\nFor experiments that use this metric, you then create a binary decision rule to decide if your experiment is successful or not.\n\n\n\nBeware of Deceptive Metrics\n\nCorrelation does not equal causation\nExample: Someone discovers that the number of transactions is correlated w/higher revenue\n\nCompany decides to make number of transactions a metric and starts trying to increase it.\n\ne.g. Retargeting former customers, offering discounts, etc.\n\nRevenue doesn’t substantially increase after considerable effort and resources\nAnalysis reveals that the revenue is being mostly driven by a few whales buying high-ticket items while the team’s increase in transactions was mostly low-ticket items\nSo, the strategy should’ve been to target more whales and not just trying to increase the number of transactions\n\n\nChecks\n\nMetric is precise (i.e. measurement isn’t noisy)\nMetric is accurate (i.e. it properly depicts the phenomenon it is supposed to depict).\n\nNext Steps\n\nFigure out the best way to calculate these metrics\nCreate dashboards to monitor them over time\nSet up alerts when thresholds are crossed\n\nAnomaly detection",
    "crumbs": [
      "KPIs"
    ]
  },
  {
    "objectID": "qmd/kpis.html#sec-kpis-prod",
    "href": "qmd/kpis.html#sec-kpis-prod",
    "title": "KPIs",
    "section": "Product Metrics",
    "text": "Product Metrics\n\nAlso see Product Development &gt;&gt; Metrics\nWhy are they important?\n\nMetrics help companies have clarity, alignment and prioritization in what to build.\nMetrics help a company decide how to build the product once they’ve prioritized what to build.\nMetrics help a company determine how successful they are and hold them accountable to an outcome.\n\n\n\nNorth Star Metric (NSM)\n\nLong-term metric focused on the desired outcome for the entire company\nExamples of potential NSMs\n\nInstagram: Monthly Active Users\nSpotify: Time Spent Listening\nAirbnb: Booked Nights\nLyft: Rides per week\nSlack: Daily Active Users\nCustomer happiness (e.g. revenue, Net Promoter Score (NPS), and customer satisfaction)\nQuora: Questions Answered\n\n\n\n\nPrimary\n\nAsk\n\n“What is the desired outcome for this product?” (keeping the company’s mission in mind)\n\nExample: Number of high quality sellers that join the platform as a result of the email outreach\n\n“What do our customers care about, and how do we solve it as fast as possible?”\n\nExample\n\nEngineering, product, and marketing agree that onboarding is a pain point, you decide to build goals and KPIs around making it easier for new customers to get started.\nAlign the company around the shared goal of reducing new tool onboarding from five days to three days\n\nData team gathers metrics on usage and helps build A/B tests\nEngineering team modifies the product\nMarketing team creates nurture campaigns\n\n\n\n\nMap this outcome to a metric that is meaningful, measurable, and movable\n\nMeaningful\n\nShould reflect the way a company intends to drive value for its customers (based on the company’s mission)\nAvoid vanity metrics.\n\nExample: Quora uses push notifications to alert users when they would be best suited to answer a question. However, the number of times users click on the notification and open the app is a vanity metric. It may make Quora feel good to see open rates going up. But Quora didn’t release push notifications to drive open rates. The outcome they were aiming for was to get high quality answers to questions asked on their platform. So an outcome oriented metric would focus on answer rates, not open rates.\n\n\nMeasurable\n\nEach component of that metric’s formula should be a datapoint that you can collect with high confidence and precision. And remember to add a time frame to your metric.\nSome aspects of a company’s or product’s mission/goal aren’t measurable directly but may be measured through latent variables, proxy, or highly associated variable or group of variables\n\nMoveable\n\nBasically means the metric shouldn’t be so noisy as to inhibit being able to measure changes when you take actions to change it.\nThe metric should measure something that as directly under control of the company as possible.\n\nExample: Page Load Time - Time difference between a user clicking on a link or typing the URL in the browser and the page being rendered to the user.\n\nThis is affected substantially by the user’s computer, ISP, browser, etc. along with the app/website code. So asis, it wouldn’t be a good metric to track.\n\n\n\nOther Desirables\n\nQuick Feedback\n\nActions taken to influence the metric should be (almost) immediately observable\nExample: Measuring (subscription) retention would require you to wait until the end of the month in order to be able to judge the effect of your action. Daily Active Users might be a better alternative\n\nEasily Interpretted\n\nThe role of a metric is to align teams around a specific goal so that they can take the right steps towards achieving that goal. If people can’t understand the metric, they can’t take the right steps to optimize for it.\nExamples:\n\nComplicated: the median of a weighted combination of viewing time, comments, likes, shares per user (with each action having different weights based on its importance)\nSimpler: Average Watch Time per user.\n\n\nNot Gameable\n\nI.e. Picking an easily moveable metric that has no real value leads to bad incentives.\n\n\n\n\n\n\nSupporting/Tracking/Input Metrics\n\nLeading indicators that the NSM or the primary metric is moving in the right direction\nInputs into the NSM and are directly correlated to its value\nAlso tells you where your efforts to move your NSM may be falling short.\nExamples\n\nNumber of emails sent\nThe number of people that opened the email\nNumber of businesses that signed up to sell on the platform\n\n\n\n\nCounter Metrics/Guardrails\n\nOther outcomes that the business cares about, which may be negatively affected by a positive change in the primary metric (or NSM)\nThey exist to make sure that in the pursuit of your primary metric, you are not doing harm to another aspect of the business\nExample: Email marketing to obtain quality sellers for Amazon)\n\nIf your primary metric focuses on product quantity, your guardrail metric might be around product quality\nThe guardrail, average number of purchases a user makes in a day, ensures that the influx of sellers that the primary metric optimizes for doesn’t result in consumers becoming so overwhelmed by choice, that they end up not buying anything at all\n\n\n\n\nAARRR (or Pirate Metric) Framework\n\nAwareness: How many people are aware your brand exists?\n\nMetric Examples: number of website visits, social media metrics (number of likes, shares, impressions, reach), time spent on a website, email open rate\n\nAcquisition: How many people are interacting with your product?\n\nA lead is any potential user who’s information you’ve been able to capture in some shape or form.\n\ne.g. People who give you their email addresses when they sign up for your mailing list are considered to be leads.\n\nA qualified lead when they show additional interest in your product beyond giving you their information.\n\ne.g. In addition to signing up for your mailing list they also watch a webinar or sign up for a demo\n\nMetric Examples: number of leads, number of qualified leads, sign ups, downloads, install, chatbot interactions\n\nActivation: How many people are realizing the value of your product?\n\nTypically in the form of an action taken x times with in a period of y days\nWhen the activation hurdle is crossed, an individual goes from unknown entity to actual user.\nExample: Dropbox\n\nTheir activation metric is the number of users that have stored at least one file in one dropbox folder on one device\n\nMetric Examples: number of connections made, number of times an action is performed, number of steps completed\n\nEngagement: What is the breadth and frequency of user engagement?\n\nDepth of their usage: How often are they using your product? Is it above or below the average users frequency?\nBreadth of their usage: Are they performing every action that’s possible with your product? Are they favoring some more than others? What if you have multiple products? Are they using all of them?\nMetric Examples: daily, weekly and monthly active users, time spent in a session, session frequency, actions taken in the product\n\nRevenue: How many people are paying for your product?\n\n** Remember, a business should optimize for the value they bring to their customers, not the revenue they generate. And, if their customers are deriving a lot of value from the business, willingness to pay will be a natural byproduct\nMetric Examples: % of paid customers; average revenue per customer; conversion rate of trial to paid customers; number of transactions completed; shopping cart abandonment rates; ad-metrics like click-through-rate and conversion rate (crucial for ads based businesses)\n\nRetention/Renewal: How often are your people coming back?\n\nMetric Examples: % of users coming back to your platform each day, month, year (product dependent); churn rates; customer lifetime value\n\nReferral: How many customers are becoming advocates?\n\nMetric Examples: Net Promoter Score, Viral Coefficient i.e. the average number of people that your users refer you to\n\nAlso see\n\nSurvey, Design &gt;&gt; Response Scales &gt;&gt; Net Promoter Score\nAlgorithms, Product &gt;&gt; Net Promoter Score",
    "crumbs": [
      "KPIs"
    ]
  },
  {
    "objectID": "qmd/location-selection.html",
    "href": "qmd/location-selection.html",
    "title": "Location Selection",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Location Selection"
    ]
  },
  {
    "objectID": "qmd/location-selection.html#sec-locsel-misc",
    "href": "qmd/location-selection.html#sec-locsel-misc",
    "title": "Location Selection",
    "section": "",
    "text": "Customer research, market expertise & experience, and competitor location analysis can all help inform the important criteria for your business\nTool to calculate population density within a certain radius of a location\nMight be more useful to aggregate smaller geographies into overlapping circular areas to compare candidates\n\n\nWould have to decide how to handle geographies that are only partially enclosed in a circlular area\n\nUse a percentage?\nInclude the whole thing?",
    "crumbs": [
      "Location Selection"
    ]
  },
  {
    "objectID": "qmd/location-selection.html#sec-locsel-fact",
    "href": "qmd/location-selection.html#sec-locsel-fact",
    "title": "Location Selection",
    "section": "Factors",
    "text": "Factors\n\nUnderstanding of the demographic or economic factors that must be in place to be successful\nExamples of questions\n\nDo you need a large population?\nHigh income population?\nHigh presence of certain age brackets?\nDo you rely on office worker foot traffic?\nIs the presence of certain business types important (restaurants, healthcare facilities)?\n\nNon-data factors\n\nAppropriate accessibility (car traffic/foot traffic, street frontage)\nSignage\nAvailability and size of space\nCost/affordability",
    "crumbs": [
      "Location Selection"
    ]
  },
  {
    "objectID": "qmd/location-selection.html#sec-locsel-locprof",
    "href": "qmd/location-selection.html#sec-locsel-locprof",
    "title": "Location Selection",
    "section": "Location Profiles",
    "text": "Location Profiles\n\nThese are created for existing stores and locations or potential new stores\nExample: Workforce and Demographic \n\nOther potential variables\n\nCustomer median driving distance\n\nMay also inform on the correct census geography to use\n\nDistance_to highways, business district, etc.",
    "crumbs": [
      "Location Selection"
    ]
  },
  {
    "objectID": "qmd/location-selection.html#sec-locsel-anal",
    "href": "qmd/location-selection.html#sec-locsel-anal",
    "title": "Location Selection",
    "section": "Analysis",
    "text": "Analysis\n\nUse thresholds for any profile variables to help narrow the group of potential candidate locations to a managable number\n\nMight be useful to fit a decision tree to develop rules to use as thresholds\nExample\n\nZip Code Population of 25,000+\n\nMay want to use census geographies other than zip code\n\nCity Population of 150,000+\nGrowing Population\nHousehold Income of $75,000+\nHigh percentage of the population in the workforce\nHigh economic activity\nPrimary industry of employment in White Collar\nPercentage millennial population\nRestaurant density\n\n\nScore candidate locations\n\nCreate weights for important profile variables and then calculate scores for each candidate location\n\nMethods for creating weights\n\nWing it with domain knowledge\nCoefficients from a regularized regression of KPI ~ standardized_profile_vars could be used as weights\n\nOr feature importance, shapely values, etc. from tree model\n\nCorrelation or association statistics as weights\n\n\nOrder scores highest to lowest\n\nIf more than one location is considered, then group_by a suitably-sized geography\n\n\nCluster candidate location profiles with current successful stores\n\nCandidate locations that are in the same cluster as your stores are the ones that should be considered\nProminent features of the cluster(s) may indicate which profile variables are more important than others\n\nTake top-n candidates and dig deeper:\n\nCompetitor analysis\n\nExample questions\n\nHow many competitors exist is location?\nWhere are they located?\nHow satisfied are consumers with the options that exist today?\n\nWhich competitors are most popular, suggesting we may want to look in other areas?\ne.g. Google Map, Yelp, etc. reviews of competitors at this location\n\n\n\nMapping may illuminate other considerations\n\ne.g. One location has large swaths of uninhabitable land — is there enough population density for us to be successful?\n\nHow close are these locations to your other stores?\n\nCould one leach customers from the other?\n\nExamine profiles of final candidates\n\nWhat are the primary differences?\nWhat are the best features?",
    "crumbs": [
      "Location Selection"
    ]
  },
  {
    "objectID": "qmd/logistics-demand-planning.html",
    "href": "qmd/logistics-demand-planning.html",
    "title": "Demand Planning",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Logistics",
      "Demand Planning"
    ]
  },
  {
    "objectID": "qmd/logistics-demand-planning.html#sec-log-demfcast-misc",
    "href": "qmd/logistics-demand-planning.html#sec-log-demfcast-misc",
    "title": "Demand Planning",
    "section": "",
    "text": "Also see\n\nReinforcement Learning for Inventory Optimization Series III: Sim-to-Real Transfer for the RL Model | by Guangrui Xie | Jan, 2023 | Towards Data Science\n\nUsed in Sales & Operations Planning (S&OP) process where the forecasts generated in this stage trickle down to other stages including supply planning, production scheduling, logistics planning, and inventory optimization.\n\nOver-Forecasting can lead to excess working capital being tied up in inventory.\nUnder-Forecasting can stock out customers or lead to a scramble to make orders using more expensive raw materials and ship on short notice with more expensive transportation\n\nChallenges\n\nChanges in the business environment — for instance, substitutes may replace a product, driving its demand down. Alternatively, new applications for a product may cause demand to go up compared to historical trends.\nShifts in business model — an organization may change its operating model and business strategy. For example, a chemical company may choose to shift their business from commodity chemicals to more specialty products, and so the historical demand patterns may not hold anymore.\nData availability — historical sales data, customer and product hierarchy data, and real-time orders data may be stored on disparate systems.\nData quality — this may include issues such as inaccurate data due to entry errors or data being captured at different and inconsistent granularity across data elements.\n\nPrepare forecasts for alternate scenarios\n\nTalk with someone on the business side and ask about the potential scenarios within the forecast horizon that could affect the forecast.\nExamples: a 10% inflation spike, supply disruption of a crucial raw material, a natural disaster, etc.\nRank these scenarios and factors in order of importance, so that you can prioritize.\n\nHorizons (Typical)\n\nShort-Term: 1 - 3 months\nMid-Term: 6 - 18 months\nLong-Term: 3 - 5 years",
    "crumbs": [
      "Logistics",
      "Demand Planning"
    ]
  },
  {
    "objectID": "qmd/logistics-demand-planning.html#sec-log-demfcast-artnot",
    "href": "qmd/logistics-demand-planning.html#sec-log-demfcast-artnot",
    "title": "Demand Planning",
    "section": "Article Notes",
    "text": "Article Notes\n\nApplying Prediction Adjustment to Demand Forecasts\n\nMisc\n\nNotes from Are Your Demand Forecasts Hurting Profits and Service Levels\nBlue Dot Thinking’s WasteNot API service ($) WasteNot (logistics analytics firm) uses prophet for demand forecasting which consistently under-predicts, so they use an adjustment for better forecasts\n\nPrediction Adjustment - (e.g. for perishables) Calculate an optimal “Buffer Multiplier” value — each predicted value from the statistical forecast is multiplied by the multiplier, resulting in a higher number of units replenished each day\n\nVariables used:\n\nUnit Sale Price/Unit Cost\nHistorical Variability\nShelf-Life (seconds)\nStock Levels\n\nThey didn’t show the formula, but I’m guessing their modeling residuals with these variables.\n\n\n\n\n\n\nCase Study: Automating Stock Replenishment\n\nMisc\n\nNotes from Case Study: Applying a Data Science Process Model to a Real-World Scenario\nA VERY detailed article that goes through a scenario of step-by-step planning and execution of changing a manual stock replenishment process to an automated one\nAs a guide, they use DASC-PM (DAta SCience - Process Model) - a structured and scientific process for project management\n\nDetermine Feasibility\n\nProject manager tries to examine whether the project can fundamentally be classified as feasible and whether the requirements can be carried out with the available resources.\nExpert Interviews: Is the problem in general is very well suited for the deployment of data science and are there corresponding projects that have already been undertaken externally and also published?\nData science team: Are there a sufficient number of potentially suitable methods for this project and are the required data sources are available?\nIT department: check the available infrastructure and the expertise of the involved employees.\n\nDemand Forecasting Model\n\nRequirements:\n\nAccuracy of 75%. This means that the forecasts for quantities of each product should deviate from actual requirements by no more than 25%.\nProduce monthly planning cycles and quantify the need for short-term and long-term materials\n\nData sources: order histories, inventory and sales figures for customers, and internal advertising plans\nFeatures: seasonality, trends, and market developments\nForecasts regenerated every month\n\nLoad Forecasts into Internal Planning Software\n\nProjections will be analyzed and, if need be, supplemented or corrected.\n\nPlanners can make their corrections during the first four working days of the month.\nThe final planning quantity will ultimately be used by the factories for production planning.\n\nExample: IBM Planning Analytics\n\nAllows for the creation of flexible views where the users can personally choose their context (time reference, product groups, etc.)and adjust calculations in real-time.\nSounds like expensive optimization software with a snazzy UI\n\nFinal plans are loaded into the Data Lake after processing by the planning teams so they can be referenced in the future.\n\nUser Integration\n\nUsers are included in the development from the beginning to ensure technical correctness and relevance and to ensure familiarity with the solution before the end of the development phase.\nSimple line and bar charts for processes and benchmarks are used, along with tables reduced to what is most important.\nPlanners get training sessions to help them interpret the forecasts and classify their quality.\nComplete documentation is drafted\n\nTechnical part: data structures and connections\nContent part: jointly prepared with the users to describe the usage of the data product\n\n\nPost-Development Phase\n\ni.e. Maintenance of the project\nConstant automated adjustment of the prediction model to new data\nVarious parameters such as the forecast horizon or threshold values for the accuracy of the prediction can be made by the planners\nProblems occurring after the release of the first version are entered via the IT ticket system and assigned to the data science area\nAt regular intervals, it is also checked whether the model still satisfies the expectations of the company or whether changes are necessary.\n\n\n\n\nDealing with Shocks\n\nMisc\n\nForecasting shocks is difficult for an algorithm\nNotes from Why Good Forecasts Treat Human Input as Part of the Model\n\nPreprocessing\n\nIt can be better to smooth out (expected) shocks in the training data and then add an adjustment to the predictions during the dates of the shocks.\nThe smoothed out data will help the algorithm produce more accurate predictions for days when there isn’t an expected shock.\n\ne.g. Kalman filter with parameters for seasonality, trends from {{tsmoothie}}\n\nNothing special about this smoother. Probably just the method in tsmoothie that performed best for them (which is a good reason to use it).\n\n\nApproaches for manually replacing oulier values\n\nReplace the outlier week with the most recent week prior to the dip as a proxy for what should have happened.\nUse domain knowledge\n\ne.g. Marketing departiment is expecting 10% growth the week after it’s new promotion\n\nReplace with one of the previous methods, then apply a smoothing algorithm\n\n\nShows the Kalman filter by itself fails to replace the outlier dip with the normal, expected peak, but the manual adjustment + Kalman gives us what we want\n\n\nExamples of shocks that may need training data to have manual adjustments and not be smoothed by an algorithm\n\nOne-time spikes due to abnormal weather conditions\nOne-off promotions\n\nReplace the outlier week with the most recent week prior to the dip as a proxy for what should have happened.\n\nA sustained marketing campaign that is indistinguishable from organic growth.\n\n\nPrediction Adjustment\n\nExtreme events (i.e. unprecedented promotions or weather), where the expected impact is well outside of any historical data, may prove impossible for forecasting methods to produce an adequate forecast\n\nSince it’s outside the range of historical data, building a more complex model or just including weather or promotional features won’t help.\n\nRequires using domain expertise of the event to use to adjust the predictions\nMake sure to build-out the code infrastructure so that each manual adjustment should just feel like adding another input to the model.",
    "crumbs": [
      "Logistics",
      "Demand Planning"
    ]
  },
  {
    "objectID": "qmd/logistics-demand-planning.html#sec-log-demfcast-stakho",
    "href": "qmd/logistics-demand-planning.html#sec-log-demfcast-stakho",
    "title": "Demand Planning",
    "section": "Stakeholder Questions",
    "text": "Stakeholder Questions\n\nMisc\n\nQuestions for the stakeholder before starting a forecasting project.\n\nFor the most part, the questions with bold terms are ones that I consider essential for producing a valuable forecasting model.\nThe other questions might be optional depending on your role.\n\ni.e. Unless you’re asked to review the overall decision making process and offer recommendations, the questions surrounding the decision-making process are not strictly necessary for producing a forecasting model.\n\n\nNotes from: An Inquisitive Journey into Forecasting\n\nDecisions and Actions\n\nWhat decisions is the forecast going to support? (e.g. plan for manufacturing, set inventory target, allocate marketing budget?) How are they made currently? Is there a clear and stable decision process?\n\nA “clear and stable decision process” would mean that the decision-making process ,for which the forecast is an input, is the same each and every time a decision is made.\ne.g. Adjusting a forecast based on vibes or politics or casually adding or removing other inputs to the decision-making process would not be stable\n\nWho are the key decision-makers? What does the decision process look like? Are there stakeholders who should have been involved but were left out?\nWhat are the subsequent actions related to the decision? Who carries them out? How do people manage and carry out these actions in their day-to-day business? How well do actions adhere to decisions?\nHow often is the decision made? Is it the right frequency for the actions? (too frequent then it disrupts day-to-day business; too infrequent then we are slow to react to crucial trends and events)\n\ne.g. Should the predicted values be weekly, monthly, quarterly, etc.?\n\nHow much further into the future do people anticipate when making decisions? (i.e. the forecast horizon) Is it the right horizon for the actions? (too short then business will be forever in reactive mode, too long then forecasting team will just waste their energy on useless reports)\nAt which granularity do people make decisions? At which granularity do people take actions? Are they the same? If not, how do people bridge the gap to carry out their day-to-day business?\n\ni.e. Is there hierarchical or group aspect to the forecast?\n\nWhat are the decisions and actions closely related to this one? Are those decisions providing input, constraints to the current one, or does the forecast enable them?\nFrom a process perspective, how do they ensure that targets and forecasts are not interfering with each other? Is there a clear distinction between targets and forecasts in the company?\n\nI think this one is asking if the forecasts get fudged to get inline with sales targets (?) which goes back to first question about a stable decision-making process.\n\n\nValue\n\nAlso see Metrics for details on drilling down into this subject\nHow is the variable being forecasted related to the company’s OKRs/KPIs?\nWhat value can a good decision bring about and what’s the cost of a bad decision? Can I make a dollar estimation of the impact? What would be a reasonable way to make this estimation, and what are the assumptions behind it?\nUsing the framework of value estimation established in the previous question, can I connect the metrics of forecast quality (e.g. accuracy, bias) to a dollar impact? What would be a reasonable level of impact to sponsor the upgrade / rebuild of the forecast model? How much model improvement would that entail?\nCan I improve / refine the decision-making process without improving the forecast model? If so, how big is the value impact?\nAre there any non-financial benefits, e.g. time and efficiency? Better adoption of data-driven decision making and higher trust in data and forecast? Internal capability building which paves the way for future technologies?\n\nThe time and efficiency in generating forecasts for the old forecasting model/process can be used as inputs in determining the overall ROI of your model.\n\nDoes the 80/20 rule apply to the value impact somewhere? For example, the majority of the value can come from a rather restricted scope of products, business lines, distribution channels, geographies and decisions.\n\nIf you’re creating a model with hierarchical or group data and for example, some products are more important for generating profit for the company than others, then the models forecasting those products best take priority in any sort of model selection process.\n\n\nDrivers and Proxies - While causal variables are good predictors (assuming they’re leading indicators), using only causal variables as predictors isn’t necessarily important to produce accurate predictions, but they are important for determining which levers can be adjusted by stakeholders in order to increase or decrease the future values of the forecast variable (e.g. sales). Use the answers to these questions to create a DAG. Then, models can be fit to measure their effects.\n\nWhat are the key factors which impact the forecast target? (e.g. for demand, it could be price, promotion, marketing, channel distribution, etc.)\nWhat is the mechanism between those factors and the forecast target? Can we establish causal links either through common sense or industry expertise?\nAmong those factors, which are the causal drivers and which are barely correlated?\nWhat are the factors that we, as an organization, can control? (e.g. price and promotion)\nWhat are the factors that we cannot control but we can observe?\nWhat are the factors that are neither controllable nor observable, but we know they have an important impact on demand?\n\nThese will require proxy variables. (Also see Causal Inference &gt;&gt; Misc &gt;&gt; Partial Identification)\n\nWhat are the factors that influence the forecast target with a delay (i.e. forward-looking indicators / signals), and how long is the lead time?\nHow do people make forecasts currently? Do they use any of those factors and their relations to help them make the forecast? Is there a data-driven model in place?\n\nData\n\nDo we have historical data for all the above-mentioned factors? How much history do we have for each factor? If direct data is not available, is it possible to find good proxies from public data sources or through data acquisition?\nCan we obtain forward-looking indicators for the forecast target from sources like research organizations, public data, or internal resources (such as pricing and promotions)? Additionally, how far into the future do these data provide insights?\nWhere do these data (historical & forward-looking) come from? Are they generated internally from an ERP system, inputted manually by staff, or obtained from external data providers?\nIs there a data cleaning process (ETL) that is used to prepare the data we have? How does it work? What are the main functions that it performs? Would this process mask key information that we want to get from the data? If that’s the case, is it possible to obtain the raw data?\nWhat’s the granularity of those data? Is it as granular as the decisions? How often do those data get updated? Is it updated frequently enough for the decision process?\nDo we have multiple sources for the same factor? If so, what are the key differences? Do different business units and teams use different sources or versions? What’s the reason behind it?\nDoes the data cover all products / business units? If not, what’s the reason for the low coverage? Can we calculate the percentage of the value pool which have proper data coverage? Does that look big enough? If not, can we think of ways to improve it?\n\n“Value pool” refers the profits of the company and if there is sufficient data collection on the products that create the most value for the company.\n\nWhat’s a normal range for key business data and KPIs (e.g. accuracy, bias)? How do business people generally account for the fluctuation of those metrics?\n\nDraw Conclusions from the Answers\n\nDo I know what this forecast is built for, what’s the value? Where and how do we generate value? How is it linked to the forecast quality?\nHow can the forecast be designed (frequency, horizon, granularity, scope) to assist and guide the process? How do decision-makers and operational staff interact with the forecast? Does that make sense?\nDo I fully understand how my forecast target is generated? What do the list of drivers, causal graph, and influence lag imply about my modeling? Does that fit well with the process?\nDo I have enough & good-quality data to build the first model? What might go wrong with the data, how to fix it (if possible)? How can we improve data quality, availability over time (how long would that be)?",
    "crumbs": [
      "Logistics",
      "Demand Planning"
    ]
  },
  {
    "objectID": "qmd/logistics-demand-planning.html#sec-log-demfcast-intdem",
    "href": "qmd/logistics-demand-planning.html#sec-log-demfcast-intdem",
    "title": "Demand Planning",
    "section": "Intermittent Demand",
    "text": "Intermittent Demand\n\nMisc\n\nNotes from\n\nCMAF FFT: Intermittent Demand Forecasting (Video)\n\nFrom authors of “Intermittent Demand Forecasting. Context, Methods and Applications” (see your book shelf)\n\n\nPreferrable to avoid intermittence by aggregating data to a category that’s higher in the product hierarchy or lengthen the frequency.\n\nNot always possible. For example, different SKUs have different lead times, so aggregating products into categories with the same protection intervals can be complicated.\n\nProtection Interval is your horizon, which equals the length of the review period + length of the lead time\n\nSee Order-Up-To (OUT) Level Policy for details\n\n\n\n\nCompound Distributions\n\nTarget\n\nIncidence/occurrence (of Demand): Poisson and Bernoulli\nDiscrete positive Demand interspersed with zeros: Neg. Binomial\n\nSounds like zero-inflated, censored distribution\n\n\nOptions\n\nDiscrete Compound Poisson (aka Stuttering Poisson): Poisson + Geometric\nNegative Binomial: Poisson + Logarithmic\n\nLumpy data\n\nData that’s intermittent and has extreme variability in demand sizes\nNegative Binomial built to handle this type of data\n\n\n\n\nForecasting Mean Demand\n\nParametric\n\nModeling\n\nExponential Smoothing\n\nPopular method but bad for intermittent forecasting\nBiased on “issue points” (see video for more details)\n\niETS\n\nCaptures multiplicative seasonality in data that has zeroes (as long as seasonal indices can be estimated somehow)\nADAM ebook chapter, post, real world use case on adding an occurrence variable to an ETS model to handle intermittent data\n\nThe use case paper also includes a GAMLSS with truncated Normal distribution model with code that performed well\n\nUses {probcast} which has functions around gams, gamlss, and boosted gamlss models from {mgcv}, {mboost}, {gamlss}, etc.\n\n\nExample: From iETS: State space model for intermittent demand forecasting\n\nModel\nset.seed(7)\ny &lt;- c(rpois(10,3),rpois(10,2),rpois(10,1),rpois(10,0.5),rpois(10,0.1)) |&gt;\n    ts(frequency=12)\n\nlibrary(smooth)\niETSModel &lt;- adam(y, \"YYN\", occurrence=\"direct\", h=5, holdout=TRUE)\n\nmodel = “YYN”: Tells function to select the best pure multiplicative ETS model based on the information criterion\n\nAICc used by default, see discussion in Section 15.1 of the ADAM book\n\noccurrence = “direct”: Specifies which of the demand occurrence models to build.\n\n“direct” by default, the function will use the same model for the demand probability as the selected for the demand sizes.\n\nSo, for example, if we end up with ETS(M,M,N) for demand sizes, the function will use ETS(M,M,N) for the probability of occurrence.\n\nTo change this, you would need to use the oes() to specify the model to predict the probability of occurrence\n\nSee examples in Section 13.4 of the ADAM book.\n\n\nh = 5: Sets the forecast horizon to 5 steps ahead\nholdout = TRUE: Says to keep the last 5 observations in the holdout sample\n\nResults\nsummary(iETSModel)\n\n#&gt; Model estimated using adam() function: iETS(MMN)\n#&gt; Response variable: y\n#&gt; Occurrence model type: Direct\n#&gt; Distribution used in the estimation: \n#&gt; Mixture of Bernoulli and Gamma\n#&gt; Loss function type: likelihood; Loss function value: 71.0549\n#&gt; Coefficients:\n#&gt;       Estimate Std. Error Lower 2.5% Upper 97.5%  \n#&gt; alpha   0.1049     0.0925     0.0000      0.2903  \n#&gt; beta    0.1049     0.0139     0.0767      0.1049 *\n#&gt; level   4.3722     1.1801     1.9789      6.7381 *\n#&gt; trend   0.9517     0.0582     0.8336      1.0685 *\n\n#&gt; Error standard deviation: 1.0548\n#&gt; Sample size: 45\n#&gt; Number of estimated parameters: 9\n#&gt; Number of degrees of freedom: 36\n#&gt; Information criteria:\n#&gt;      AIC     AICc      BIC     BICc \n#&gt; 202.6527 204.1911 218.9126 206.6142 \n\nSelected the iETS(M,M,N) model\n“Mixture of Bernoulli and Gamma” tells us that the Bernoulli distribution was used for the demand occurrence (this is the only option), while the Gamma distribution was used for the demand sizes (this is the default option, but you can change this via the distribution argument).\n\nDemand Forecast\n\nforecast(iETSModel, h=5, interval=\"prediction\", side=\"upper\") |&gt;\n    plot()\n\nside = “upper”: Says we only want the upper prediction interval\n\nFor pure multiplicative models, simulations are used.\nSince the lowest bound is zero, the upper bound is what’s useful for stocking decisions\n\n\nOccurence Forecast (i.e. Stock or Not)\n\nforecast(iETSModel$occurrence, h=5) |&gt; plot()\n\nProbability reaches roughly 0.2 over the next 5 months (i.e. we might sale once every 5 months).\nIf we think that this is too low then we should discontinue the product. Otherwise, if we decide to continue selling the product, then it makes more sense to generate the desired quantile of the cumulative demand over the lead time.\n\nCumulative Demand Forecast\nforecast(iETSModel, h=5, interval=\"prediction\", side=\"upper\", cumulative=TRUE)\n\n#&gt;       Point forecast Upper bound (95%)\n#&gt; Oct 4      0.3055742          1.208207\n\nBased on the estimated model, we need to have two items in stock to satisfy the demand over the next 5 months with the confidence level of 95%.\n\n\n\n\nMethods\n\nSince these are methods they can’t incorporate predictive variables or produce prediction intervals\nCroston Method\n\nInversion Bias: we believe the mean demand is higher than predicted\n\nSBA (Syntetos-Boylan Approximation)\n\nRecommended method\nCorrects Croston bias\nSupported by empirical evidence\n\nTemporal Aggregation: Overlapping vs Non-Overlapping\n\nself-improving mechanism\nSee video for details (although it wasn’t discussed in great detail)\n\n\n\nNon-Parametric (aka Empirical)\n\nBootstrapping\n\nResample in blocks or resample independently\n\nIndependently is how you normally see bootstrapping\nWith blocks, bin your sequential data into non-overlapping partitions and resample independently within each partition.\n\n\n\n\n\n\nForecasting Demand Variance\n\nUsing the variance of the forecast error of the protection interval to estimate the variance of demand over the protection interval\n\nClassical method is to calculate the variance of the errors over each review period and aggregate to the get the variance over the protection interval, but the above method is better empirically and theoretically\n\n\n\n\nDiagnostics\n\nAlso see\n\nMetrics\nDecision Impact Metrics\n\nDo NOT use metrics based on absolute errors (e.g MASE, MAPE) by themselves\n\nMinimization of these metrics, by themselves, can result (i.e. over half of your values are zeros) in always recommending 0 value forecasts\nCan be used in conjunction with bias correction measures\n\nScaled Mean Squared Error\nRelative Root Mean Squared Error\nLook at the predictive distributions instead of just the point forecasts\nAccuracy Implication Metrics - Measure the effect of the forecast on inventory\n\n\nY Axis: 1 - Fill_Rate; X Axis: Average on-hand inventory\nEach method is a forecasting model\nThink the Fill Rate is estimated from a simulation using the method’s forecast and a range of average SOH values as parameters",
    "crumbs": [
      "Logistics",
      "Demand Planning"
    ]
  },
  {
    "objectID": "qmd/logistics-demand-planning.html#sec-log-demfcast-metrics",
    "href": "qmd/logistics-demand-planning.html#sec-log-demfcast-metrics",
    "title": "Demand Planning",
    "section": "Metrics",
    "text": "Metrics\n\nSee\n\nDemand Forecasting &gt;&gt; Intermittent Demand &gt;&gt; Diagnostics &gt;&gt; Accuracy Implication Metrics\nDecision Impact Metrics\n\nProcess\n\nDiscuss with your manager and team the purpose of the demand forecast. Is its goal to set accurate sales targets? To lower inventory levels? What are the underlying concerns behind these forecast numbers?\n\nIf increasing sales is the top concern, then maybe adjusting the forcast towards the higher end of the prediction interval is better. Increased inventory leads to less stockouts which leads to more sales\nIf decreasing inventory is the top concern, then maybe adjusting the forcast towards the lower end of the prediction interval is better as this will lead to decreased inventory and decreased cost.\n\nCreate a simple business case to translate forecast accuracy metrics.\n\nThis will allow you to present your benefits of your model in business terms to a non-technical decision maker\nExample: Decreasing MASE by 50%\n\nCurrent situation:\n\nExisting forecasting method produces an average MASE of 20%.\nExcess inventory costs average $50,000 per month due to inaccurate forecasts.\nStockouts lead to an average of $20,000 in lost sales per month.\n\nAfter Improving Forecast:\n\nBy reducing MASE to 10% (decrease of 50%), the company can expect:\n\nReduced excess inventory costs: Inventory costs could be reduced by $25,000 per month (50% decrease).\nIncreased sales: More accurate forecasts allow for optimal stock levels, potentially leading to a $10,000 increase in monthly sales.\n\n\n\n\nDiagram the decision making process\n\nIn many cases, the decision-maker or others will make subjective adjustments to forecasts\nThe diagram should include the decision being made, the inputs for the decision, who’s making the decision, and the outcomes that follow.\nEach adjustment should be documented, so as to better assess how where improvement needs to made in the process.\n\n\nWasteNot method for judging its adjusted forecast model vs prophet\n\n\nChange in the Number of Stock-Out Days (lower is better)\nPercent Change in Service Level (higher is better)\nChange in Profit (higher is better)\nChange in Wastage (lower is better)\nChange in Inventory Costs (lower is better)",
    "crumbs": [
      "Logistics",
      "Demand Planning"
    ]
  },
  {
    "objectID": "qmd/logistics-demand-planning.html#sec-log-demfcast-dim",
    "href": "qmd/logistics-demand-planning.html#sec-log-demfcast-dim",
    "title": "Demand Planning",
    "section": "Decision Impact Metrics",
    "text": "Decision Impact Metrics\n\nMisc\n\nAlso see\n\nMetrics\nDemand Forecasting &gt;&gt; Intermittent Demand &gt;&gt; Diagnostics &gt;&gt; Accuracy Implication Metrics\n\nDI metrics for all generated forecasts can be used to calculate a quarterly, semestrial, or annual ROI for the product planning department\n\nROI = sum(\\(DI_{na}\\))- (cost of people, tools, etc. used by department to generate forecasts)\n\nProbabilistic Forecasts + DI metrics\n\nIdentify parameters (i.e. whatever value you’re forecasting) with the highest economic risk\n\nEconomic risk is related to the size of the PI of the forecast\n\nInvestigate potential causes for the risk and find solutions\nExample\n\nCalculate\n\n\\(\\min(DI_a)\\) - the costs associated with the forecast at the 5% percentile\n\\(\\max(DI_a)\\) - the costs associated with the forecast at 95% percentile.\n\nCalculate economic risk\n\\[\n\\text{risk}_{\\text{econ}} = \\left|\\max(DI_a) - \\min(DI_a)\\right|\n\\]\n\n\n\n\n\nForecast Accuracy Metrics Are Flawed\n\nIndustry benchmarks for these metrics aren’t useful. Each business regardless whether they’re in they’re in the same industry has diverse business strategies (e.g. size of portfolio, product & brand positioning)\nConstraints and business rules have to be considered and not just a forecast metric\n\ne.g. Allowed pack sizes, the minimum order quantity, the storage finite capacity, holding/excess costs, shortage costs, fixed costs, etc.\n\nForecast Accuracy (FA) metrics are difficult to understand for normals\n\nUsing Decision Impact (DI) metrics can help eliminate:\n\n“The forecast is always wrong!”, “The forecast is too this”, “The forecast is not enough that”, “What does 70% FA mean?”, “Is this good or bad?”\n\n\nForecast Value Added (FVA)\n\nFVA: The change in a forecasting accuracy metric that can be attributed to a particular process or participant in the forecasting process.\n\nExample\n\n\nStatistical model has 5% FVA vs Naive forecast\nAdjusted Statistical Forecast has 2% FVA vs Naive Forecast\n\nForecast gets manually adjusted by management\n\n\n\nReasons why FVA shouldn’t be used by itself\n\nThe difference in Forecast Accuracy (FA) metrics (e.g. MAPE) of the production forecasting algorithm and the naive forecasting algorithm is usually weighted by portfolio revenue, volume or number of items\nA FA metric is not a key business performance indicator (KPI)\n\nFA has little correlation with business performance.\nTherefore, improving FA does not mean you are generating value.\nCosts may increase, decrease or remain the same as accuracy changes.\n\nFA metrics can contradict each other\n\nSwitching from one FA metric to another could profoundly alter your FVA results.\n\n\n\n\n\n\nDI Components\n\nDecision-based Components - components that take into account the decision being made using the forecast\n\nExample: One weather forecast predicts 4 inches of rain but another forecast predicts 0 inches of rain. The next day it rains 1 inch.\n\nIf the decision being made was whether to take an umbrella, the first forecast is the best forecast even though its error is worse than the second forecast.\n\nComponents\n\nDecision Function - For any forecast input, simulate the decision process and evaluate the quality of the final decision\nDecision Impact (DI) - Metric that defines how decision quality is measured\n\nUsually in terms of financial cost\nA “North Star” type metric\n\n\n\nDecision Cost Function: The cost function is used to score each stock replenishment decision based on its true business impact usually in terms of financial cost\n\nExample: Walmart Retail Data (M5)\n\nThe Decision Cost is the sum of following factors:\n\nOrdering, shipping and handling cost (Fixed Costs)\n\nFulfilling an order generates costs for the ordering, preparation, expedition, transportation, etc.\nLet’s assume these costs represent $40 per $1000 of purchase value.\n\nHolding cost (Excess Costs)\n\nHolding costs are associated with the storage of unsold inventories.\nLet’s assume the annual holding cost is 10% of the inventory value (valued at purchase price), e.g. 0.19% per week.\n\nShortage cost\n\nWhen demand exceeds the available inventory, both the demand and customer goodwill may be lost.\nAs retailers propose a wide range of similar products, a part of the excess demand is carried on to other products.\nLet’s assume that only half of the sales will effectively be lost. The shortage cost could then be measured as 50% of each lost sale gross margin.\n\n\n\nOther Considerations\n\nReplenishment Strategy\n\nLeadtime\nOrder Cycle\nReplenishment Policy\nSafety Stock\n\nAdditional Product Information\n\nGross Margin\nPack Sizes\nInitial Inventory\n\nExample: Using the safety stock value\n\n\nComputational Costs\n\nAlgorithm training time\nCompute size\nData pipeline\n\n\n\nRequired Forecasts\n\n“Actual” Forecast: Forecast from a candidate model that will potentially go into production\n“Naive” Forecast: Forecast from a simple method (e.g. seasonal-naive, simple moving average, etc.) or a previous used method\n\n\n\n\nSteps\n\nDecide how best to measure the Decision Impact as related to the forecast\n\ne.g. Financial Cost\n\nFormulate a decision cost function\n\ne.g. Fixed Costs + Holding Costs + Shortage Costs (See Walmart example above)\n\nGenerate forecasts on a test/assessment set.\nCalculate Decision Impact Costs\n\nActual Cost (\\(DI_a\\)): Cost after applying the decision cost function to the “actual” forecast (See above)\nNaive Cost (\\(DI_n\\)): Cost after applying the decision cost function to the “naive” forecast (see above)\nOracle Cost (\\(DI_o\\)): Cost after applying the decision cost function to the observed values\n\nThis would be the costs of a forecast in which we had perfect knowledge. (So probably just Fixed Costs)\n\n\nCalculate Decision Impact metrics:\n\nEarned Value (aka Forecast Value Added) (\\(DI_{na}\\))\n\\[\nDI_{na} = DI_n - DI_a\n\\]\nUnearned Value (aka Yet-To-Be-Earned Value)(\\(DI_{ao}\\))\n\\[\nDI_{ao} = DI_a -DI_o\n\\]\n\nThe value that could still be gained by improving forecasts\n\nTotal Earnable Value (\\(DI_{no}\\))\n\\[\nDI_{no} = DI_n - DI_o\n\\]\n\nThe range of earnable value\n\nProportion of the Earned Value\n\\[\n\\frac{DI_{na}}{DI_{no}}\n\\]\nProportion of Yet-To-Be-Earned value\n\\[\n\\frac{DI_{ao}}{DI_{no}}\n\\]",
    "crumbs": [
      "Logistics",
      "Demand Planning"
    ]
  },
  {
    "objectID": "qmd/logistics-demand-planning.html#sec-log-demfcast-pffpp",
    "href": "qmd/logistics-demand-planning.html#sec-log-demfcast-pffpp",
    "title": "Demand Planning",
    "section": "Profit Functions for Perishable Products",
    "text": "Profit Functions for Perishable Products\n\nThe newsvendor problem is a class of problems, where the product can only be sold one day, after which it goes to waste. So this is appropriate, for example, for perishable products in retail\nNotes from An Integrated Method for Estimation and Optimisation; associated paper\nIf we order more than needed, we will have holding costs. In the opposite case, we will have shortage costs.\n\nBased on these costs and the price of product, we can find the optimal amount of product to order, that will give the maximum profit.\n\nInstead of a two-stage problem (see DoorDash section): Optimising the forecast model via MSE or any other conventional loss and then solving the optimisation problem, we could estimate the model via maximization of the specific profit function, thus obtaining the required number of product orders directly.\nCalculate profit as a linear function\n\\[\n\\pi(q_t,y_t) =\n\\left\\{ \\begin{array}{lcl}\npy_t - vq_t - c_h(q_t - y_t) & \\text{for}\\;\\; q_t \\geq y_t \\\\\npq_t - vq_t - c_s(y_t-q_t) & \\text{for}\\;\\; q_t &lt; y_t\n\\end{array}\\right.\n\\]\n\n\\(y_t\\): Actual Sales\n\\(p\\): Price of the Product\n\\(q_t\\): Order Quantity\n\\(v\\): Cost of Production\n\\(c_h\\): Holding Cost\n\\(c_s\\): Shortage Cost\n\nExample\nlibrary(greybox)\n# Generate artificial data\nx1 &lt;- rnorm(100,100,10)\nx2 &lt;- rbinom(100,2,0.05)\ny &lt;- 10 + 1.5*x1 + 5*x2 + rnorm(100,0,10)\nourData &lt;- cbind(y=y,x1=x1,x2=x2)\n# Define price and costs\nprice &lt;- 50\ncostBasic &lt;- 5\ncostShort &lt;- 15\ncostHold &lt;- 1\n# Define profit function for the linear case\nlossProfit &lt;- function(actual, fitted, B, xreg){\n    # Minus sign is needed here, because we need to minimise the loss\n    profit &lt;- -ifelse(actual &gt;= fitted,\n                    (price - costBasic) * fitted - costShort * (actual - fitted),\n                    price * actual - costBasic * fitted - costHold * (fitted - actual));\n    return(sum(profit));\n}\n# Estimate the model\nmodel1 &lt;- alm(y~x1+x2, ourData, loss=lossProfit)\n# Print summary of the model\nsummary(model1, bootstrap=TRUE)\nCoefficients:\n            Estimate Std. Error Lower 2.5% Upper 97.5% \n(Intercept)  36.5177    14.2840    2.7783    51.4844 *\nx1            1.3622    0.1622    1.1909      1.7528 *\nx2            3.3423    2.7810    -6.5997      5.9101\n\nInterpretation: With the increase of the variable x1, the orders should change on average by 1.36\nPlot\n\nplot(model1, 7)\n\nFigure above corresponds to the orders (purple) and would cover roughly 90.91% of cases (black), so that we would run out of product in approximately 10% of cases, which would still be more profitable than any other option.\n\n\nNonlinear case\n\nSee link to associated paper above\nThe only thing that would change is the loss function, where the prices and costs would depend non-linearly on the order quantity and sales.",
    "crumbs": [
      "Logistics",
      "Demand Planning"
    ]
  },
  {
    "objectID": "qmd/logistics-demand-planning.html#sec-log-demfcast-drdsh",
    "href": "qmd/logistics-demand-planning.html#sec-log-demfcast-drdsh",
    "title": "Demand Planning",
    "section": "Case Study: DoorDash",
    "text": "Case Study: DoorDash\n\nNotes from https://towardsdatascience.com/managing-supply-and-demand-balance-through-machine-learning-70d4f0808617\nProblem Definition\n\nSupply and demand imbalance  (i.e. drivers and food orders)\nEffects\n\nFor consumers, a lack of driver availability during peak demand is more likely to lead to order lateness, longer delivery times, or inability to request a delivery and having to opt for pick up.\nFor Dashers, a lack of orders leads to lower earnings and longer and more frequent shifts in order to hit personal goals.\nFor merchants, undersupply leads to delayed deliveries, which typically results in cold food and a decreased reorder rate.\n\n\n2-Stage Solution\n\nForecast supply and demand\nOptimize supply of workers with demand of food orders\n\nOptimization Strategies\n\nBalancing at the delivery level means every order has a Dasher available at the most optimal time\n\nConsumer preferences and other changing conditions in the environment, such as traffic and weather, make it difficult to balance supply and demand at the delivery level\n\nBalancing at the market level means there are relatively equal numbers of Dashers and orders in a market but there are not necessarily optimal conditions for each of these groups at the delivery level.\n\n\n\nMetrics\n\nNumber of hours required to make deliveries during a time period\n\nOptimize keeping delivery durations low and Dasher (drivers) busy-ness high\nAble to account for regional variation driven by traffic conditions, batching rates, and food preparation times.\n\nUnits: hourly or parts-of-day (e.g. breakfast, lunch, dinner)\n\nThere’s too much variation during a day in order to aggregate to a higher level metric. Demand and supply would be artifically smoothed.\n\nExample:\n\nSunday at dinner time in New York City, and we estimate that 1,000 driver hours are needed to fulfill the expected demand. We might also estimate that unless we provide extra incentives, only 800 hours will likely be provided organically. Without mobilization actions we would be undersupplied by about 200 hours.\n\nOptimization\n\nAdjust supply of drivers by incentivizing with pay bonues during high demand hours\n\n\nForecast Supply and Demand\n\nDemand\n\nLightGBM\nPredictors\n\nInformation about population size, general traffic conditions, number of available merchants, climate, and geography\nCharacter variables were replaced with embedding vectors\n\n\nSupply\n\nCounterfactual to understand how to make tradeoffs between supply and costs\n\nHow will supply levels change if we changed incentive levels so that we can ?\nI assume “incentive_level” is a variable in the supply forecast model, so this could just be adjusted in “newdata” and a  prediction (or maybe during a training session?) made to see the effects on supply.\n\n\n\nOptimization\n\nInput supply and demand predictions and generate a set of optimal actions\nMixed-Integer Programming (MIP)\n\nAlso see Optimization, Equation Systems\nLinear Optimization ({ompr})\nEasy to formalize, implement, and explain to stakeholders\nCustom objective function for minimizing undersupply with several constraints.\n\nCan be configured to favor either profitability (profit per customer?) or growth (increase in orders per customer?)\n\nOr maybe its about drivers — profitability (just enough incentive to get just enough drivers?) or growth (not sure what the “driver” angle is here)\n\nConstraints\n\nNever allocate more than one incentive in a particular region-time unit.\nNever exceed the maximum allowable budget set by our finance and operations partners.\nRegional\n\nDifferent budgets, custom penalties, exclusion criteria for which units should not be included in the optimization, or incentive constraints that are guided by variability of the inputs.\n\n\n\n\nOptimizer must account for uncertainty\n\nCity B’s forecast distribution has substantial uncertainty, and it’s mean says it will have enough drivers (i.e. oversupply)\nCity A’s forecast distribution is more certain, and it’s more likely to be undersupplied.\nW/o taking uncertainty into account, the optimizer will not take into account that there’s a sizeable chance B will be undersupplied\nTaking uncertainty into account sometimes causes an over-allocation of resources to these uncertain regions (small areas, fewer orders, fewer drivers, larger variance)\nThey do something with resampling to solve this but I didn’t quite understand it.",
    "crumbs": [
      "Logistics",
      "Demand Planning"
    ]
  },
  {
    "objectID": "qmd/logistics-inventory-management.html",
    "href": "qmd/logistics-inventory-management.html",
    "title": "Inventory Management",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Logistics",
      "Inventory Management"
    ]
  },
  {
    "objectID": "qmd/logistics-inventory-management.html#sec-log-invman-misc",
    "href": "qmd/logistics-inventory-management.html#sec-log-invman-misc",
    "title": "Inventory Management",
    "section": "",
    "text": "Goal: develop a replenishment policy that will minimize your ordering, holding and shortage costs.\n\nOUT level policy is widely recommended as a replenishment policy\nCosts (Also see EOQ, DIM &gt;&gt; DI Components)\n\nOrdering Costs ($ per order): Fixed cost to place an order due to administrative costs, system maintenance or manufacturing costs\n\nAlso see\n\nHolding Costs ($ per unit per unit of time): All the costs required to hold your inventory (storage, insurance, and capital costs)\n\nForecast PI widths serve as a proxy for inventory holding costs and provides valuable input for setting a target service level\n\nShortage/Stock-out Costs ($ per unit): The costs of not having enough inventory to meet the customer demand (Lost Sales, Penalty)\n\n\nPackages\n\n{planr} - Uses opening inventory, sales forecasts and supply variables to calculate projected inventory and projected coverage calculations (article)\n\nQuestion: “Due to the complexity/cost of maintaining inventory management system. Would it be sufficient to just set a safety stock level and replenish once the SKU dips below that level?”\n\nThe more products you handle, the more an inventory management system matters. If you have 1000s or 10s of 1000s of different products, it makes a large difference whether you do demand forecasting along with implementing the Order-Up-To Level Policy, etc.\n\nVariables of interest that need to be forecasted for various decisions\n\nStock/No Stock - Should we continue to stock a product or discontinue the product and just let the current stock dwindle to zero.\n\nMean of Demand\n\nReplenishment - How much product should we restock\n\nMean and Variance of Demand\nForecasts in conjunction with a hypothesized demand distribution (parametric) vs build-up of the empircal distribution via bootstrapping (non-parametric)\n\nReturns - Customer returning products\n\nNet Demand which is equal to Demand - Returns\n\nCan be forecasted itself or by forecasting Demand and Returns separately.\n\n\nLast Time Buy (LTB) - the supplier’s “last call” for a part or component. The final chance an enterprise will have to buy the part before the supplier stops producing it.\n\nRate of Demand Decline",
    "crumbs": [
      "Logistics",
      "Inventory Management"
    ]
  },
  {
    "objectID": "qmd/logistics-inventory-management.html#sec-log-invman-terms",
    "href": "qmd/logistics-inventory-management.html#sec-log-invman-terms",
    "title": "Inventory Management",
    "section": "Terms",
    "text": "Terms\n\nActive References - A product may have multiple SKUs. All SKUs would be needed in order to calculate stats for that product. The active reference for a product is one SKU that encompasses all other SKUs.\n\ne.g. Last season’s dress “C” has been replaced by the new dress “D”. Even though both dresses are identical, they have different SKUs. Dress D’s SKU will be the active reference. Therefore, if the retailer sold two units of C in the past and three units of D this week, Nextail will show D having sold five units.\nAlso seen this term used when referring to all unique SKUs on an order sheet as the total active references.\n\nFill rate (aka Fulfillment Rate)- The percentage of orders that you can ship from your available stock without any lost sales, backorders, or stockouts.\n\n\\[\n\\text{Fill Rate} = \\frac{\\text{Total Orders Shipped}}{\\text{Total Orders Placed}} \\cdot 100\n\\]\n\nOn average, companies typically maintain a fill rate of about 85%-95%. But ideally, you should strive for a fill rate between 97% and 99%.\n\nFulfillment Rate - See Fill Rate\nLead Time (aka performance cycle) - Time required between the creation of a replenishment order and the effective store replenishment\nLinked Lines (aka Silent Switches) - Refers to when several products in a retailer’s inventory are commercially equivalent. In other words, when identical products are identified by multiple SKUs.\n\nExample: Continuity products are ordered over multiple seasons or years or when large orders are split among different suppliers\n\nOrder Components\n\n\nOrder: The “shopping basket” full of items you’ve just purchased.\nLines: The different products within your order, recognized by warehouses as each individual Stock Keeping Unit (SKU) or Universal Product Code (UPC) number.\nUnits: The quantity of each line.\n\nPerformance Cycle - See Lead Time\nReference - ID for a product (e.g. SKU)\nReplenishment cycle - Cycle between replenishment (i.e. restocking) orders\nRotations - Speed at which products enter and exit the warehouse\n\nHigh Rotation: Units enter and exit continuously. These items are in high demand.\nMedium Rotation: Units enter and exit in smaller volumes than those in High Rotation.\nLow Rotation: These are the items that spend the most time in the warehouse, and are in low demand.\n\nSafety stock - Extra inventory held by a retailer or a manufacturer in case demand increases unexpectedly. This means it’s additional stock above the desired inventory level that you would usually hold for day-to-day operations.\n(Cycle) Service Level (Z) - The probability that the amount of inventory on hand during the lead time is sufficient to meet expected demand – that is, the probability that a stockout will not occur.\n\nAlso see figure in Fill Rate\n\nSilent Switches - See Linked Lines\nSKU - Stock Keeping Unit - Usually a bar code that has all the information to distinguish it from another product. These attributes can include manufacturer, description, material, size, color, packaging, and warranty terms. When a business takes inventory of its stock, it counts the quantity it has of each SKU.\nStock Outs - Out of stock events\nSupply Chain - A network of processes and stock locations built to deliver services and goods to customers.\nWastage - Where supply greatly outstrips demand, and the product expires\nS&OP - Sales & Operations Planning",
    "crumbs": [
      "Logistics",
      "Inventory Management"
    ]
  },
  {
    "objectID": "qmd/logistics-inventory-management.html#sec-log-invman-safstk",
    "href": "qmd/logistics-inventory-management.html#sec-log-invman-safstk",
    "title": "Inventory Management",
    "section": "Safety Stock (aka Buffer Stock)",
    "text": "Safety Stock (aka Buffer Stock)\n\nMisc\n\nNotes from Article\nCalculation of Safety Stock can be more useful than trying to improve forecast accuracy for intermittent(or sporadic) product time series (lotsa zeros).\n\n\n\nReasons for Safety Stock\n\nDemand Uncertainty - Every retailer and manufacturer will have products that sell well all year round and products that fluctuate in demand.\nLead time Uncetrainty - Deliveries arriving earlier or later than expected, a safety stock formula will help you to cover unexpected delays and demand fluctuation to maintain a consistent output.\n\nThe lead time is usually a distribution and not a constant, so it needs to be recorded in order to get a sample standard deviation thats used the safety stock formula\nFactors:\n\nDeciding what to order or produce\nApproval time\nSubmitting a purchase requisition\nEmailing vendors\nManufacturing and processing of the product\nDelivery time from vendor\nIncoming inspection time\nTime it takes to put on the shelf\nAny additional time required to return to the start of the next cycle\n\n\n\n\n\nStockouts\n\nSafety stock determinations are not intended to eliminate all stockouts — just the majority of them Usually caused by:\n\nChanges in consumer demand\nIncorrect stock forecasts\nVariability in lead times for raw materials\n\nCosts Due to Stockouts\n\nLoss of revenue\nLoss of gross profit\nLoss of customers\nReduced market share\nPoor efficiency\nStrained supplier and retailer relationships\n\n\n\n\n(Cycle) Service level (Z)\n\nHigher service level \\(\\rightarrow\\) more safety stock\nIndependently choose a service level for groups of products based things such as strategic importance, profit margin, or dollar volume.\n\nThe retail industry aims to maintain a typical service level of between 90% and 95% depending on the product\n\nExample:\n\n\nAt 95 percent service level, expect:\n\n(D1) for 50 percent of replenishment cycles, not all cycle stock will be depleted and safety stock will not be needed\n(D2) for 45 percent of replenishment cycles, the safety stock will suffice.\n(D3) and for 5 percent of replenishment cycles, expect a stockout.\n\nKind of a confusing diagram but I think the y-axis is total stock and x-axis represents time (kinda sorta)\n\nStock dwindles as product is sold as the cycle ends, then stock is replenished after an order and begins to dwindle again.\n\n\n\n\n\nSafety Stock Equations\n\nWhen the demand interval doesn’t equal lead time interval\n\nUsed to mitigate demand variability and lead time variability (σLT see next formula) is very small or zero\n\\[\n\\text{Safety Stock} = Z \\cdot \\sigma_D \\cdot \\sqrt{\\frac{PC}{T_I}}\n\\]\n\n\\(Z\\): z-score of the service level (1-sided, upper-tail)\n\ne.g. 95% service level \\(\\rightarrow\\) Z = 1.64; 90% service level \\(\\rightarrow\\) Z = 1.28\nqnorm(0.95) = 1.644854, qnorm(0.90) = 1.281552\n\n\\(PC\\): Performance Cycle, another term for total lead time\n\\(T_I\\): Time increment used for calculating standard deviation of demand\n\\(\\sigma_D\\): Standard Deviation of Demand.\n\nExample: if the standard deviation of demand is calculated from weekly demand data and the total lead time including review period is three weeks.\n\n\\(PC\\) = 21 days (3 weeks)\n\\(T_I\\) = 7 days (weekly data)\nSafety Stock = Z * √3 * σD\n\nExample: Desired service level = 95%; seven-day manufacturing time and the one day needed to arrive at the warehouse; Standard deviation of weekly demand = 10 rolls\n\nSafety stock = 1.64 * √[8/7] * 10 rolls\n\nSafety stock = 18 rolls\n\n\n\nWhen the lead time varies and demand variability (σD see previous formula) is very small or zero\n\\[\n\\text{Safety Stock} = Z \\cdot \\sigma_{LT} \\cdot D_{\\text{avg}}\n\\]\n\n\\(Z\\): z-score of the service level (1-sided, upper-tail)\n\ne.g. 95% service level \\(\\rightarrow\\) Z = 1.64; 90% service level \\(\\rightarrow\\) Z = 1.28\nqnorm(0.95) = 1.644854, qnorm(0.90) = 1.281552\n\n\\(\\sigma_{LT}\\): Standard Deviation of the Lead Time\n\\(D_{\\text{avg}}\\): Demand Forecast\n\nDepends what the frequency of the series is, but you want an estimate of the total demand between orders\n\nExample: If orders are made monthly and you forecast weekly sales, then your horizon is likely monthly and you sum around 4 weeks of point estimates to get D.\n\nWhen both demand variability and lead time variability are present\n\nDemand and lead time variability are independent\n\\[\n\\text{Safety Stock} = Z \\cdot \\sqrt{\\frac{\\sigma_D^2 PC}{T_I} + (\\sigma_{LT}D_{\\text{avg}})^2}\n\\]\nDemand and lead time variability are not independent\n\\[\n\\text{Safety Stock} = \\left(Z \\cdot \\sigma_D \\cdot\\sqrt{\\frac{PC}{T_I}}\\;\\right) + (Z \\cdot \\sigma_{LT} \\cdot D_{\\text{avg}})\n\\]\nDemand variability is the dominant influence on safety stock requirements.\n\nWith the recognition of what factors dominate an equation, it becomes easier to focus improvement efforts\n\n\n\n\n\nIssues\n\nSometimes recommended safety stock volumes are larger than business leaders are comfortable having\n\nAlternative or Supplement: Order Expediting\n\nReduce safety stock volume by keeping small amounts of expensive products and rely on air freight to cover peaks in demand. The cost of shipping a small percentage of total demand via air can be minimal compared to the cost of carrying large amounts of safety stock of the valuable material on an ongoing basis.\n\nAlternative or Supplement: Make-to-Order (MTO) or Finish-to-Order (FTO) production environment\n\nIf lead times allow, MTO eliminates the need for most safety stock. Meanwhile, FTO allows for less differentiation in safety stock than finished-product inventory, which lowers demand variability and reduces safety stock requirements.\nFTO and MTO also are well suited for situations where customers are willing to accept longer lead times for highly sporadic purchases.",
    "crumbs": [
      "Logistics",
      "Inventory Management"
    ]
  },
  {
    "objectID": "qmd/logistics-inventory-management.html#sec-log-invman-eoq",
    "href": "qmd/logistics-inventory-management.html#sec-log-invman-eoq",
    "title": "Inventory Management",
    "section": "Economic Order Quantity (EOQ)",
    "text": "Economic Order Quantity (EOQ)\n\nAKA Wilson Formula\n\\[\nQ = \\sqrt{\\frac{2DS}{H}}\n\\]\n\n\\(Q\\): EOQ units (i.e. quantity of product to be ordered)\n\\(D\\): Demand in units (Typicall on an annual basis)\n\\(S\\): Order Cost (per Purchase Order)\n\\(H\\): Holding Costs (per unit, per year)\n\nThe ideal order quantity a company should purchase to minimize inventory costs such as holding costs, shortage costs, and order costs\n\nUsually used for purchase ordering (not production)\nAssumes demand, ordering, and holding costs remain constant over time\n\nGoal: minimize the cost of ordering and holding stock, while still meeting demand and service level requirements\nCosts of Ordering\n\nPlacing your order\nDelivery\nTransportation\nReceiving the order\n\nCosts of Holding Stock\n\nPaying for stock in advance\nWarehousing\nStorage\nDepreciation",
    "crumbs": [
      "Logistics",
      "Inventory Management"
    ]
  },
  {
    "objectID": "qmd/logistics-inventory-management.html#sec-log-invman-outpol",
    "href": "qmd/logistics-inventory-management.html#sec-log-invman-outpol",
    "title": "Inventory Management",
    "section": "Order-Up-To (OUT) Level Policy",
    "text": "Order-Up-To (OUT) Level Policy\n\n\nTime intervals (i.e. Review Interval) trigger a replenishment, not reorder points\n\nLength depends on the industry\nExamples\n\nManufacturing: 1 month\nRetail: 1 week\n\n\nMisc\n\nNotes from\n\nCMAF FFT: Intermittent Demand Forecasting (Video)\n\nFrom authors of “Intermittent Demand Forecasting. Context, Methods and Applications” (see your book shelf)\n\nShould have more details on OUT replenishment model\n\n\nUT-Dallas “Basestock Model CH. 13” Slides\n\nBased on Cachon & Terwiesch book, “Matching Supply with Demand” (link)\n\n\n\nInventory Position (IP) = Stock-on-Hand - backorders + On-Order-Inventory\nAfter every Review Interval, the OUT gets optimized according to Replenishment variables\n\nMean and Variance of Demand are estimated\nForecasts in conjunction with a hypothesized demand distribution (parametric) vs build-up of the empircal distribution via bootstrapping (non-parametric)\n\nSee Demand Forecasting &gt;&gt; Intermittent Demand &gt;&gt; Forecasting Mean Demand\n\nGiven updated variables, place order that raises IP to OUT level (S)\n\nProtection Interval is period that you should have enough inventory to cover.\n\nThe forecast horizon which equals the Review Interval + Lead Time\n\nEvaluate service at each level of an order: orders, lines, units.\n\nLines (SKU Level)\n\nWhether to use Cycle Service Level (CSL) or Fill Rate (ReadyRate (?) is also a possibility)\n\nCSL - probability of not going out of stock -  not realistic but easy to calculate\nFill Rate measures true service offered to customers, but more involved in its application\nAlso see Safety Stock for more details on CSL and Fill Rate",
    "crumbs": [
      "Logistics",
      "Inventory Management"
    ]
  },
  {
    "objectID": "qmd/logistics-inventory-management.html#sec-log-invman-reordpt",
    "href": "qmd/logistics-inventory-management.html#sec-log-invman-reordpt",
    "title": "Inventory Management",
    "section": "Reorder Point",
    "text": "Reorder Point\n\nThe reorder point is the threshold amount of inventory at which you need place an replenishment order\n\nonce an item’s stock falls below PAR level, an optimised order quantity is generated\n\nComponents used to determine a reorder point\n\nSafety Stock\nReorder Point Formula\nPeriodic Automatic Replacement (PAR)\n\nReorder Point = Safety Stock + (Davg × Lead time)",
    "crumbs": [
      "Logistics",
      "Inventory Management"
    ]
  },
  {
    "objectID": "qmd/logistics-inventory-management.html#sec-log-invman-warmang",
    "href": "qmd/logistics-inventory-management.html#sec-log-invman-warmang",
    "title": "Inventory Management",
    "section": "Warehouse Management",
    "text": "Warehouse Management\n\nMisc\n\nPicking operations account for the largest proportion of the total warehouse costs (up to more than 60%). This is why the design of these areas is of such importance.\nThe closer the high demand or large goods are to the loading and unloading docks, the lower the handling costs.\n\nMaterial Flow Types\n\nSimple Flows: To understand how these movements work, we can examine the simplest possible flow, which takes place when units sent by the supplier are used, without dividing these up.\n\nMedium flows: Movements start to become more complex with this type of flow. It is normally found in warehouses with single or combined picking operations, generally with the supply of full pallets.\n\nComplex Flows: There are warehouses with different working areas, depending on the types of product and their consumption. They normally have intermediate handling areas and can require various operations that in turn need flows of a certain (and at times great) complexity. This diagram shows an example of this type of facility and the loading movements that occur there.\n\n\nWarehouse Optimization\n\n\n\nA: High Rotation, B: Medium Rotation, C: Low Rotation\n(Top) Pareto Plot shows how High Rotation products are classified as those accounting for 20% of total products but also 80% of sales (point on the curve)\n(Bottom) Shows how the “A” products have been positioned closest to the loading and unloading area.",
    "crumbs": [
      "Logistics",
      "Inventory Management"
    ]
  },
  {
    "objectID": "qmd/logistics-supply-chain.html",
    "href": "qmd/logistics-supply-chain.html",
    "title": "Supply Chain",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Logistics",
      "Supply Chain"
    ]
  },
  {
    "objectID": "qmd/logistics-supply-chain.html#sec-log-supch-misc",
    "href": "qmd/logistics-supply-chain.html#sec-log-supch-misc",
    "title": "Supply Chain",
    "section": "",
    "text": "Notes from: 4 Smart Visualizations for Supply Chain Descriptive Analytics",
    "crumbs": [
      "Logistics",
      "Supply Chain"
    ]
  },
  {
    "objectID": "qmd/logistics-supply-chain.html#sec-log-supch-flowdist",
    "href": "qmd/logistics-supply-chain.html#sec-log-supch-flowdist",
    "title": "Supply Chain",
    "section": "Flow Distribution of Units Between Production Areas and Markets",
    "text": "Flow Distribution of Units Between Production Areas and Markets\n\n\nData\n\nSource: the production facility name (left-side)\nTarget: the market supplied (right-side)\nUnits: the number of items flowing (width of bars)\n\nInterpretation\n\nIndia is the biggest country for production output\nJapan market demand is mainly supplied locally\nUSA and Germany do not have local production facilities",
    "crumbs": [
      "Logistics",
      "Supply Chain"
    ]
  },
  {
    "objectID": "qmd/logistics-supply-chain.html#sec-log-supch-netopt",
    "href": "qmd/logistics-supply-chain.html#sec-log-supch-netopt",
    "title": "Supply Chain",
    "section": "Network Optimization",
    "text": "Network Optimization\n\n\nX-Axis: each column represents a demand scenario (i.e. there are 50 demand scenarios in this example)\nY-Axis: are the production/supply locations\nA blue box means that that location is included in the optimal configuration of locations for that scenario\n\ne.g. In scenario 1, having a low capacity facility in India and a high capacity facility in India is optimal for this scenario.\n\nI think this viz can be done with {waffle} using geom_waffle without theme_enhance_waffle\nSimulate how the variability of demand in various markets (e.g. 50 scenarios) affects the optimal distribution of production/supply locations\n\nHopefully a configuration of locations will be optimal for a preponderance of scenarios. Assuming each scenario is equally important, that configuration of locations is the optimal choice.\n\nOr I guess you could weight each scenario by frequency or something. Maybe you have a distribution of scenarios from which you drawing from.\n\n\nLinear programming\n\nAlso see Optimization, Equation Systems\nSet decision variable, objective function\nList the constraints according to the demand for each market\nSolutions are indicator variables for production/supply locations and whether they are 1 or 0.\n\nThere should be a boolean variable for a high capacity location and low capacity location in each country\nFor each variable, 1 indicates that location should be built or that it should be in operation at that particular capacity",
    "crumbs": [
      "Logistics",
      "Supply Chain"
    ]
  },
  {
    "objectID": "qmd/logistics-supply-chain.html#sec-log-supch-pareto",
    "href": "qmd/logistics-supply-chain.html#sec-log-supch-pareto",
    "title": "Supply Chain",
    "section": "Pareto Plot",
    "text": "Pareto Plot\n\n(Unfinished Note but I think there is an examle of a Pareto Plot in Warehouse Management)\nData\n\n\n“BOX” is the number of box/packs picked of that product (“SKU”) for that order (“ORDER_NUMBER”) on that date (“DATE_FORMAT”)\n\nPreprocessing\n\nSum the number of boxes picked per SKU\nSort your data frame by descending order on BOX quantity\nCalculate the cumulative sum of BOX\nCalculate the cumulative number of SKU",
    "crumbs": [
      "Logistics",
      "Supply Chain"
    ]
  },
  {
    "objectID": "qmd/low-hanging-fruit.html",
    "href": "qmd/low-hanging-fruit.html",
    "title": "Low Hanging Fruit",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Low Hanging Fruit"
    ]
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-misc",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-misc",
    "title": "Low Hanging Fruit",
    "section": "",
    "text": "What is a data scientist? (GPT-3)\n\nA data scientist analyzes and interprets complex data sets to inform business decision-making. They use statistical techniques, programming, and machine learning to extract insights and patterns from data. Data scientists often clean and preprocess data, create models, and develop algorithms to solve specific problems. They collaborate with cross-functional teams to identify business challenges and formulate questions that can be addressed through data analysis. They also communicate findings to non-technical stakeholders through visualizations and reports, aiding in strategic decision-making. Additionally, data scientists may be involved in designing experiments, testing hypotheses, and continuously improving models to enhance predictive accuracy. Their role requires a combination of analytical skills, domain knowledge, and proficiency in programming languages such as Python or R.\n\nMost companies don’t have a holistic view of their business performance\n\nLarge proportion of the budget is spent on campaigns that generate little to no return\nMonitoring business performance in real-time and swiftly halting initiatives with negative returns will give a company more time and cash to survive the unstable economy.\n\nWhy a business needs statistics\n\nIdentifying Opportunities\n\nFind new markets, promote better customer retention, increase sales, and identify sales opportunities\nIncrease efficiency by finding duplication in the market or pinpointing areas that you want to eliminate from your current strategic plan\n\nUnderstanding customer behavior\n\nLooking at their buying patterns and how they use your products or services\nCan make decisions on the type of products or services you should offer to your customers\nIdentify new opportunities for product development by looking at areas that may require further research and study\n\nDetermining the correct target market\n\nIdentify the best possible choice for your business because all decisions must be made around this key area\nHelp determine whether your current target market is as profitable as it should be\n\nEvaluating products or services\n\nDetermine what your customers are using or how they are accessing your products to find new ways to improve or alter a product or service you offer\n\nMaking better decisions\n\nAllows you to make better decisions about business changes, hiring new employees, marketing, and advertising strategies",
    "crumbs": [
      "Low Hanging Fruit"
    ]
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-wyo",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-wyo",
    "title": "Low Hanging Fruit",
    "section": "What you offer",
    "text": "What you offer\n\nSee Job, Organizational and Team Development\nIn the beginning the value comes from creating a more automated, streamlined, and reliable process and developing metrics that better measure aspects of the business.\nInsights are an iterative process which depends heavily on the quality, quantity, and predictiveness of the data\nA data scientist automates and improves data-centric processes such as collecting, storing, accessing, analyzing, and reporting.\nDS shrinks the haystack rather than finds the needle\n\n“Using machine learning or data science to predict &lt;thing&gt; is just really hard. There are tons of reasons &lt;thing&gt; may not happen, many of which you have no chance of observing in your data. What you CAN do with comparative ease is predict relative likelihoods though. So you say find me the 20000 records most likely to do &lt;thing&gt; according to my model. Within them you’re going to find more of &lt;thing&gt; than you would in a random sample. If you can make / save money doing this, you win.” McBain",
    "crumbs": [
      "Low Hanging Fruit"
    ]
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-raaa",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-raaa",
    "title": "Low Hanging Fruit",
    "section": "Report Automation and Alerts",
    "text": "Report Automation and Alerts\n\nSee Job, Reports\nAutomate daily, monthly, quarterly etc. reports\nTypical components: analysis, report template, automation script, shiny dashboard\nAlerts\n\nSend text or email (i.e. RPushbullet (text) or blastula (email))\nUseful when dashboards designed to monitor things, aren’t being used. This way a manager is only bothered when something is wrong\nExamples\n\nWhen an overtime limit is reached, send alert to department head\nWhen inventory reaches a limit, send alert",
    "crumbs": [
      "Low Hanging Fruit"
    ]
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-churn",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-churn",
    "title": "Low Hanging Fruit",
    "section": "Churn",
    "text": "Churn\n\nSee Algorithms, Marketing &gt;&gt; Churn\nWhy are customers leaving and how much is it costing\nLogistic regression if just a probability of whether a customer will churn is wanted\nDecision tree if it needs to presented to an executive or stakeholder not familiar with statistics",
    "crumbs": [
      "Low Hanging Fruit"
    ]
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-mom",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-mom",
    "title": "Low Hanging Fruit",
    "section": "Monitoring Operational metrics",
    "text": "Monitoring Operational metrics\n\nSee\n\nJob, Organizational and Team Development &gt;&gt; Developing a data strategy &gt;&gt; OKRs\nKPIs\n\nThis is about understanding the levers that drive your business, then using them to improve operations. A key aspect is making data available and understandable to those who are making daily decisions.\nMonitoring business performance in real-time allows businesses to swiftly halt initiatives with negative returns\nMetrics are also ways for organizations to align stakeholders around one vision of the world and one common goal.\nMetrics help companies have clarity, alignment and prioritization in what to build.\nMetrics help a company decide how to build the product once they’ve prioritized what to build.\nMetrics help a company determine how successful they are and hold them accountable to an outcome.\nExamples:\n\nDaily updates about key metrics.\nIs there a drop in conversion rate?\nAre we meeting our KPIs?",
    "crumbs": [
      "Low Hanging Fruit"
    ]
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-pdf",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-pdf",
    "title": "Low Hanging Fruit",
    "section": "Product Demand Forecasting",
    "text": "Product Demand Forecasting\n\nSee Logistics &gt;&gt; Demand Forecasting\nhttps://www.remixinstitute.com/blog/automated-demand-forecasts-using-autocatboostcarma-in-r/#.XX_q6ShKi1s (some pitch material)\n“Over 20% of Amazon’s North American retail revenue can be attributed to customers who first tried to buy the product at a local store but found it out-of-stock”\nA demand forecasting system can help save a lot of dollars in term of workforce management, inventory cost and out of stock loss.\n\nSee inventory bkmk folder, SCPerf pkg. Has inventory/supply chain functions that utilize demand forecast as a variable.\n\nSales forecasts by store\nMonthly units sold by SKU (item)\neCommerce\n\nDaily or Weekly Visits by Channel, Source, and/or Medium using Google Analytics data\nDaily Customers, New Customers, Revenue, and Units Sold by Channel",
    "crumbs": [
      "Low Hanging Fruit"
    ]
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-custseg",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-custseg",
    "title": "Low Hanging Fruit",
    "section": "Customer Segmentation",
    "text": "Customer Segmentation\n\nSee Marketing &gt;&gt; Workflow &gt;&gt; Find the Ideal Customer Profile (ICP) and target them\nIncreases marketing efficiency by helping to indicate which campaigns are more likely to succeed with certain groups of customers\nThe company can achieve a higher return on ad spend with a smaller marketing budget, yielding a higher profit margin and more room for market expansion.\nForm marketing hypotheses based on cluster characteristics and test these hypotheses by varying campaigns based on the customer’s cluster membership.\n\nRecommend a product they’re likely to purchase, a multi-buy discount, or on-boarding them on a loyalty scheme (e.g. rewards program)\nOnce you know who your customers are and what their value is to your business, you can:\nPersonalize your products and services to better suit your customers’ needs\nCreate Communication Strategies tailored to each segment\nFocus Customer Acquisition to more profitable customers with messages and offers more likely to resonate with them\nApply Price Optimization to match customer individual price sensitivity\nIncrease Customer Retention by offering discounts to customers that haven’t purchased in a long time\nEnhance Customer Engagement by informing them about new products that are more relevant to them\nImprove your chance to Cross-sell and Up-sell other products and services by reaching out for the right segment when they’re more likely to respond\nTest which type of incentive a certain segment is more likely to respond to (e.g. pricing discounts, loyalty programs, product recommendation, etc.)",
    "crumbs": [
      "Low Hanging Fruit"
    ]
  },
  {
    "objectID": "qmd/manufacturing.html",
    "href": "qmd/manufacturing.html",
    "title": "Manufacturing",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Manufacturing"
    ]
  },
  {
    "objectID": "qmd/manufacturing.html#sec-manuf-distrmanuf",
    "href": "qmd/manufacturing.html#sec-manuf-distrmanuf",
    "title": "Manufacturing",
    "section": "Distributed Manufacturing (DM)",
    "text": "Distributed Manufacturing (DM)\n\nNotes from Online Scheduling Approach for Distributed Additive Manufacturing\nGlobal Mass Production vs Global Distributed Production\n\nDynamic Allocation of Production Orders (POs)\n\n\nProduction order inputs delivery location, spare part to be manufactured, quantity needed\nThe only thing being locally modeled seems to be production time\n\nTravel Time from PC to Delivery Location is a request from an API\n\nSeems like there should be a cost element here. Freight charges if delivery is outside the company. If manufacturing and delivery is local though, then maybe the only real cost is gas and that would indirectly included in the Travel Time calculation\n\nQueue time should be something that’s monitored and can be looked up in a table\nSet-up time should be something that’s considered a constant depending on the spare part\n\n\nImplementation\n\n\nFASTEN is a manufacturing software company\n\nNot sure if this tool is used to feed simulated POs to the pipeline or if it’s used to optimize the objective function or both\n\nComponents in this project make sense to me but these arrows don’t all make sense to me\n\nBenchmark\n\nThe null model was one where the PC with shortest distance from PC to delivery location was chosen every time\nThe metric was Average Wait Time to receive the spare part after being ordered",
    "crumbs": [
      "Manufacturing"
    ]
  },
  {
    "objectID": "qmd/marketing.html",
    "href": "qmd/marketing.html",
    "title": "Marketing",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-misc",
    "href": "qmd/marketing.html#sec-mark-misc",
    "title": "Marketing",
    "section": "",
    "text": "Notes from\n\nhttps://hbr.org/2021/07/why-you-arent-getting-more-from-your-marketing-ai?ab=seriesnav-spotlight\nhttps://hbr.org/2021/07/how-to-design-an-ai-marketing-strategy?utm_source=Data_Elixir&utm_medium=social\n\nMost marketing AI addresses segmentation, targeting, and budget allocation.\nTools to engage customers: Salesforce, Marketo, Braze, Facebook Ads, or Google Ads\n\nGoogle and Facebook are perennially at the top of charts for highest return on ad spend\n\nwww.singular.net may be useful resource to check from year to year\n\nExample: run a Facebook Ad campaign for these at-risk customers\n\nManual: Create a SQL query, pull a list from the data warehouse, download it to CSV, and upload it into Facebook Ads or build an integration\nAutomate:\n\nReverse ETL\n\nI think this process is described in the “Manual” part (above)\n\nPlatforms: Flywheel\n\n\n\nFacebook enables smart mobile user acquisition for mobile brands with 2 methods:\n\nApp Event Optimization (AEO)\n\nLooks for new-to-you people who are similar to customers you already have in your apps at defined stages\n\nValue Optimization (VO)\n\nLooks for people who will spend a certain amount of money in your mobile app",
    "crumbs": [
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-terms",
    "href": "qmd/marketing.html#sec-mark-terms",
    "title": "Marketing",
    "section": "Terms",
    "text": "Terms\n\nAction-Based - Clicks, impressions, and GRP data\nAdstock - An average decaying function for the carryover effect (see Channel Attribution section)\nAdvanced Mobile Measurement (AMM) - Facebook’s tracker. Allowed advertisers to access click-through conversion data through their MMP\nAffiliate marketing - A type of performance-based marketing in which a business rewards one or more affiliates for each visitor or customer brought by the affiliate’s own marketing efforts.\nCarryover Effect - The lag between the time a consumer is touched by an ad and the time a consumer converts because of the ad\nClick-in - When a user click on something to reach your website/app\nClick-out - The last click a user makes that take them to another website/app\nClearing Price - The final price paid for an impression.\nCPC - Cost Per Click - refers to the cost an advertiser pays each time an online user clicks on his or her digital ad\nCPM - Cost Per Thousand Impressions is the amount advertisers are willing to pay for every one thousand impressions they receive\n\\[\nCPM = \\frac{\\text{Campaign Budget} \\cdot 1000}{\\text{Number of Desired Impressions}}\n\\]\neCPM or effective CPM is the predicted revenue earned by a publisher for every one thousand impressions; metric for ad testing\n\\[\neCPM = \\frac{\\mbox{Estimated Earnings} \\cdot 1000}{\\mbox{Total Impressions}}\n\\]\n\nFactors that affect eCPM (for all, more is better)\n\nMonthly website traffic\nNumber of ad networks\n\nFind ad networks offering better deals](https://www.adpushup.com/blog/the-best-ad-networks-for-publishers/) for different geographical locations\n\nViewability Score\n\nMultiple links in this article\n\n\n\nFloor prices - Are traditionally used by publishers to increase the closing price of their auctions. Think these are implemented by publishers with second-price auctions when they feel reductions are too large.\nGRP - Gross Rating Point. A standard measure in advertising, it measures advertising impact.\n\nOne GRP is one percent of all potential adult television viewers (or in radio, listeners) in a market\nCalculatations:\n\nPercent of the Target-Market-Reached multiplied by the Exposure Frequency.\n\\[\n\\begin{align}\n&GRP (\\%) = \\frac{100 \\cdot \\mbox{Impressions} \\;(\\#)}{\\mbox{Defined Population}\\; (\\#)} \\\\\n&GRP (\\%) = 100 \\cdot \\mbox{Reach}\\; (\\%) \\cdot \\mbox{Average Frequency}\\; (\\#)\n\\end{align}\n\\]\nExamples:\n\nif you get advertise to 30% of the target market and give them 4 exposures, you would have 120 GRP.\nIf an average of 12% of the people view each episode of a television program, and an ad is placed on 5 episodes, then the campaign has 12 × 5 = 60 GRPs.\nIf 50% view three episodes, that’s 150 GRPs\n\n\n\nHeader Bidding - Just the name for technical auctioning process behind bidding on ad space on a website.\n\nUsed to be done by waterfall archetecture where ads would pass from one publisher’s website to another until it reached one with a price floor that was below the advertiser’s bidding price\n\nIdeal customer profile (ICP) - Customers who bring in the most long-term value for a company\nIdentifier for Advertisers (IDFA) - a random device identifier assigned by Apple to a user’s device. Advertisers use them to precisely target and track users within apps on iOS devices\nImpression - An instance of each time your ad is shown on a search result page or other site on the Google Network. (i.e. number of people your ad reaches)\n\nEach time your ad appears on Google or the Google Network, it’s counted as one impression.\nIn some cases, only a section of your ad may be shown. For example, in Google Maps, we may show only your business name and location or only your business name and the first line of your ad text.\nYou’ll sometimes see the abbreviation “Impr” in your account showing the number of impressions for your ad.\n\nInventory - the amount of ad space (or the number of advertisements) that a publisher has available to sell. While the term originated from print, it has grown to encompass ad space on the web and on apps and mobile ads\nPrice Floor - The minimum price a publisher will accept for its inventory — ignoring all bids below that price.\nReduction - Money saved in a second-price auction; difference between the bid price and the clearing price\nRPM - similar to eCPM; it’s the amount earned by publishers per thousand pageviews\nTouchpoint - any time a potential customer or customer comes in contact with your brand–before, during, or after they purchase something from you. Interactions with marketing campaigns and the home page provide rich information about who they are and what they like\n\nExamples:\n\nBefore Purchase: Social media, Ratings and reviews, Testimonials, Word of mouth, Community involvement, Advertising, Marketing/PR, visits your website\nDuring Purchase: Store or office, Website, Catalog, Promotions, Staff or sales team, Phone system, Point of sale\nPost Purchase: Billing, Transactional emails, Marketing emails, Service and support teams, Online help center, Follow ups, Thank you cards\n\n\nUTM - Urchin Traffic Monitor - used to identify marketing channels or results from ad campaigns\n\nName comes from Urchin Tracker, a web analytics software that served as the base for Google Analytics.\ne.g. http://yourwebsite.com/your-post-title/?utm_source=google\n\nutm code = string after “?”\n\nseparate each UTM parameter with the ‘&’ sign.\n\nThis person clicked a google ad to get to your site\n\nGoogle URL builder tool\nSee article for me details, best practices, etc.\nParameter types\n\nutm_source - traffic source (e.g. google, facebook, twitter, etc.)\nutm_medium - type of traffic source (e.g. CPC, email, social, referral, display, etc.)\nutm_campaign - campaign name, track the performance of a specific campaign\nutm___content - In case you have multiple links pointing to the same URL (such as an email with two CTA buttons), this code will help you track which link was clicked (e.g utm_content=navlink )\nutm_term - track which keyword term a website visitor came from. This parameter is specifically used for paid search ads. (e.g. utm_term=growth+hacking+tactics)",
    "crumbs": [
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-usecases",
    "href": "qmd/marketing.html#sec-mark-usecases",
    "title": "Marketing",
    "section": "Use Cases",
    "text": "Use Cases\n\nChatbots for lead development, customer support, and cross-selling or upselling\nInbound call analysis and routing, and customer comment and email analysis, classification, and response\nMarketing campaign automation (including emails, landing page generation, and customer segmentation)\nChannel Attribution\n\nMarketing mix modeling (MMM)\nMulti-touch Attribution Modeling (MTA)\nUnified Approach Online product merchandising\n\nPricing Product or service recommendations and highly personalized offers\nProgrammatic digital ad buying ( digital ads are served up almost instantaneously to users)\nSales lead scoring\nSocial-media planning, buying, and execution\nSocial-media sentiment analysis\nTelevision ad placement (partial)\nWeb analytics narrative generation\nWebsite operation and optimization (including testing)\nSales propensity models in customer relationship management (CRM) systems\nAutomated Tasks\n\nSend a welcome email to each new customer\nChatbots on social media platforms\n\nStart with a stand-alone non-customer-facing task-automation app, such as one that guides human service agents who engage with customers.\n\nLess-capable bots can irritate customers. It may be better to have such bots assist human agents or advisers rather than interact with customers.\n\n\nOnce companies acquire basic AI skills and an abundance of customer and market data, they can start moving from task automation to machine learning\n\nApps\n\nStand-alone applications continue to have their place where integration is difficult or impossible, though there are limits to their benefits.\nExample\n\nUsing IBM Watson’s natural language processing and Tone Analyzer capabilities (which detect emotions in text), the application delivers several personalized Behr paint-color recommendations that are based on the mood consumers desire for their space. Customers use the app to short-list two or three colors for the room they intend to paint.\n\n\nIntegrated ML/DL\n\nNetflix - recommendations are on a separate standalone app where the customer needs to request recommendations\nML that coaches a customer service rep through a call",
    "crumbs": [
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-metrics",
    "href": "qmd/marketing.html#sec-mark-metrics",
    "title": "Marketing",
    "section": "Metrics",
    "text": "Metrics\n\nConversion Rates, Bounce Rates, and Average Basket Sizes\nReturn on Ad Spend (ROAS)\n\\[\n\\mbox{ROAS} = \\frac{\\mbox{Sales Revenue}}{\\mbox{Advertising Budget Spend}}\n\\]",
    "crumbs": [
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-custlift",
    "href": "qmd/marketing.html#sec-mark-custlift",
    "title": "Marketing",
    "section": "Conversion Lift Tests",
    "text": "Conversion Lift Tests\n\nAlso see\n\nData Science &gt;&gt; Algorithms, Marketing\nData Science &gt;&gt; Business Plots\n\nRCTs on a customer sample that tries to understand the impact of an ad campaign by randomly showing it to one group of users, with-holding it from another, and looking for a difference in behaviour between the two groups over some predetermined period of time.\n\nExample:\n\nA typical ecommerce brand might want to understand whether their Facebook ads are driving sales that wouldn’t have happened anyway, and so they’d want to track purchases as a KPI in their lift test.\n\n\nGold standard of incrementality measurement\n\nIncrementality Tests also help with re-engagement strategies to highlight the optimal day, post-install, to re-engage users and to ensure the highest incremental lift from your marketing efforts\n\nAlternatives\n\nA/B tests\n\nNo way to measure incrementality\nThe difference seems to be semantics IMO.\n\nWith A/B, the control group gets a “different treatment” and with conversion lift tests, the control group gets no treatment\n\n\nBrand Lift Tests\n\nseek to measure a campaign’s impact on brand metrics, not conversions\n\nGeo-experiments (see below))\n\nCapable of measuring incrementality. Viable alternative to lift tests\n\n\nHow do you not serve ads to an audience (i.e the control group), yet still “own” the ad real-estate?\n\nMethodologies:\n\nIntent-to-treat (ITT) – This method calculates the experiment results based on the initial treatment assignment and not on the treatment that was eventually received (meaning you mark each user for test/control in advance and do not rely on attribution data. You have the “intent” to treat them with ads / prevent them from seeing ads, but there’s no guarantee it will happen).\nGhost ads/bids – This is another example of a randomly split audience, but this time it is done just before the ad is served. The ad is then withheld from the control group, simulating the process of showing the ad to the user, known as ad serving, without paying for placebo ads. This is a tactic mostly used by advertising networks carrying out their own incrementality tests.\nPublic service announcements (PSAs) – These are in place to show ads to both the test and control group however, the control group is shown a general PSA while the test group is shown the variant. The behaviors of users in both groups are then compared to calculate incremental lift.\n\n\nCost per Incremental Conversion (CPiP)\n\nMetric used to determine whether the treatment’s effect (e.g incremental conversion) is enough to warrant implementing the treatment in production\n\nHigher is worse\n\nExample:\n\neCommerce brand spent $100k on their campaign.\nThey measured\n\n7,500 sales from their campaign’s treatment group (both during and for some fixed time after the campaign)\n5,000 sales from their campaign’s control group\n7500 - 5000 = incremental sales\n\nCPiP = $100,000 / (7500 - 5000) = $40\n\nIf the CPiP &lt; Customer Lifetime Value (CTV) (See Algorithms, Marketing &gt;&gt; Customer Lifetime Value) and this margin is acceptable at which to acquire customers, then the treatment (e.g. ad campaign) can move to production\n\nIncremental Return on Advertising Spend (iROAS)\n\\[\n\\mbox{iROAS} = \\frac{\\mbox{Treatment Group Revenue} - \\mbox{Control Group Revenue}}{\\mbox{Treatment Spend}}\n\\]\n\nLess than 100% you can redistribute budgets to better-performing campaigns and channels\nEqual to or higher than 100% you know you are not cannibalizing organic traffic and that your ads are effective",
    "crumbs": [
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-geoexp",
    "href": "qmd/marketing.html#sec-mark-geoexp",
    "title": "Marketing",
    "section": "Geo-experiments",
    "text": "Geo-experiments\n\nPackages\n\n{GeoLift} - Facebook package for end-to-end geo-experimental methodology based on Synthetic Control Methods used to measure the true incremental effect (Lift) of ad campaign.\n\nA quasi-experimental methodology where non-overlapping geographic regions (geos) are randomly assigned to a control or treatment group\n\nExample: ads are served only in the geos of the treatment group while users in geos of the control group won’t be exposed to the advert\n\nAll the major ad networks/tracking platforms allow for targeting ads to the relevant level of location (neighbourhood, city, state, etc.)\n\nConversions are measured at a geo-level.\n\nSteps\n\nDecide geo-level based on market geography\n\ncountry \\(\\rightarrow\\) states\nstate \\(\\rightarrow\\) zip codes, Designated Market Area (DMA)\nDon’t select geos that are too small as people may travel across geo-boundaries and the volumes of conversions may be too low\n\nPerform a preliminary analysis of the geos\n\nDetermine factors that differ between geos that may influence the experiment\nFind geos most similar to each other so the experiment isn’t biased even after randomization\n\nAssignment\n\nRandomly assign treatment and contol to geos\n\nAnalysis\n\nUse difference in differences or synthetic controls to measure incremental conversions\nExample DiD",
    "crumbs": [
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-chanatt",
    "href": "qmd/marketing.html#sec-mark-chanatt",
    "title": "Marketing",
    "section": "Channel Attribution",
    "text": "Channel Attribution\n\nMisc\n\nAlso see UTM parameters in Terms on how to use URLs to track different ad accounts.\n\nWhen you start tracking the UTM parameters and tying it to the account creation, you can now measure the conversion rate from ad click to purchase.\nThe main KPI is now end-to-end customer acquisition cost, rather than cost per click.\n\nModel should reflect these characteristics\n\nThe concave shape of diminishing returns\n\n\nAs the market becomes saturated by your ads, additional ad spend generates less return\nsales ~ ln(spend) and you’d have make sure you change the 0s to a really small number or add 1 , e.g y = ln(x+1).\n\nBy adding 1, zeroes will be zeros on the log-scale since log 1 = 0\n\n\nCarryover Effect\n\nThe lag between the time a consumer is touched by an ad and the time a consumer converts because of the ad.\nAdstock is a decaying function that describes the average carryover effect\n\n\nThe advertising effect of this example channel was about 40% in the initial week, then decrease to 20% in the following week, and so on\n\nNot sure how this “effect” is measured. Maybe it means a 40% increase in conversions during the initial week of the ad buy. Maybe this is measured experimentally. Maybe it’s something like a ratio of conversions after clicks on digital ads/paid search to total clicks on the ad.\n\nCalculate a “spend_with_adstock” variable\n\n\nDecay rate is specific to an ad channel and business type\n\nOnline ads have a more immedicate effect since users are usually ready to buy\nOffline ads have a longer delay since it takes time to respond\n\n\n\n\n\nTypically dealing with sparse data\n\nThere can be a big spike in spending due to a new launch but no consistent spending afterward.\nFor certain channels, the budget could be turned off for some time and back on due to market dynamics or business strategy changes.\nIn some cases, we can pool together smaller locations that are believed to behave similarly to each other and estimate them using a hierarchical Bayesian model (the same logic can be applied to similar channels)\n\nIssues\n\nSelf-selection/endogeneity\n\nExample: Paid Search\n\nA user already has the brand she wants to purchase in mind and searches the brand name online. Then she clicked the paid search ad and made a purchase. This would incur ad spend, however, the purchase would not be incremental sequence of steps (i.e customer journey) because she would have purchased anyway. So the ad didn’t induce the purchase since the user was already ready to purchase before encountering the ad.\n\nUpper funnel channels can get less credit than downstream channels\n\ne.g. A user saw an ad on TV first and wanted to make a purchase online. He then searched for the product on Google and bought it from the paid search ad. The model could attribute more credit to the search if not treated well.\n\nSolutions\n\nInformative priors from reliable experimentation on channels with high selection bias to guide the model and prevent the model from being biased towards endogenous channels.\nInstrumental variables to better control for the bias (although instrumental variables may be hard to find or construct)\nSelection Bias Correction approach developed by Google (Paper).\n\nGoogle team used DAGs of the search ad environment and derived a statistically principled method for bias correction\nAdjusted for search queries to close backdoor paths.\nAfter the adjustment, the coefficient for the search channel is much less than that from the naive model and is aligned with the result from the experimentations.\n\n\n\nMulti-Collinearity\n\ne.g. Two channels might be launched together to reinforce each other or to support a product launch\nSolutions: variable reduction techniques: PCA, ElasticNet, etc.\n\n\n\n\n\nUnified Approach\n\nUse experimentation results as priors for MMM and use MTA results to validate and calibrate MMM\n\nExperiment examples: Conversion Lift Tests, Geo-Experiments, A/B Tests, RCTS, etc.\nUse trustworthy priors obtained from experiments or external sources to inform a bayesian model\n\nA bayesian model with time-varying coefficients can handle violatility of marketing effects (particularly for new companies)\n\n\n\n\n\nMarketing Mix Modeling (MMM)\n\nSales (or ) ~ Channel_Spend (+ Trend + Seasonality(s) + Campaigns + Events + Holidays + Weather +… etc.)\n\nOther outcome variables\n\nKPI or google search volume or website visits or account sign-ups\n\nOther explanatory variables:\n\nmedia cost for each channel\nproduct pricing\neconomic indicators\nsynergistic effect between different media (i.e. interaction)\ncompetitor activities\n\n\nBetter for measuring performance of offline traditional Out-of-Home (OOH) media like TV, radio and print which is unlike online advertising where we have access to metrics like clicks, clickthrough rate, impressions, etc.\nRequires relatively large budgets and longer data history to have reliable reads. Sparse data and short data history could make the results biased and unstable\n\nTypically requires at least 2 years of weekly data (longer is better) and a good volume of media spend.\nFor small data situations, reduce the number of features by combining smaller channels and prioritizing bigger channels\n\nTends to underestimate upper funnel channels and overestimate lower funnel channels.\nLess likely to get more granular level insights, and will not show the nuances of the user journey\n\n\n\nMulti-Touch Attribution Modeling (MTA)\n\nAlso see\n\nMulti-touch attribution: The fundamental to optimizing customer acquisition (only skimmed, haven’t taken notes yet)\n\nUnlike MMM, it acknowledges that the customer journey is complex and customers may encounter more than one (or even all) of our marketing channel activities in their journey\nRequires metrics like clicks, clickthrough rates, impressions, etc.\n\nTherefore, only suited for digital media\nMTA data aims to model the entire user ad journey. However, in reality, the real-world data can be partial and only includes part of the touch points due to tracking difficulty.\n** MTA is also subjective to user data privacy initiatives, such as Apple no IDFA and Facebook no AMM. In the foreseeable future, Google will also join the force. Without user-level tracking data, MTA models cannot be built. **\n\nMost MTA methods utilize click data, not impressions, which tends to give more credit to more clicky channels, like search. This can also make the analysis biased.\nWeighting is assigned to these channels based on their touchpoints to determine each channel’s contribution / involvement towards the sale\n\nWeighting methods:\n\nLast Click Attribution and First Click Attribution\n\nassigns 100% weighting to the last/first channel that was clicked on\nNaive default model used on many analytics platforms\n\nGoogle switched from this model to “data driven attribution” (see below) in Google Analytics 4 (GA4)\n\n\nTime Decay - weighting is distributed based on the recency of each channel prior to conversion. The most weighting will be given to the last channel prior to conversion (PPC). The least weighting is assigned to the first channel.\nPosition-Based - the first and last channels each receive 40% of the weighting, with the remainder 20% distributed equally to the middle channels.\n\nSee https://support.google.com/analytics/answer/10596866?hl=en#zippy=%2Cin-this-article  for explanations of other rules based models in GA4\n\nData-Driven Attribution (DDA) - weighting is assigned objectively by an algorithm based on the probability of conversion, given the touchpoints. Methods like\n\nGoogle Search Ads 360 - Markov Chain, game-theory approaches using Shapley values\nGA4\n\nAnalyze the available path data to develop conversion rate models for each of your conversion events\nUse the conversion rate model predictions as input to an algorithm that attributes conversion credit to ad interactions\n\n\n\n\n\n\n\nMobile Channel Attribution\n\nAdvertising Identifiers - helps mobile marketers attribute ad spend.\n\nGoogle’s advertising identifier (GAID)\nExample:\n\nWhen a company like Lyft or Kabam runs user acquisition campaigns to gain new mobile customers, a mobile measurement partner like Adjust, Singular, Kochava, or AppsFlyer can help them connect a click on an ad with an eventual app install on a specific device. That helps Lyft know that an ad worked, and that whatever ad network they used for it succeeded.\nif the person who installed that app eventually signs up for an account and takes a ride share, Lyft knows where and how to attribute the results of that marketing effort, and connect it to the ad spend that initiated it. Even better, from Lyft’s perspective, it can use the IDFA to tell mobile ad networks essentially: I like users like this; go find me more.\n\nApple’s SKAdNetwork \n\nPrivacy-safe framework for mobile attribution\nAllows advertisers to know which ads resulted in desired actions without revealing which specific devices — or which specific people — took those desired actions.\nYou go to an ad network like Vungle or AdColony or Chartboost — or Facebook or Google, for that matter — and kick off an ad campaign. They show your ads to potential new mobile customers, and when one clicks on it and downloads your app from the App Store, Apple itself will handle sending a cryptographically signed notification — a postback — to the ad network. That postback will not include any user or device-specific information, so while it will validate the conversion for marketing purposes, it won’t reveal any personal information of your new app user.\nCan’t be used in Conversion Lift Tests\n\nPostbacks require a ad campaign ID for the conversion to be attributed to, so it’s not possible to measure the conversion volume from a lift test’s control group (since the control group isn’t exposed to the ad)",
    "crumbs": [
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-sem",
    "href": "qmd/marketing.html#sec-mark-sem",
    "title": "Marketing",
    "section": "Search Engine Marketing (SEM)",
    "text": "Search Engine Marketing (SEM)\n\nMarketing on search engines where you pay to place ads at the top so that the user sees your ads before the organic search results (e.g. Google, Bing, etc.)\nMost SEM ads work on an auction system, where you have to place a bid on each of the search terms (can be millions of terms) relevant to your business and if you win a position in the auction(there are usually 2–3 SEM ads per search, or more based on your region and the search engine you are using), then you would pay the cost equal to your bid or lower depending on which auction system the search engine follows.\nAuctions\n\nTypes\n\nFirst Price Auctions: A model wherein the buyer pays exactly the price they’ve bid on any given advertising impression. (greater transparency)\nSecond-Price Auctions: A model wherein the buyer pays $0.01 more than the second highest bid for an ad impression.\n\nExample: 3 bidders\n\nBids\n\nBidder A $2.20\nBidder B $2.80\nBidder C $2.50 first-price auction: B wins\nClearing Price will be the same as the bid- $2.80\nSecond Price auction: B wins\nClearing Price = $0.01 + Decond-Highest Bid ($2.50) = $2.51\nReduction = $2.80 (Winning Bid) - $2.51 (Clearing Price) = $0.29\n\n\n\nBid Shading\n\nTechnique buyers use in first-price auctions in an attempt to avoid overpaying\nTakes a maximum possible bid and tries to forecast the market value for a given impression, in order to determine the actual bid price to submit.\n\nOutcome: bid price; model features: site, ad size, exchange and competitive dynamics\nIf win rates decrease, the algorithm raises the price they pay\n\nSome publishers (google, bing, etc.) use intelligent price floors to counteract bid shading\n\nOptimize bidding for value vs traffic volume characteristics of key words\n\nSparsity Issues (i.e. Zero-Inflation)\n\nDue to the long tail nature of the search terms (some search terms are more popular than some other obscure ones)\nDue to the conversion ratio itself — not every click from these SEM ads converts to a purchase\nWhat should the outcome variable be? (binary: conversion/no_conversion or numeric: bid value)\n\nOther considerations\n\nHow does the effect of a competitor’s bid change based on the type of auctioning method?\nSeasonality?\n\nPre-corona vs post-corona?\n\nIs there an optimal level of investment?",
    "crumbs": [
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-custseg",
    "href": "qmd/marketing.html#sec-mark-custseg",
    "title": "Marketing",
    "section": "Customer Segmentation",
    "text": "Customer Segmentation\n\nMisc\n\nAlso see\n\nCustomer Journey\nAlgorithms, Product &gt;&gt; Customer Journey\n\nSeeks to answer:\n\nWho are the most valuable customers?\nWhere do they come from?\nWhat do they look like?\nWhat and how do they like to buy?\n\nBenefits\n\nThe company can achieve a higher return on ad spend with a smaller marketing budget, yielding a higher profit margin and more room for market expansion.\nIncreases marketing efficiency by helping to indicate which campaigns are more likely to succeed with certain groups of customers\n\n\n\n\nProcess\n\nForm marketing hypotheses based on cluster characteristics and test these hypotheses by varying campaigns based on the customer’s cluster membership.\n\ne.g. Recommend a product they’re likely to purchase, a multi-buy discount, or on-boarding them on a loyalty scheme (e.g. rewards program)\n\nOnce you know who your customers are and what their value is to your business, you can:\n\nPersonalize your products and services to better suit your customers’ needs\nCreate Communication Strategies tailored to each segment\nFocus Customer Acquisition to more profitable customers with messages and offers more likely to resonate with them\nApply Price Optimization to match customer individual price sensitivity\nIncrease Customer Retention by offering discounts to customers that haven’t purchased in a long time\nEnhance Customer Engagement by informing them about new products that are more relevant to them\nImprove your chance to Cross-sell and Up-sell other products and services by reaching out for the right segment when they’re more likely to respond\nTest which type of incentive a certain segment is more likely to respond to (e.g. pricing discounts, loyalty programs, product recommendation, etc.)\n\n\n\n\nRFM Analysis\n\n\nRecency, Frequency, and Monetary Value Analysis\nThe most profitable segments will have low recency, high frequency, and high monetary value.\n\n\nData\n\nJust need a transactional database with client’s orders over time (at least 2.5 to 3 yrs of data to capture enough behavior variation).\n\n\ntime frame could be monthly or yearly or any other time frame required by the business\n\nExplicitly creates sub-groups based on how much each customer is contributing.\n\n\nRecency – How recently did the customer purchase?\n\nInvoice date can be used to calculate the recency\nNumber of days since the last purchase\n\nFrequency – How often do they purchase?\n\nInvoice numbers can be used to calculate the frequency\nNumber of invoices per month\n\nMonetary Value – How much do they spend?\n\nTotal price can be used to calculate the monetary value.\nTotal monetary value of all purchases in a month\n\n\n\n\n\nModeling\n\nAlso see\n\nAlgorithms, Marketing &gt;&gt; Customer Lifetime Value &gt;&gt; BG/NBD Gamma-Gamma model\nVideo: Bizsci\n\nlab-49-feature-engineering-customer-segmentation\nlab-58-customer-lifetime-value-rfm-calc\npython customer lifetime value, rfm + xgboost (youtube)\n\n\nGroup into Quantiles\n\nProcess\n\nFor each RFM variable, rank customers based on values\nFor each customer, add up the ranks to get a RFM score\nBin each customer into a quantile based on their RFM score\n\nIssues\n\nCorrelation between RFM variables\n\ne.g. Someone who bought a lot a long time ago could be in the same bin as someone who bought a little recently.\n\nShould look at the distribution of RFM scores. Grouping into a particular quantiling schema may not be informative\n\n\nClustering (k-means is popular)\n\nCluster Customer IDs based on Recency, Frequency, and Monetary Value variables\n\nAdd additional variables if relevant to helping you choose a market strategy\nIf you end up with too many variables for the amount of data you have, then perform PCA before clustering\n\n\nInterpret clusters and create strategy for each cluster\n\nExample\n\n\nCharts\n\nBoth charts have the same info, right shows the patterns better but you can’t read the values\nRFM variables were standardized between 0 and 100 before clustering\n\nCluster 0\n\nInterpretation — Customers who have not made purchases recently\nStrategy — Make offers to bring them back to purchasing\n\nCluster 1\n\nInterpretation — Customers who have a high monitory value\nStrategy — Create a loyalty program so that they can continue spending more\n\nCluster 2\n\nInterpretation — Customers who have not made purchases recently\nStrategy — Make offers to bring them back to purchasing\n\nCluster 3\n\nInterpretation — Customers who are likely to churn\nStrategy — Retain them with exciting offers\n\nCluster 4\n\nInterpretation — Regular customers\nStrategy — Create a loyalty program to keep them purchasing on a regular basis\n\n\nExample\n\n\n\nFrom A Data Science Project with ChatGPT Code Interpreter\n\nExample: Categorizing by Rank\n\nFrom How to Create an RFM Model in BigQuery (See article for code)\nAdvocates\n\nWho They Are: Recent, frequent shoppers with high spending.\nStrategy: Ideal for exclusive rewards and further engagement opportunities.\nRanks\nrecency_rank between 4 and 5 \nand frequency_rank between 4 and 5 \nand monetary_rank between 4 and 5 \n\nLoyalists\n\nWho They Are: Customers showing strong recent engagement and significant spending.\nStrategy: Deserve recognition and retention efforts, perfect for loyalty programs.\nRanks\nrecency_rank between 3 and 5 \nand frequency_rank between 3 and 5 \nand monetary_rank between 3 and 5 \n\nPromising\n\nWho They Are: Recent customers showing potential but with lower frequency and spend.\nStrategy: Need encouragement to boost their shopping frequency and spending.\nRanks\nrecency_rank between 3 and 5 \nand frequency_rank between 2 and 5 \nand monetary_rank between 1 and 5 \n\nNeed Attention\n\nWho They Are: Customers who have made a recent purchase but have only done so once.\nStrategy: An opportunity to introduce these typically new customers to the brand and products, encouraging deeper engagement.\nRanks\nrecency_rank between 3 and 5 \nand frequency_rank = 1\nand monetary_rank between 1 and 5\n\nLosing High Value\n\nWho They Are: Once frequent and high-spending customers now showing decreased recency.\nStrategy: Require personalized re-engagement strategies and incentives to rekindle their interest and loyalty.\nRanks\nrecency_rank between 2 and 3 \nand frequency_rank between 1 and 5 \nand monetary_rank between 3 and 5 \n\nLosing Low Value\n\nWho They Are: Customers similar to the high-value segment but with overall lower spending.\nStrategy: Also in need of re-engagement efforts, though strategies may differ given their lower value.\nRanks\nrecency_rank between 2 and 3 \nand frequency_rank between 1 and 5\nand monetary_rank between 1 and 3\n\nLost\n\nWho They Are: Customers who have not made a purchase in a long time, making them unlikely to return.\nStrategy: Focus efforts on more engaged segments, as re-engaging lost customers may not be resource-efficient.\nRank\nrecency_rank = 1 \n\n\n\n\n\n\n\nBehavioral Segmentation\n\nTries to understand how customers interact with your product or website\nMisc\n\nAlso see\n\nCustomer Journey\nProduct Development &gt;&gt; Behavioral Data\n\n\nData\n\nDemographics: location, age, gender, and income\nTouchpoints (see Terms): view-through, click-through, the time between each action, etc\n\nIf a user sees an ad on Instagram and quickly swipes over it, that probably indicates they are not very interested in it.\nOften impacted by advertisers’ programmatic push notifications\n\nCompany Website Navigation: visiting behaviors provide enlightening ideas about what a visitor is truly looking for and how they like to shop (See Customer Journey)\nPurchasing: what a user buys, their purchasing frequency and timing, purchasing price and discounts, etc.\nSocial media network: Is there an influencer within a customer’s network? (see bkmks &gt;&gt; Network Analysis)\nExample: Telecommunications\n\n\nCluster the data and interpret the clusters\n\nExample using the telecommunications data above\n\nThe green cluster (aka segment) is Digitally engaged customers.\n\nA market strategy could be to create a digital loyalty card and reward them based on the use of digital services. This will in turn also increase revenue for the company.\n\nThe blue cluster is moderately engaged with low tenure.\n\nA market strategy could be to offer discounts and convert them into long-term contracts.\n\nThe red cluster is basic customers with only phone service.\n\nA market strategy could be to educate them about the advantages of digital services and then upsell digital products.",
    "crumbs": [
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-custjourn",
    "href": "qmd/marketing.html#sec-mark-custjourn",
    "title": "Marketing",
    "section": "Customer Journey",
    "text": "Customer Journey\n\n\n\nTop Figure: Typical SaaS User Journey\nBottom Figure: General User Journey\n\nArea of the rectangle represents the amount of customers that haven’t dropped off and remain the funnel\nThe red arrow represents customers that drop off and don’t convert\nThe start of the arrow show the section of the rectangle that represents the amount customer that drop off from on point to the next.\n\nMisc\n\nAlso see Algorithms, Product &gt;&gt; Customer Journey\n\nPre Sale\n\nWhen potential customers are in the “consideration” phase and researching a product, AI will target ads at them and can help guide their search\n\nDetermine which customers are most likely to be persuadable and, on the basis of their browsing histories, choose products to show them\n\nUse extremely detailed data on individuals, including real-time geolocation data, to create highly personalized product or service offers\n\nDuring Sale\n\nUpselling and cross-selling and can reduce the likelihood that customers will abandon their digital shopping carts\n\nAfter a customer fills a cart, AI bots can provide a motivating testimonial to help close the sale—such as “Great purchase! James from Vermont bought the same mattress.”\n\n\nPost sale\n\nAI-enabled service agents (chatbots) triage customers’ requests—and are able to deal with fluctuating volumes of service requests better than human agents are\nCan handle simple queries about, say, delivery time or scheduling an appointment and can escalate more-complex issues to a human agent",
    "crumbs": [
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-wkflw",
    "href": "qmd/marketing.html#sec-mark-wkflw",
    "title": "Marketing",
    "section": "Workflow",
    "text": "Workflow\n\nFind the Ideal Customer Profile (ICP) and Target Them\n\n\nNotes from article\nCustomer Long Term Value\n\\[\n\\mbox{Customer's long-term value} = \\mbox{Revenue from the customer's repeated purchase} - \\mbox{customer's acquisition cost}\n\\]\nSegment in Negative, Low-Value, Medium-Value, and High Value cohorts\n\nSee Customer Segmentation &gt;&gt; RFM Analysis\nAlternatively, based on its sales and marketing attribution systems, the brand can calculate one-year revenue and acquisition costs for each customer (or visitor that doesn’t convert). With that, the brand quickly gets each customer’s value and decides who is worth its marketing budget.\nThe split between medium-value segment and high-value segment typically follows the 80/20 rule\n\nThe High-Value segment can be further segmented into three smaller cohorts using pca or clustering algorithms.\n\nSee Customer Segmentation &gt;&gt; Behaviorial Segmentation\nThe customer data used to cluster can include: interactions with marketing campaigns, browsing history, purchase history, records of using coupons, etc.\nDomain knowledge is usually required to meaningfully label these cohorts and develop marketing plans based on the cluster attributes\nExamples\n\nCohort 1 likes to buy the latest model, cohort 2 prefers to buy with discounts, and cohort 3 often makes purchases as gifts\nCohort 1 reacts positively to campaigns about cool geeky features. On the other hand, cohort 2 engages more with campaigns spotlighting the practical benefits of using a new gadget.\nCohort 1 trusts YouTubers who talk about the latest and greatest tech products and make purchases after watching unboxing videos. Cohort 2 likes to shop at Costco and buy gadgets at a discount\n\nPartnering with tech influencers helps the company attract cohort 1 while working with wholesale stores speeds up selling to cohort 2.\n\n\n\nUse experimentation on High-Value cohorts to test which acquisition/marketing campaigns have high ROAS.\n\n\nFor cohort 3, campaign A seems to have more success than campaign B\n\nFuture campaigns can now target high value cohorts with greater conversion and less expenditure.\n\n\n\nPotential Pitfalls\n\nMake sure the metric fits the business question\n\nExample: Churn\n\nAlso see Algorithms, Marketing &gt;&gt; Churn\nDon’t model who was most likely to leave, they should have asked who could best be persuaded to stay — in other words, which customers considering jumping ship would be most likely to respond to a promotion.\n\ni.e. Swing Customers - like politicians looking for swing voters because they are persuadable.\n\nIf you model the wrong objective, you can squander money on swaths of customers who were going to defect anyway and underinvest in customers they should have doubled down on.\n\nThink this has to be 2 stages\n\nFilter data before promotions over some window \\(\\rightarrow\\) model traditional churn \\(\\rightarrow\\) filter data after promotion \\(\\rightarrow\\) label which likely churns left and which ones didn’t \\(\\rightarrow\\) model churn with new churn labels using data after promotion because you want probability of churn given promotion\n\nSo you’d have 2 models: 1 to identify churners and 1 to identify swing customers from churners\nDoes a lift model help here?\n\n\nExample: Increasing user spend for a video game\n\nFinding features that increase time spent playing doesn’t efficiently raise revenue or maybe not at all if most of your players play for free.\n\ne.g. Increasing my time spent playing FOE doesn’t increase FOE’s revenue since I don’t pay for shit on that game.\n\nGuessing there’s a decent correlation between weekly total_time_spent_playing and overall revenue and they thought they could increase revenue by increasing total time_spent_playing. They should have been looking at correlations between features and spend_per_customer.\n\nMaybe just filter top spenders and see which features they prefer (i.e. correlated to)\n\n\n\nMake sure when you increase the accuracy of a model, it doesn’t hurt the bottom line due to a bad tradeoff\n\nConsider the consumer goods company whose data scientists proudly announced that they’d increased the accuracy of a new sales-volume forecasting system, reducing the error rate from 25% to 17%. Unfortunately, in improving the system’s overall accuracy, they increased its precision with low-margin products while reducing its accuracy with high-margin products. Because the cost of underestimating demand for the high-margin offerings substantially outweighed the value of correctly forecasting demand for the low-margin ones, profits fell when the company implemented the new, “more accurate” system.\n\nDetermine the optimal granularity of predictions\n\nConsider a marketing team deciding how to allocate its ad dollars on keyword searches on Google and Amazon. The data science team’s current AI can predict the lifetime value of customers acquired through those channels. However, the marketers might get a higher return on ad dollars by using more-granular predictions about customer lifetime value per keyword per channel.\n\n\n\n\nRefine the Business Question\n\nWhen defining the problem, managers should get down to what we call the atomic level — the most granular level at which it’s possible to make a decision or undertake an intervention.\nA good business question captures the full impact of the decision on the Profit and Loss (P&L), recognizes any trade-offs, and spells out what a meaningful improvement might look like.\nGradaute from an extremely vague question to a more finely grained question\n\n“How do we reduce churn?”\n“How can we best allocate our budget for retention promotions to reduce churn?”\n\nHas the retention budget been set, or is that something we need to decide?\nWhat do we mean by “allocate”?\nAre we allocating across different retention campaigns?\n\n“Given a budget of $x million, which customers should we target with a retention campaign?”\n\nDefine Missed Opportunity metrics\n\nIdentify sources of waste and missed opportunities\n\nWaste example: targeting a likely-to-churn customer with a promotion that is not persuadable\nMissed opportunity example: not targeting a likely-to-churn customer with a promotion that is persuadable\n\nCompare the distribution of success versus failure to quantify waste and missed opportunities.\nMeasure\n\nFor difficult to quantify situations use aggregate data\n\nExample of an approach for churn model example using aggregated data\n\ncustomers who received promotion: what’s the cost of the promotion incentive relative to the incremental lifetime value\n\ndifference or ratio?\n\ncustomers who didn’t receive promotion: what’s the lost profit associated with the nonrenewal of their contracts\n\n\n\n\nDetermine the cause of waste and missed opportunities\n\nIn an ideal world, what knowledge would you have that would fully eliminate waste and missed opportunities? Is your current prediction a good proxy for that?\n\nIn churn example, rather than the basic churn/no churn model, focusing on persuadability would have led to great improvements.\n\nDoes the output of your AI fully align with the business objective?\n\nChurn example: A persuadable user with low expected profitability should have a lower priority than a persuadable user with high expected profitability.\n\nAdditional factor that further optimizes the solution to the refined business question\n\n\nHow much are we deviating from the business results we want, given that the AI’s output isn’t completely accurate?\n\nQuantifying the errors of your model\nChurn example: the cost of sending a retention promotion to a nonpersuadable customer (waste) is lower than the cost of losing a high-value customer who could have been persuaded by the offer (missed opportunity). Therefore, the company will be more profitable if its AI system focuses on not missing persuadable customers, even if that increases the risk of falsely identifying some customers as being receptive to the retention offer.",
    "crumbs": [
      "Marketing"
    ]
  },
  {
    "objectID": "qmd/meteorology.html",
    "href": "qmd/meteorology.html",
    "title": "Meteorology",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Meteorology"
    ]
  },
  {
    "objectID": "qmd/meteorology.html#sec-meteor-misc",
    "href": "qmd/meteorology.html#sec-meteor-misc",
    "title": "Meteorology",
    "section": "",
    "text": "Method Types for Grid-type Meteorological Models\n\nDynamical\n\nUses a regional climate model has the advantage of producing physically and dynamically balanced data.\nLimitations: A regional climate model requires considerable integration time and significant storage to produce high-resolution data and that the model outputs may include systematic errors\n\nStatistical - Efficient methods that produce meteorological and climate data at a high resolution without the limitations of Dynamical models\n\nExample: Extreme hot weather events in Western Europe-Atlantic region\n\nNotes from video, around ~1:07:03\nPapers\n\nGradient boosting with extreme-value theory for wildfire prediction\nSpatiotemporal wildfire modeling through point processes with moderate and extreme marks\n\nSee “Supplemental Content” tab for link to zip file with code — uses {INLA} for modeling.\n\n\nData: “ERA5 reanalysis data”\n\nMonthly\nTraining: 1979 - 2020\nValidation: 2020 - 2022 (simple train-test split?)\n\nGoals:\n\nPredict probability of an event (i.e. binary classification), intensity of event (ie regression), spatial dependence of events (i.e. spatial correlation)\nIdentify weather patterns driving extremes (i.e. predictors)\n\nGlobal thermodynamic (i.e.. climate change)\nLocal land surface: soil moisture or snow cover determin surface energy budget (i.e. wet soil buffers heat)\nRegional dynamic conditions: diabatic (clear skies), adiabatic warming (subsidence) or advection of warm air from anti-cyclonic circulation (atmospheric blocking)\n\n\nModel\n\nResponse:\n\nExtreme 2m daily temp anomalies over land\n\nPredictors\n\nAtmospheric blocking measured by variable, “Z500” (5-day avg)\nSoil moisture, “SM” (15-day avg)\n\nResolution\n\nThere’s a predictor for each 5.6 x 5.6 degree (lat-lon) grid points\n\nShows the Western Europe-Atlantic region (disregard the horizontal black line)\n\nThere’s a response variable for each 1.3 x 0.3 degree (lat-lon) grid points\n\nStudy region, larger box, is a smaller subregion within the Western Europe-Atlantic region, and the Risk area is the smaller box\nThe response variable measurements are for the grid points in the study region\n\n\nModel\n\nModel has 3 components\n\nExceedence Probability is fit first. Then, its probabilities are used in the Marginal Extermal Intensity model. Then, both results are used to model the Spatial Dependence\nExceedence Probability: indicator response for whether temp has exceeded the threshold.\n\nFits a boosted tree with mean log loss\n\nMarginal Extremal Intensity: uses a univariate loss function discussed at the beginning of the talk\n\nksi (shape?) is a hyperparameter along number of trees, m.\n\nSpatial Dependence\nPaper doesn’t come out until June or July, so will have to wait to see if there’s a package and to get further details\n\nFeature importances will be the Z500 and SM for each of the geographical location variables. These feature importances can used to figure out weather patterns in those areas that were important to occurance of the high temperature event in the risk area.",
    "crumbs": [
      "Meteorology"
    ]
  },
  {
    "objectID": "qmd/nonprofits.html",
    "href": "qmd/nonprofits.html",
    "title": "Nonprofits",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Nonprofits"
    ]
  },
  {
    "objectID": "qmd/nonprofits.html#sec-nonprof-misc",
    "href": "qmd/nonprofits.html#sec-nonprof-misc",
    "title": "Nonprofits",
    "section": "",
    "text": "Notes from:\n\nData Strategy for Non-Profits: Why?",
    "crumbs": [
      "Nonprofits"
    ]
  },
  {
    "objectID": "qmd/nonprofits.html#sec-nonprof-devdatstrat",
    "href": "qmd/nonprofits.html#sec-nonprof-devdatstrat",
    "title": "Nonprofits",
    "section": "Developing a data strategy",
    "text": "Developing a data strategy\n\nWhy They Need a Data Strategy\n\nIdentify opportunities, inform decision makers on how to allocate scarce resources, and to measure and communicate impact\nProviding greater transparency and accountability to funders.\n\nThe availability of data has driven donors and grantmakers to be more conscious on the measurement of the impact that their dollars have. Non-profits which can better demonstrate their impact receive more financial support.\n\nGrowing impact of programs and services.\n\nData is utilized internally to improve services and expand impact which results in being able to generate greater impact.\n\n\n\n\nAssess Organization\n\nMission and Theory of Change\n\nA concrete outline which states\n\nThe impact that will be generated\nThe conditions needed to generate the impact\nThe programs in place to create those conditions.\n\nEach piece of the Theory of Change can then be stated in terms of a quantifiable measure of success which will serve as the starting point for developing a data strategy.\nExample\n\n\nStakeholders\n\nAnswer these questions:\n\nWho are your stakeholders?\n\nIdentify subgroups and individuals who fall into these groups\n\nDonors and Volunteers.\nManagement and Employees.\nBeneficiaries.\n\n\nWhat questions do stakeholders have that can be answered through data?\n\nDonors and Volunteers\n\nExample: A data-driven Impact Report which provides a holistic view of how the nonprofit utilizes their resources to achieve its mission.\n\nContains anectdotal stories with data that demonstrate how effectively their resources are being used\n\n\nManagement and Employees\n\nExample: more granular views on how individual programs and initiatives are performing on metrics related to their Theory of Change\n\nBeneficiaries\n\nExample: data around how projects in different sectors are performing\n\n\nHow will the data affect stakeholder decision making?\n\nDonors and Volunteers\n\nCan influence decisions around donating time and money\n\nManagement and Employees\n\nProvides visibility into how resources are allocated internally and empowers internal decision makers to evaluate how to get the most impact out of the limited resources they have\n\nBeneficiaries\n\nCan be used to garner buy in and allow the nonprofit access to communities that they would otherwise not have\n\n\n\n\nData Gap Analysis\n\nIdentify gaps between current data capabilities and those needed to answer all stakeholder questions\nContents\n\nOutline all data needs in the form of questions derived from your Theory of Change and stakeholder analysis.\nDeep dive into the required data to answer the questions and an estimate of how much that data would cost to obtain. Don’t forget that the same data could answer multiple questions.\nIdentify data that has already been collected and any existing efforts to collect additional data.\nConnect existing data and data efforts to questions and determine gaps between questions and data.\nPropose strategies to bridge data gaps and sustain data assets. Evaluate both the benefits of answering the question and costs of acquiring the data.\nPrioritize data gaps to close.\nCommunicate findings to relevant stakeholders.\n\n\n\n\n\nIdentify Key Questions\n\nShould be designed to be measurable, clear, and actionable\n\nAlso see nonprofit example in Projects, Planning &gt;&gt; General Steps to Starting a Project\n\nMake sure that you are addressing the social outcome and not program output of your organization.\n\nJust answering the question of how many people go through your program, how many volunteers you recruit / retain, are not sufficient because they focus only on outputs — but not social impact.\nYou would still need to assess whether those outputs are truly feeding into the social outcomes you want to accomplish through your organization.\n\nThe answers to key data questions should be resolved by a “North Star metric” or the single, defining outcome metric that best captures the core value that your nonprofit delivers.\n\nChasing non-critical metrics can strain resource constraints of the budget and time constraints of staff.\n\nExamples:\n\nDoctors Without Borders: Are we delivering adequate medical aid to people affected by crises (e.g., conflicts, epidemics, disasters) or exclusion from healthcare?\nAmerican Red Cross: How many lives have been saved from the blood donated by our donors?\nFeeding America: How many people have we been able to provide food for this year?\nBUILD: Are we improving the academic and professional outcomes of our students?\n\n\n\n\nOutputs vs Outcomes\n\nLogic model\n\nIt focuses on how inputs and activities translate to outputs and eventually\n\nExample",
    "crumbs": [
      "Nonprofits"
    ]
  },
  {
    "objectID": "qmd/politics.html",
    "href": "qmd/politics.html",
    "title": "Politics",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Politics"
    ]
  },
  {
    "objectID": "qmd/politics.html#sec-politics-misc",
    "href": "qmd/politics.html#sec-politics-misc",
    "title": "Politics",
    "section": "",
    "text": "Notes from Ethan Epping talk\n\nMost common modeling are for “support” (who they vote for) and “turn-out” (whether they vote at all) responses\nMessages around civic and community engagement is better at encouraging low propensity voters than pushing your candidate’s policy agenda\nTurnout Model\n\nFeatures: previous vote history of an individual; feature that influence turnout in general\n\nNCOA data (National Change of Address)\n\nVoter file info can be outdated\nWhen people move, nobody calls to cancel their voter registration\n\nGeocoding\nCommercially Available\n\nMobile phone data\nExperion\n\nGovernment\n\nGun and Boat Registration\nCollege student lists at public universities\n\nData collected through contact by the campaign\n\n\nSupport Model\n\nFeatures: survey data + other data\n\nDNC Stack\n\nBigQuery/SQL for a db and analytics,\nPython for engineering;\nDatascience does some R but more python\n\nVoter File\n\nList of registered voters in every state\nPublicly vailable\n\nStates have various restrictions\n\nOH and NC publish online and is free\nSome cost thousands and only available to campaigns\nNot available for commercial use though\n\n\nIncludes everything that’s on voter registration\n\nAddresses. phone numbers, etc.\nVote history (participation)\n\nIf you don’t vote in 2 straight federal elections you are usually eligible to be removed from the registry\n\n“Arbor” (sp?) tool\n\nFor registration drives\nAt census block or precinct level, take census estimate of eligible voters and subtract the number of registered voters\nHelps to figure out where to prioritize efforts (i.e. where the biggest pool of unregistered, eligible voters are)\n\n“Election Base” tool\n\nDe-aggregate results down to the census block level\n\nResolution is usually at the census block level because precinct boundaries often change\nEarly voting data\n\nWant to find voters that have requested a ballot but have yet to mail it in\n\nBest time to talk to them\n\nRejection data: talk to voters who incorrectly filled out a ballot\n\nVoter needs to talk to county clerk\nOr show up on voting day\n\n\nJobs\n\nhttps://www.progressivedatajobs.org/\n\nGuide to application process: https://guide.progressivedatajobs.org\n\nhttps://www.digidems.com/\n\nTakes tech people and gives some political training then deploys to campaigns",
    "crumbs": [
      "Politics"
    ]
  },
  {
    "objectID": "qmd/product-development.html",
    "href": "qmd/product-development.html",
    "title": "Product Development",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Product Development"
    ]
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-misc",
    "href": "qmd/product-development.html#sec-proddev-misc",
    "title": "Product Development",
    "section": "",
    "text": "Also see Decision Intelligence &gt;&gt; Conjoint Analysis for surveying consumers about product attributes to calculate attribute utilities and probability of purchase\nFlaws of Purchase-Funnel Based Attribution Metrics\n\nNotes from The Causal Analysis of Cannibalization in Online Products\nUsers usually take more complicated journeys than a heuristically-defined purchase-funnel can capture.\n\nExamples\n\nIf the recommendations make users stay longer on Etsy, and users click listings on other pages and modules to make purchases, then the recommendation-attributed metrics fail to capture the contribution of the recommendations to these conversions.  The purchase-funnel is based on “click”, and there is no way to incorporate “dwell time” to the purchase-funnel.\nSuppose the true user journey is “click A in recommendation → search A → click A in search results → click A in many other places → purchase A”.  Shall the conversion be attributed to recommendation or search? Shall all the visited pages and modules share the credit of this conversion? Any answer would be too heuristic to be convincing.\n\n\nViolation of causal experiment assumptions (ignorability assumption)\n\nSegments of users who follow the pattern of purchase-funnel may not be comparable between treatment and control groups, because the segmentation criterion (i.e., user journey) happens after random assignment and thus the segments of users are not randomized between the two groups.\n\nSolution involves causal mediation analysis (also see Causal Inference &gt;&gt; Mediation Analysis)\n\nDistribution of effect sizes for webpage variations using Optimizely (platform for A/B testing) data\n\n\nMedian (and average) webpage variations have roughly zero effect on webpage Engagement\nThread summarizing paper\n\n70% of effects will not show any impact on Engagement compared to a baseline\nalpha=0.05 yield an FDR of 18%-25% (i.e. much higher than alpha)\nRecommends only testing sizeable changes as these are the type of changes that yield the largest effects and fewer false discoveries (i.e. the tails of the effect size distribution)\n\nSee paper for other tests and recommendations\n\n\n\nLoyalty measure by app category\n\nTrust Thermocline - Point at which the number of customers that use your product suddenly drops off a cliff (3 nested Threads)\n\nBreaching this point typically occurs when a company continually changes the service, raises prices year after year but doesn’t notice a major change in customer subscriptions/engagement. Then customers start abruptly dropping the product.\nMost of the time the company cannot regain the customers after this point and even if they do they will never reach heights they were at previously.\nIf you’re a relatively large company you should have customer retention department/team/process in place\nSignals to watch out for\n\nWatch for grumbling and LISTEN to it.\nDon’t assume that because people have swallowed a price or service change that they’ll swallow another one.\nTreat user trust as a finite asset. Because it is.\n\nGround-level customer-facing people will be the ones receiving these signals\nMaintaining your customer base\n\nGet your customer retention people in a room with a white board and list all the issues they CONSTANTLY hear from customers but have stopped bothering to report up the chain.\nHowever painful it is to your bottom line. However politically tough it is. However complex the problem. These are the things that need to be fixed.",
    "crumbs": [
      "Product Development"
    ]
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-terms",
    "href": "qmd/product-development.html#sec-proddev-terms",
    "title": "Product Development",
    "section": "Terms",
    "text": "Terms\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. Low bounce rate can indicate the landing page needs improvement\nProduct Listing Page (PLP) -  webpage that lists all your products or a category of your products\nProduct Details Page (PDP) - webpage for a specific product that shows in-depth information",
    "crumbs": [
      "Product Development"
    ]
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-behdat",
    "href": "qmd/product-development.html#sec-proddev-behdat",
    "title": "Product Development",
    "section": "Behavioral Data",
    "text": "Behavioral Data\n\nBehavioral data serves two main purposes for teams — understanding how the product is being used or not used (user behavior) and building personalized customer experiences across various touchpoints to influence user behavior.\n\nLaunching new features without instrumenting them (e.g. tracking) beforehand takes away the opportunity to analyze how those features are used (if at all) and to trigger in-app experiences or messages when relevant events take place (or don’t).\n\n\n\nMisc\n\nNotes from How to Collect Behavioral Data\nAlso see Marketing &gt;&gt; Customer Segmentation &gt;&gt; Behaviorial Segmentation\n\n\n\nSources\n\nPrimary: web app, mobile apps, a smart device, or a combination — powered by proprietary code\nSecondary: all external or third-party tools that your customers interact with directly or indirectly\n\ne.g. Tools used for authentication, payments, in-app experiences, support, feedback, engagement, and advertising\n\nAuth0 for authentication, Stripe for payments, and AppCues for in-app experiences\nOpening a support ticket via Zendesk, leaving feedback via Typeform, opening an email sent via Intercom, or engaging with an ad on Facebook\n\nTo collect data from secondary sources, you can either use source integrations offered by data collection tools or write your own code\n\n\n\n\nExtract Data\n\nUsing vendors are the best option. Maintenance and troubleshooting are not trivial for homemade solutions even for experienced engineers.\nCDI or ELT?\n\nCDI is best-in-class to collect behavioral data from primary or first-party data sources — web and mobile apps, and IoT devices\nELT is best-in-class to collect all types of data including behavioral data from secondary data sources — third-party tools that power various customer experiences.\n\nCustomer Data Infrastructure (CDI)\n\nCharacteristics\n\nPurpose-built to collect behavioral data from primary or first-party data sources but some solutions also support a handful of secondary data sources (third-party tools).\nData is typically synced to a cloud data warehouse like Snowflake, BigQuery, or Redshift, but most CDI solutions have the ability to sync data to third-party tools as well.\nAll CDI vendors offer a variety of data collection SDKs and APIs\nSome CDI solutions store a copy of the data, some make it optional, and some don’t.\n\nCDI Vendors\n\nSegment Connections - supports data warehouses and a host of third-party tools as destinations, as well as store a copy of your data that can be accessed later if needed.\nmParticle - offers CDI capabilities along with identity resolution in its Standard edition whereas audience building is available on the Premium plan\n\nAlso supports data warehouses and a host of third-party tools as destinations, as well as store a copy of your data that can be accessed later if needed.\n\nRudderStack Event Stream and Jitsu - Open Source; support warehouses and third-party tools but RudderStack offers a more extensive catalog of destinations\nSnowplow - open-source and unlike the others, Snowplow doesn’t support third-party tools as it is focused on warehouses and a few open source projects as destinations.\nFreshpaint that offers codeless or implicit tracking\nMetaRouter which is a server-side CDI that only runs in a private cloud instance\n\n\nELT tools\n\nThese provide more comprehensive source integrations than CDIs can\nAirbyte\n\nOpen-source\nOffers source connectors with 150+ tools like Zendesk, Intercom, Stripe, Typeform, and Facebook Ads, many of which generate event data\nOffers a Connector Development Kit (CDK) that you can use to build integrations that are maintained by Airbyte’s community members\n\nOther (all open source): Fivetran, Stitch, and Meltano\n\n\n\n\nProcess Data\n\nNotes from The Modern Customer Data Stack\nIdentity Resolution: Identifying the same users in different data sources\n\nIdentify match keys: Determine which fields or columns you’ll be using to determine which individuals are the same individual within and across sources.\n\nExample: email address and last name.\n\nAggregate Customer Records: Create a source lookup table that has all the customer records from your source tables.\nMatch & Assign a Customer ID: Take records that have the same (or in some cases, similar) match keys and generate a unique customer identifier for that matching group of customer records. Every customer id that is generated can be used to link the customer sources together going forward.\nRoll in more sources: As you get more sources you can start rolling them into the same process by setting the correct rules and precedence for the source.\n\nMaster Data Models: Creating a final/clean view of your customers and associated facts and dimensions.\n\nStart with a “Customer → Transaction → Event” framework (more details)\n\nCustomers: Create a table of your customers with the ability to quickly add new fields\nTransactions: Join key from customers table to their transaction history\nEvents: Any events you track for each customer\nOther business types may have other tables\n\ndouble-sided marketplace - tables for both sellers and buyers as different entities.\nB2B business - separate accounts and contacts entities\n\n\n\n\n\n\nAnalysis Tools\n\nGeneral characteristics\n\nOffer SDKs and APIs to collect data from your primary (first-party) data sources\n\nUsing purpose-built data collection tools (CDI and ELT) is more efficient and prevents vendor lock-in\n\nStore a copy of your data and allow you to export the data (usually for an additional fee)\n\nAmplitude, Mixpanel\n\nCan be integrated with Airbyte\n\nIndicative, Heap\nPostHog (open-source)\n\nCan be integrated with Airbyte",
    "crumbs": [
      "Product Development"
    ]
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-convfun",
    "href": "qmd/product-development.html#sec-proddev-convfun",
    "title": "Product Development",
    "section": "Conversion Funnels",
    "text": "Conversion Funnels\n\n\nFigure: Typical eCommerce Funnel\n\nFunnel moves down and users drop off until only 3% of users (“% users”) reach Transaction (aka conversion)\n“CR” is the conversion rate at each point in the funnel\n\nProvide you with quantitative data about the number of customers that churn at each stage of the funnel. While this information is already valuable, paired up with qualitative data, it gives you all the insights you need to retain more customers.\nMisc\n\nAlso see\n\nMarketing &gt;&gt; Customer Journey\nAlgorithms, Product &gt;&gt; Customer Journey and Conversion Funnel\n\nTypical e-commerce user conversion rate can be benchmarked at around 2.5–3% in a regular business as usual time.\n\nBenefits\n\nProduct Managers:\n\nUseful for new feature launches. By grouping your sessions by user or device properties, you can compare the conversions between different user cohorts.\nIs the new feature sticking or not? Are your users struggling with it? Are they simply not interested? Looking at the numbers is one thing, but try getting deeper by watching session replays. Now you see what went wrong exactly.\n\nMarketers:\n\nBreak down conversion numbers according to the different acquisition channels and figure out where your most valuable users are coming from. This way, you can focus your efforts on the more relevant channels.\n\n\nSignals\n\nThe more rare an event is that’s high in an e-commerce funnel (i.e more towards Home Page), the more weight it carries in terms of purchase signaling.\nIf a user has entered the bottom of the funnel (i.e. more towards Begin Checkout) and simply dropped off, it strong reason to reach out in attempts to facilitate or promote movement down the funnel.\n\nData\n\nFunnel data at the user-level over a time period\n\n\nThe columns are in sequence according the conversion funnel (i.e. “Home” is the beginning of the funnel and “Purchase” is the end.)\nValues are counts of viewing events for webpages in the conversion funnel\n\nValues for Home must be an indicator of whether they have visited or started on the Home Page.\n\nInterpretation\n\nUser A\n\nLooks similar to a window shopper, that is engaged enough — 50% of their PLP views turn to PDP views.\nThey have not added anything to a cart but they may have had something there from previous sessions — which is indicated by 1 cart view.\n\nUser B\n\nLikely a customer who is actively trying to make a choice. They may be preparing their cart for a transaction but have not started checking out yet.\n\nUser C\n\nWent way deeper into PLP browsing.\nShows signs of being ready to commit to a purchase and even started checking out once. However, they did not complete a transaction.\n\nPossibly, they dropped off in search for coupon codes or better deals elsewhere.\nMaybe, based on the high PLP view count, they were deep into search but did not manage to find the products of interest.\n\n\nUser D\n\nProbably knew what they wanted, which is\n\nIndicated by a relatively high ratio from PLP to PDP views and high PDP views to Add to Cart ratio.\n\nThey viewed their cart multiple times, reviewing it. But somehow, they have not started the checkout.\nThis could be a perfect candidate for the abandoned cart campaign.\n\nUser E\n\nProbably a returning customer who come back shortly after another session.\n\n\n\nSignal Scoring Steps\n\nChoose a timeframe\n\nDepends on the business model and the action you’re expecting to take with it.\nExamples\n\nUsers are taking up to around 1 month to consider a purchase, then update segments on a 30-day rolling basis.\nYou want to communicate with your customers daily, then daily morning updates could be something to consider.\n\n\nIndividual Signals: events that are positively associated with the conversion (e.g. the number of daily PLP views, PDP views, add to carts, etc.)\n\nUnderstanding how a user scores in each one of these signals can help identify which part of the conversion funnel was not covered by a user.\n\nThese scores can inform how the business should interact with that customer\n\nFor each signal in terms of activity (i.e. counts of events), segment customers into 3 quantiles (&lt; Q33, Q33 &lt; Q66, &gt;Q66) with labels below average, average, above average customers\n\nCan also label customers who did not have any events for a signal at all (0-score users)",
    "crumbs": [
      "Product Development"
    ]
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-sopd",
    "href": "qmd/product-development.html#sec-proddev-sopd",
    "title": "Product Development",
    "section": "Stages of Product Development",
    "text": "Stages of Product Development\n\nStage 1: Coming up with initial product ideas.\nStage 2: Selecting ideas:\n\nQuantitative analysis to select a subset of ideas to which to devote resources, often referred to as opportunity sizing.\n\nStage 3: Experiment design:\n\nInvolved with selecting success and guardrail metrics, running sanity checks, choosing randomization units, etc.\nCandidates will need to consider alternatives when it is not possible to run A/B tests.\n\nStage 4: Making a launch decision:\n\nMaking scientific decisions based on experimentation results.\nDiagnosing problems and evaluating tradeoffs.\nThroughout the product development lifecycle, working with the appropriate metrics is of paramount importance",
    "crumbs": [
      "Product Development"
    ]
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-wdplas",
    "href": "qmd/product-development.html#sec-proddev-wdplas",
    "title": "Product Development",
    "section": "Why Do People Leave and Stay?",
    "text": "Why Do People Leave and Stay?\n\nMight need to be answered by user research and experiments instead of user analytics data\nCollaborate with the UX research team to help design the surveys and interviews, and generate insights\nCollaborate with the engineers to design experiments and analyze results\nSome reasons people churn:\n\nThey might not understand the product.\nThe product might be hard to use.\nPeople might not see the value of the product.\nPeople prefer a competitor’s product.\nThe product might have some issues like bugs or being slow.\nThe new user is not the target user. There could be a mismatch between the user acuiqition and core features.\nPeople might only need our product for a short period of time.\n\nSome reasons people become long term users:\n\nPeople love the product.\nPersonalized notification works.\nIt has become a habit for people to use the product.",
    "crumbs": [
      "Product Development"
    ]
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-fdbk",
    "href": "qmd/product-development.html#sec-proddev-fdbk",
    "title": "Product Development",
    "section": "Feedback",
    "text": "Feedback\n\nMisc\n\nBefore even touching any of the data, ask yourself some user privacy questions: do we have good reason to be looking at the user data? Would we be violating any user privacy? In modern technology, user privacy is at the forefront so this should always be your first thought.\n\n\n\nTypes\n\nIn-product bug reporting: Many products give users a way to leave feedback if the system crashes or they navigate somewhere unexpected.\nIn-product feedback: Products can ask for more general feedback in specific locations or always give users an option in site menus.\nSocial media: Thanks to the wide penetration of social media, it’s common to find feedback about products online. You can track specific tags on Twitter, monitor your company’s Facebook or Instagram profiles or check your executive’s posts.\nInternal feedback: Many people working internally in a company will have a view of how the product behaves and capturing feedback from these people is also important.\nUser research: Companies can actively seek user feedback through user research which often includes recruiting individuals and asking questions directly.\n\n\n\nPotential Insights\n\nInform product direction: If users are consistently asking for a specific feature or requesting you to add more types of content, then the product may very well benefit from following their advice.\nIdentify bugs: When something goes wrong, users are often happy to complain about it. Monitoring user feedback is a great way to get quick insights into potential problems with your product.\nUnderstand product sentiment: Product teams will often ask themselves, how are we doing? There are many ways to answer this question including benchmarking against similar products and looking at retention metrics, but an equally effective method is to understand product sentiment through user feedback. This can help you know when you may need to invest more in building a better product, or if you can go heavily into marketing spend.\n\n\n\nIssues\n\nUser feedback is typically biased: If people are unhappy with something, then more likely to shout about it, then if they’re not fussed by it. This can lead to the feedback you actually receive being weighted heavily to the extremities, both users complaining about negative experiences and also those strong advocates who loved your product. There’s no silver bullet for dealing with this, so it’s important that all analyses done be caveated with this.\nUser feedback is noisy: Feedback comes from people, and people have very different circumstances when coming into contact with your product. A feature might be loved by one user and hated by the next. To this end, it’s important to try to get as much data as possible, and from different sources. Another important approach is to look deeper than the averages, rather than just the mean, look at the actual distribution of reviews — are there a few people with very negative ratings changing the average?\n\n\n\nDS Outputs\n\nSelf-service dashboards: Users across a company can use the dashboard to get insights on feedback.\nRegular reporting: Having a good understanding of the feedback data can lead to automatic generation of reports which can drastically reduce the time to actually understand feedback and get insights out of it.\nScalable datasets: Datasets which can be used by employees across the company.\n\n\n\nEDA/Processing\n\nFor your product area, identify the keyword/s that users are likely to use, for example if you’re running a clothing e-commerce store, they might be: “fit”, “style”, “expensive”, “ugly”, etc. Try to keep the total number to less than ten.\nSample at least 100 pieces of feedback from the total population and do a simple text match with the terms identified in the step above.\nManually scan through the sampled feedback. Did they all fall into the categories already thought of? Does the category actually match the content? For those missing a category, should there be a new category added to capture this?\nYou can iterate on the three steps above until you’re happy with how the feedback looks.\nPlot the volume of reports matching your keyword/s, do you see a consistent pattern, are there increases around any product launches? Correlation with product launches and known bugs can help confirm the reports correlate with what you’re interested in.\nCheck the volume is high enough to be useful. You should aim to find a few hundred reports per day in order to get meaningful feedback. Of course, you can work with fewer if that’s all you have, but it will be harder to detect quick changes.\n\n\n\nModeling\n\nPerform sentiment analysis to understand if users are positive or negative towards your product.\nRun topic modeling algorithms to have a more flexible understanding of the topics users are giving feedback on.\n\n\n\nTrack Metrics\n\nThis should include taking the analyses completed above and making timeseries to add to dashboards.\nSet up anomaly detection to be alerted if there are significant movements in the metric. This can be simple percentage movements in the key metrics that you identified above in the understand and modelling phases, or you can use more sophisticated algorithms.\nIt’s also good to create automated reports where possible, to summarize longer term trends, such as quarterly reports. You’ll always require some manual work, but setting up the queries and making it as easy as possible to action is important.",
    "crumbs": [
      "Product Development"
    ]
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-metrics",
    "href": "qmd/product-development.html#sec-proddev-metrics",
    "title": "Product Development",
    "section": "Metrics",
    "text": "Metrics\n\nTypes\n\nSuccess\nGuardrail\nFunnel\nInput-Output\nGrowth\nPlatform\n\n\n\nSuccess Metrics\n\nSometimes called a “north star metric” or “primary metric” or “OKR metric”) are used to measure the success or health of a product. Commonly used (daily/monthly) success metrics include:\n\nActive Users\nBookings / Purchases\nRevenue\nClick Through Rate\nConversion Rate\n\n\n\n\nGuardrail Metrics\n\nMeasures core quantities that should not degrade in pursuit of a new product or feature. Success metrics of other core product teams can sometimes also serve as guardrail metrics. Examples can include:\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. High bounce rate can indicate the landing page needs improvement\nCancellation / Unsubscription Rate\nLatency\n\n\n\n\nFunnel Metrics\n\nAny family of metrics that tracks the “user journey” through various parts of a product.\nAARRR growth metrics (see below for details: A user who is activated must have been acquired (through some channel or organically); a user who generates revenue must have been previously activated.\nConversion Funnels: a standard “conversion path” through the product, captured through basic metrics such as:\n\nTech Company\n\nNumber of visitors to webpage;\nNumber of logged-in users; Number of users who click particular parts of the logged-in pages;\nNumber of users who visit the checkout page;\nNumber of users who purchase.\n\nB2B (Business to Business)\n\nNumber of visitors to webpage (leads);\nNumber of leads who request free trials;\nNumber of leads to which the Sales team proactively reaches out;\nNumber of paid customers (each of which is a company / business) who made recurrent purchases.Number of visitors to webpage (leads);\nNumber of leads who request free trials;\nNumber of leads to which the Sales team proactively reaches out;\nNumber of paid customers (each of which is a company / business) who made recurrent purchases.\n\nWhat makes a good step to monitor in a conversion funnel?\n\nNotes from Step Suggestions: How We Take the “Guess and Check” Out of Building Funnels\nA good funnel step is ubiquitous: almost no users reach the next step without performing this action.\n\n\nExamples:\n\nUbiquitous: “Enter Password” or “Click Add to Cart”\nNot Ubiquitous: “Subscribe to Mailing List” or “View Warranty”\n\nTo qualify as ubiquitus, at least 97% of completers need to have completed the step.\n\nA good funnel step is divisive: some users drop off before performing the step, and others drop off after.\n\n\nX is a good milestone: 25% of users drop off from A to X, and 33% drop off from X to A (divisiveness = min(25%, 33%) = 25%).\nY isn’t a good milestone: 0% of users drop off from A to Y (divisiveness = min(0%, 50%) = 0%).\nZ isn’t a good milestone, 0% of users drop off from Z to B (divisiveness = min(0%, 50%) = 0%)\nNot Divisive Example: “Submit” at the end of a form\n\nIf 100% of people that click that button reach the next page, then it’s not worth adding a step to break that down\n\n\nExamples\n\nform field change events, like entering a password or checking a checkbox.\npageview\n\nPitfalls\n\nAdding a step to the funnel that isn’t required for conversion\n\nIf you add this step, the conversion rate will be biased\nExample:\n\nFunnel for a login form which has a true conversion rate of 36% (login submits/login pageviews)\nEntering a phone number was an optional step. Adding it to the funnel drops the conversion rate to 19.98%\n\n\n\n\n\n\n\n\nInput-Output Metrics\n\nInput / Driver Metrics: Metrics that track the activities / resources used to work towards an outcome.\nOutput metrics: Metrics that demonstrate the outcome of an initiative.\nExample: Search and Recommendation\n\nInput Metrics\n\nClick-Through-Rate (CTR)\nAverage Time Spent on a particular types of content\n\nOutput Metrics\n\nAverage Time Spent on the platform per user\nSessions-per-User\nSuccessful Sessions per User\nAdvertisement Revenue\n\n\nExample: Fraud Detection\n\nAlso see Platform Metrics\nInput metrics\n\nTrue / False Positive Rates of\n\nFraud Rules and Models\n\nFraud scores are calculated based on rules, which add or subtract points. The user action may be a transaction, signup or login. Rules look at data points such as an email address, IP address, or social media presence.\n\nOperations Manual Review Volumes\n\nA human reviews the case to determine whether action is needed. In fraud, an model output may trigger a “manual review” to determine whether an event was indeed fraudulent.\n\nCustomer Transaction Amounts\n\nWhether or not a transaction not fit a customer’s normal buying habits\n\n\n\nOutput metrics\n\nFraud Loss Volumes\nLosses Prevented\nRevenue\n\n\n\n\n\nGrowth Metrics (AARRR)\n\nAre responsible for enlarging a product’s user base and keeping current users engaged\nAlso see KPIs &gt;&gt; Product Metrics &gt;&gt; AARRR\n\n\nAARRR\n\nAcquisition: Getting customers to sign up for a website or product, which requires driving product awareness (often known as “top of funnel”).\nActivation: Getting customers to gain basic familiarity with the product, and appreciating the basic product value.\nRetention: Getting customers to come back to the product on a regular basis; a customer that exhibits no (or minimal) activity over some predetermined time period is known as churned (the precise definition depends on the business context and varies greatly across teams / companies).\nReferral: Getting customers to share the product with others, in turn further growing the user base.\nRevenue: Getting customers to adopt one or more paid features that generate revenue for the company; also known as Monetization.\n\n\n\nConcepts\n\nTradeoffs Between Acquisition and Revenue\n\nHow should a company strike the optimal balance between revenue maximization and acquiring new customers (the latter can always be done via expensive campaigns, such as providing large discounts / gifts for new users, etc.)?\n\nDifferent Levels of Engagement\n\nTypically, a company will have multiple tiers of user engagement metrics, which reflect different levels of product engagement (with “activated user” being the lowest tier).\nIf daily use is what is most valuable (e.g. Facebook, ad revenue), Engagement = Daily Active Users (DAU) / Monthly Active Users (MAU)\n\nEmails and push notifications tend to increase casual numbers (the MAU) but not the daily users (DAU).\nThe magnitude of the engagement of this type is mostly determined by the product category and doesn’t change much over time\n\nsee Misc &gt;&gt; Loyalty measure by app category\n\nhigh-frequency, high-retention apps (e.g. communication)\n\ni.e. an app with 10% DAU/MAU doesn’t usually turn into a 40% DAU/MAU\n\n\nIf daily use is NOT where the company’s value comes from, then find and investigate power users.\n\ne.g. Users of companies like Uber, ecommerce, Airbnb don’t use their services every day but these companies are huge.\n\n\nRetention and Churn Cannot be Easily Estimated Over the Short Term:\n\nDefinitions of retention and churn vary from company to company, but typically require weeks if not months of continuous observation. Furthermore, it is possible for a churned customer to recover (often known as resurrection), so it is important to clarify the precise metric definitions and assumptions before answering any questions (e.g. first time churning, most recent time churning, ever churned, etc.).\n\nDifficulty of Measuring Incrementality for Referral Initiatives:\n\nWhen new referral programs are launched, network effects are typically introduced because any new customer can be simultaneously a referrer and a referee.\n\nDifferences Between B2B and B2C Settings:\n\nIn B2B companies, acquisition/activation/retention etc. are typically defined at the “company / business” level, because a customer is a company / business. There could be analogous granular user-level metrics as well.\n\nChallenges of International Expansion:\n\nThese include additional costs associated with localization (adapting a product appropriately to better fit the customers in a particular market), regulatory / compliance requirements, understanding the competitive landscape, etc.\n\n\n\n\nPower User Analysis\n\nQuestions\n\nWhich customers generate the most value for your company?\nHow are they doing it?\n\nFrequency of use?\n\nWhat % of your users are active (engage in valuable activity) every day last week?\n\nNetwork effects?\nContent they’ve produced or saved?\nWhat characteristics make them different from the casual user?\n\nThe action(s) should correlate to some value metric.\nHow are you going to produce or acquire more of them?\nhow can I understand their needs better and make sure we continue building in a direction that supports their daily workflow (and that we can upsell new features)?\n\nPower User Curve (aka Activity Histogram or the “L30”)\n\nHistogram of users’ engagement by the total number of days they performed which action you’ve deemed valuable in a month\n\nCan be applied to a cohort if you’re a subscription business, track product launches, track results from feature changes, or results from marketing campaiagns, etc. (see below)\nI don’t think this concept has to be limited to time being on the x-axis. It could be any metric value you consider representative of a “power user.”\n\nAlso see Google, Analytics, Analysis &gt;&gt; Power User Analysis (Example 24)\n\nY axis: % users, X axis: visits\n\n\n\nTime Period\n\nDepends on your products natural cycle. May be monthly (“L30”), weekly (“L7”), daily, etc.\nExample: productivity/work-related products that users engage with Monday through Friday. B2B SaaS products will often find it useful to show this version, as they want to drive usage during the work week.\n\n“Smile” Curves are Good\n\nMonthly\n\n\nThe 7% (far right) that engage daily shows that a group of users are returning frequently (daily) so it may be possible that the impressions can support an ad business\nSuccessive Power User Curves should ideally show users shifting over more to the right side of the smile\n\nWeekly\n\n\nThe “smile” occurs between 1 and 5 days which makes sense for productivity type product that is most active during the workweek.\n\n\nCohort Analysis\n\n\nAlso see Algorithms, Product &gt;&gt; Cohort Analysis\nShows a positive shift in user engagement from August to November, where a larger segment of the population is becoming active on a daily basis, and there’s more of a smile curve.\nInflection points where the line starts to bend upward for a particular month can indicate:\n\nWhen a critical product release, unblocking features, or a marketing effort might have started to bend the curve\n\nThis might be a place to double down, to increase engagement.\n\nFor a network effects product, you might expect to see newer cohorts gradually improve as you achieve network density/liquidity.\n\n\n\n\n\n\n\nRARRA\n\n\nAARRR focuses too much on Acquisition even though Retention is more important, especially for many internet products that struggle to retain people.\nReasons why retention is more important:\n\nAcquisition strategies (e.g., ads) are expensive and it is often cheaper to retain a user than get a new user\nRetention is the foundation of growth\nUser retention is more directly connected to revenue than acquisition.\n\n\n\nPlatform Metrics\n\nDepartments include Customer Support, Trust and Safety, Payments, Infrastructure, Operations\nMany of these are indirectly related to the Growth Metrics\nMetrics\n\nCosts of Operations / Infrastructure: A granular understanding of various infrastructure costs allows one to effectively evaluate tradeoffs for expansion.\n\nAverage cost of a help center ticket / escalation - essential quantity to track for many types of product changes in customer-facing companies\n\nSuccess / Failure Rates: These metrics are strongly correlated with conversion, particularly in the Monetization context (since payments are typically the final step of the monetization funnel).\n\nPercentage of transaction attempts that succeed / failure out of all transaction attempts\n\nFalse Positive / False Negative / True Positive Rates (for fraud detection): rules and models for detecting bad user behavio\n\nAlso see input/output metrics\nFalse positives can decrease a good customer’s lifetime value (LTV or CLV)\nFalse negatives increase fraud losses\n\nFraud Loss Metrics: As a company scales, fraud losses typically follow (from a mixture of)\n\n“Born Bad Users” - Users who abuse the product for inappropriate financial gains and\nThird Party Fraud - Users who take over the accounts of good users\n\nVendor Costs: (e.g. Stripes for payments processing)\n\nNeed to be carefully weighed against other core business metrics.",
    "crumbs": [
      "Product Development"
    ]
  },
  {
    "objectID": "qmd/real-estate.html",
    "href": "qmd/real-estate.html",
    "title": "Real Estate",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Real Estate"
    ]
  },
  {
    "objectID": "qmd/real-estate.html#sec-rlest-misc",
    "href": "qmd/real-estate.html#sec-rlest-misc",
    "title": "Real Estate",
    "section": "",
    "text": "Listing Price\n\nAffects the final selling price, how long the home spends on the market, the volume of interest in the house, and anchors price negotiations with buyers\nAlgorithmic estimates have better performances when they take the list price into account\n\nA home has a value distribution as different potential buyers place different values on the various home features. The eventual selling price is a function of this value distribution and the specific individuals who consider the home.\nPrice/Income ratio\n\nExample: Median house price in middle class suburbia in a very affordable region is $323,000. To maintain an price/income ratio of 3, that requires a household income of $107,000. In 2019, regional median HH income was $62,502. ~30% of HH make over $100K.",
    "crumbs": [
      "Real Estate"
    ]
  },
  {
    "objectID": "qmd/real-estate.html#sec-rlest-terms",
    "href": "qmd/real-estate.html#sec-rlest-terms",
    "title": "Real Estate",
    "section": "Terms",
    "text": "Terms\n\nPrice Anchoring - a pricing strategy that plays on buyers’ inherent tendency to rely heavily on a piece of initial information to guide subsequent decisions. In the context of pricing, many businesses will set a visible initial price for a product but make a point of showing that it’s now being sold at a discount.\nPrice Index - a normalized average (typically a weighted average) of price relatives for a given class of goods or services in a given region, during a given interval of time\n\nCase-Shiller U.S. National Home Price Index (also {fredr})\n\nSee Appraisal Methods &gt;&gt; CMA &gt;&gt; Market Price &gt;&gt; Case Shiller\n\nWiki has formulas\nSee Appraisal Methods &gt;&gt; CMA &gt;&gt; Repeat Sales Model\nIf a price index rises 10%, it means the average level of prices has risen 10%",
    "crumbs": [
      "Real Estate"
    ]
  },
  {
    "objectID": "qmd/real-estate.html#sec-rlest-bizq",
    "href": "qmd/real-estate.html#sec-rlest-bizq",
    "title": "Real Estate",
    "section": "Business Questions",
    "text": "Business Questions\n\nAgent\n\nWhich house should I buy or build to maximize my return?\nWhere or when should I do so?\nWhat is its optimum rent or sale price?\n\nBuyer\n\nMatch them with agent\nFind them a home\nAppropriate mortgage",
    "crumbs": [
      "Real Estate"
    ]
  },
  {
    "objectID": "qmd/real-estate.html#sec-rlest-usecases",
    "href": "qmd/real-estate.html#sec-rlest-usecases",
    "title": "Real Estate",
    "section": "Use Cases",
    "text": "Use Cases\n\nUsing computer vision and NLP to enhance searches\n\nIncorporating Image Search Capabilities: Extract information from pictures of the property utilizing object detection and image classification techniques, to be used in search matching.\nRecommendation-Engine-Powered Rankings: Search results could be ranked according to the likelihood that the specific user will interact with the results, based on previous searches, profile characteristics, and contextual information.\nSearch Intent Matching: Enhance the user experience by adding the ability to write (or dictate) their home preference(s) instead of manually filtering the results. It may be very wise to incorporate such a feature given the rise of home assistants.\nVisual Search: Perform a search based purely on images of homes. This would enhance the search experience or complement the keyword search and produce more accurate and useful results.\n\nIdentify homeowners who are more likely to sell and estimate their selling price using publicly available data\nFind correlations between past sellers and current homeowners, in order to help predict the likelihood that a given owner is willing to sell.\nEstimate appropriate selling price point and the interest of possible future owners so as to increase the chances of successful outreaches.\nLead classification:\n\nBased on web-based actions executed by a user, understand where they are at in the customer journey and gather accurate information to move them to the next stage in the funnel.\n\nRisk assessment:\n\nMultiple risks should be considered when assessing a real estate transaction. Forecasting models powered by machine learning could complement a traditional risk analysis approach well, especially given their multiple data source analysis capabilities.\n\nHome valuation:\n\nA classic, yet constantly evolving, machine learning task is to set the price of a house based on MLS and alternative data (e.g. satellite imagery). The big iBuyers are all betting heavily on this since it’s key to their business model. An example of the level of detail obtainable is that of how Opendoor analyzed the impact of busy roads in their property valuation model.",
    "crumbs": [
      "Real Estate"
    ]
  },
  {
    "objectID": "qmd/real-estate.html#sec-rlest-apprais",
    "href": "qmd/real-estate.html#sec-rlest-apprais",
    "title": "Real Estate",
    "section": "Appraisal Methods (Price Estimation)",
    "text": "Appraisal Methods (Price Estimation)\n\nTraditional\n\nComparative Market Analysis (CMA)\n\na collection of recently sold “comparable” homes (“comps”) that, taken in aggregate, can provide a view on the value distribution of the appraised home (the “subject” home)\nselecting similar properties (in terms of attributes and location) and inferring the target value from the comparables\n\nCost Approach\n\nThe “replacement cost” is determined by totaling values such as the value of raw land (again, using comparison), the cost of rebuilding a new building that could perform the function of the existing property, and then making necessary adjustments (e.g., deprecation of the existing building).\n\nProfits Method (or Income Approach)\n\nestimates the value of the property based on the income it generates. The value is linked to the business carried out within that property. For example, the market value of a hotel depends on the potential cash flow derived from ownership.\n\nRepeat Sales Method\n\nanalysis is restricted to just comparing price changes on properties sold more than once\n\n\n\n\nML Approaches\n\nMisc\n\nAutomated Valuation Models (also known as AVMs) were found to have an absolute error below 4% for homes and below 6% for commercial properties, which is much less than the error rates of traditional appraisals.\n\n\n\nFeatures\n\nDifferent predictors may be more valuable for each of the price models.\n\nSale price: Zillow and Redfin use their own algorithms for real estate price estimation.\nRental price: HomeUnion developed a tool called RENTestimate for this.\nTemporary rental price: Airbnb’s pricing tips use a mathematical model that learns how likely a guest is to book a specific listing, on specific dates, at a range of different prices.\n\nAdding nontraditional data sources can improve estimates\n\n\n\n\n\nHedonic Pricing Method\n\nA typical regression that accounts for property features such as size, number of rooms, property age, home quality characteristics (granite countertops, air conditioning, pool, etc.), and location\n{hpiR} (vignette)\nIt’s just multivariable regression where log sale price (or ppsf, etc.) is the outcome and includes house characteristics and has dummy variables for the years (or days, weeks, etc.) when the house was sold\nThe exponential of the coefficients of the year dummy variables are the index and they represent the average change in the outcome variable from the reference year to the year represented by the dummy variable\n\n1 is the index of the reference year\n\n\n\n\nCMA with Comp Price Adjustments\n\nMisc\n\nNotes from\n\nPricing Homes like Agents Do: AI for Real Estate CMA Adjustments\n\nAll models of adjustments seem to use Price per Square Foot (PPSF) for a local area as the outcome variable\nProbably want to go as small an area as you can depending on the amount of training data at that resolution\n\n\n\nProcess\n\nData Scientist Role\n\nSuggests good comparable properties for a CMA\nHighlights the important differences between the comparable and the subject property (the fewer, the better)\nSuggests pricing adjustments for the differences\n\nReal Estate Agent Role\n\nSelects which properties to use as comps\nSelects which adjustments are relevant and appropriate for their CMA report\n\n\n\n\nMarket Price\n\nRepeat Sales Model\n\nEstimates how the price-per-square-foot evolved for each locality. Then we can use the model to estimate the difference in average price per square foot (PPSF) in the locality across any two-time points.\nIf a price index rises 10%, it means the average level of prices has risen 10%\n\nExample:\n\nCMA model produces a comp that sold for $1,100,000 almost a year ago in April 2019\nRepeated Sales model’s indexes say that property prices have decreased by 4.6% in area since the sale of this comp.\nThus, to make this comp comparable now, we would need to subtract $51K (1,100,000 * 0.046 = 50,600) from the price it sold for back in April 2019.\n\n\nIn this description and the examples, I’m using sale price, but the same methods can be applied to PPSF.\nThe data is house id, year sold, sale price\n\nThere will be repeated measures because each house will need to have been sold multiple times.\n\nMaybe can this be fudged a bit by using really good comps for the repeated measures\n\nOnly the data from the 1st and last sales are used\nThe first year in the sample data is NOT included as a variable in the regression\n\nThis is the reference or base year. This year automatically gets an index of 1.\n\n\nAssumes homeoskedacity\nSpecification\n\nFormula:\n\\[\n\\log\\left(\\frac{\\mbox{Last Sale Price}}{\\mbox{First Sale Price}}\\right) = \\beta_0 + \\beta_1 I(\\mbox{year}_1) + \\beta_2I(\\mbox{year}_2) + \\cdots\n\\]\nWhere \\(I(\\mbox{year}\\_*)\\) is an indicator variable with values of\n\n1 for year of last sale\n-1 for year of first sale\n0 for years where the house wasn’t sold\n“*” is the year of the sale of the house.\n\nNote that there is no intercept so will need 0 + in the formula\n\\(e^{β_*}\\) gives the index for that year Index for the first year of the sample data is automatically set to 1.\n\nExample\ndat &lt;- tibble(\n  y = rep(c(0.182, 0.041, 0.039), 3),\n  year = rep(c(\"2013\", \"2014\", \"2015\"), each = 3),\n  value = c(-1, -1, 0, 0, 1, -1, 1, 0, 1)\n) %&gt;% \n  tidyr::pivot_wider(names_from = year, values_from = value, names_prefix = \"Y\")\n      y Y2013 Y2014 Y2015\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.182    -1    0    1\n2 0.041    -1    1    0\n3 0.039    0    -1    1\n\nmod &lt;- lm(y ~ 0 + Y2014 + Y2015, data = dat)\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)\nY2014  0.07500    0.04808  1.560    0.363\nY2015  0.14800    0.04808  3.078    0.200\n\nexp(coef(mod))\n  Y2014    Y2015 \n1.077884 1.159513\n\n“y” is the log of the sales price ratio\nIndexes:\n\n2013: 1\n2014: 1.08\n2015: 1.16\n\n\n\nCase-Shiller Method\n\nHandles heteroskedasticity by Weighted Least Squares\nTypically follows the same trend (highly correlated) as the regular repeated sales model but on a lower level\nPackages\n\nAlso see Regression, Other &gt;&gt; Weighted Least Squares\n{hpiR}(vignette)\n\nMultiple modeling options, including weighted regression, robust regression, random forest\nAlso splits up houses with more than 2 sales into combinations of binary periods\n*Doesn’t sqrt the residuals in the weighted regression like I think the original paper recommends*\n\n\nProcess\n\nFirst fit the Repeated Sales Model\nRegress the squared residuals on the holding period\n\\[\n\\epsilon^2 = \\beta_0 + \\beta_1 \\cdot \\mbox{Holding Period}\n\\]\n\nHolding Period: The period between the first sale and last sale\n\nCalculate weights using fitted values\n\\[\n\\mbox{weights} = \\frac{1}{\\sqrt{\\hat{ε^2}}}\n\\]\nFit Weighted Repeated Sales Model\n\nModel\nmod &lt;- lm(LogP ~ 0 + ., data = dat %&gt;% select(-HoldingPeriod))\nmod_resids &lt;- resid(mod)\n\nholding_dat &lt;- dat %&gt;% \n  select(HoldingPeriod) %&gt;% \n  mutate(resids_sq = mod_resids^2)\n\nresids_mod &lt;- lm(resids_sq ~ HoldingPeriod, data = holding_dat)\nresid_preds &lt;- fitted(resids_mod)\nwgts &lt;- ifelse(resid_preds &gt; 0, 1 / sqrt(resid_preds), 0)\n\nfinal_mod &lt;- lm(LogP ~ 0 + ., \n                data = dat %&gt;% select(-HoldingPeriod),\n                weights = wgts)\n\n# indexes\nexp(coef(final_mod))\n\nHolding Period: the period between the first sale and last sale\n“LogP”: See previous model’s Specification\n\n\n\n\n\nBuilding-Floor\n\nHigher floors in apartment building cost more\nAdjustment varies per building but there isn’t enough training data to have a model per building\n\nSolution: Bin apartment data by building size, then fit a model for each bin.\n\nPerform Market Price adjustment to prices before modeling price differences by floor\n\n\n\nLocation\n\nBuildings are of different ages, have various contractual constraints, and offer vastly different amenities.\n\nExample: A subject propert in a very expensive building for this neighborhood. The comp property is in a building where similarly sized apartments tend to sell for lower prices. So an adjustment to the comp needs to be made.\n\nSpatial nearest neighbor models for ppsf.\n\nTo prepare the training data, we use the Market Price adjustment discussed already\nPackages: {nngeo}",
    "crumbs": [
      "Real Estate"
    ]
  },
  {
    "objectID": "qmd/real-estate.html#sec-rlest-feats",
    "href": "qmd/real-estate.html#sec-rlest-feats",
    "title": "Real Estate",
    "section": "Features",
    "text": "Features\n\nAlso see Feature Engineering, Geospatial\nScrape Traditional Variables\n\nNumber of bedrooms\nNumber of bathrooms\nLiving area (square footage)\nPrice per Square Foot (PPSF)\nNumber of Stories\nYear Built\nProperty Type (e.g. Condo, House)\nFurnished/Not Furnished\nFireplace/No Fireplace\nHeating/No Heating\nPool/No Pool\nGarage Space\nZIP Code\nLatitude and Longitude\nListing Price of comparable homes in the area\nSale Price\nDays on the Market\n\nOff-Market Data\n\nTax Assessments\nPrior Sales and other publicly available records\nResident Surveys\nMobile Phone Signal Patterns\nYelp Reviews of local restaurants help identify “hyperlocal” patterns—granular trends at the city block level rather than at the city level.\n\nMarket Trends\n\nInflation\nSeasonality in Demand\nEconomic Shocks\n\nGeographic Indicators\n\nUber’s H3 grid system instead of zip codes, etc. (Also see Geospatial, General)\nThe quality of local schools\nEmployment Opportunities\nProximity to shopping, entertainment, and recreational centers\nSources Google Places, Yelp, or SchoolDigger\n\nImage data\n\nSatellite Imagery\n\nMapbox’s Satellite API\n\nAllows you to query any location by its latitude and longitude coordinates\nSelect from 20 different zoom levels\n50,000 free requests each month!!\n\nNumber of roads and their condition\nHouses with Pools\n\nMethod\n\nConvolutional Neural Networks (CNNs) can extract visual features, revealing underlying information captured in photos\nSatellite\n\nA classification network can distinguish between the top and bottom 15% of houses in a dataset to test this. A model achieved an outstanding 91% accuracy, using only satellite images. After interpreting the model to understand which visual patterns governed its decision process, we saw that it placed high importance on recreational areas such as parks and lakes.\n\nExterior Frontal Images\nInterior Photos\n\nFeatures\n\nThe activeness of a street frontage\nAccessibility of the Area (number of roads, condition of the roads)\nAmount of Urbanization\n\n{uci} (Urban Centrality Index) - measures the extent to which the spatial organization of a city or region varies from extreme polycentric to extreme monocentric in a continuous scale from 0 to 1.\n\nProximity to parks, lakes and beaches (recreational areas)\nAmount of Greenery\nPopulation Density\n\n\nNLP\n\nIdentify the most important words in a description and learn which words tend to be used to describe more expensive or affordable houses.\n\n\nSociodemographic\n\nMisc\n\nWhich data resolution is most predictive? (e.g. state, county, neighborhood, etc.)\nOfficial government data sources are best (consistent, unbiased, and up-to-date indicators, with the largest sample sizes)\n\nMedian Household Income\nUnemployment Rate\nCrime Rate\nEthnicity\nNumber of Inhabitants\nAverage Age\nPoverty Level\nSource: American Community Survey (ACS) at census tract level\n\nSmall subdivisions of a county (around 4,000 inhabitants on average),\nDesigned to be relatively homogeneous units concerning population characteristics, economic status, and living conditions\n\nTotal Population\nTotal Employed Population\nPopulation Median Age\nMedian Household Income\nPopulation with Income Below Poverty Level\nMinority Percentage\nVacant Housing Units for Rent\nMedian Value for all owner-occupied housing units",
    "crumbs": [
      "Real Estate"
    ]
  },
  {
    "objectID": "qmd/retail.html",
    "href": "qmd/retail.html",
    "title": "Retail",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Retail"
    ]
  },
  {
    "objectID": "qmd/retail.html#sec-retail-misc",
    "href": "qmd/retail.html#sec-retail-misc",
    "title": "Retail",
    "section": "",
    "text": "DS Use Cases (intermediate level - data needs to be centralized and not siloed)\n\nDemand forecasting models for optimizing the stock levels.\nMarket basket analysis models for creating better newsletter offers.\nDynamic pricing models using competitors’ prices for better pricing strategies.\nCustomer segmentation model for better understanding our customers’ shopping preferences and providing them with customized offers and loyalty discounts.\n\nApproach to deciding on a store to close\n\nConsiderations\n\nCatchment Area: How to define the area that matters around my store\nStore Network: What are the relationships between stores\nPredicting Sales Transfer of your Customers (aka Customer Retention)\n\nOverlap in catchment areas and understanding how customers behave in your store network can help in this estimation\n\nPredicting the Impact on Acquiring New Customers (aka Customer Acquistion)\nBringing it together\n\nCalculate 1-year, 3-year impact of removing a particular store\nRank these impacts to choose a store whose removal will have the least impact\n\nMeasurement & Model Improvement\n\ne.g. Including real estate data such as lease terms and market intelligence on competition and anchor stores\n\n\nImpact on Customer Retention\n\nDepends on the convenience and quality of other channels for doing business (such as online or in physical locations), customer loyalty to the company and its products, and market competition.\nIf you have historical data on customer retention after a store closure, you can use catchment and store network features to predict what will happen with customer retention if you decide to close a particular store.\n\nImpact on Customer Acquistion\n\nFor a particular store, take the its catchment area and remove any section that overlaps with another store’s catchment.\n\nThis area will not have any support from a brick-and-mortar perspective to help in your acquisition efforts\n\nLook at trends over time to get a sense of the relative importance of this store’s area to the overall chain and whether you should build a digitally supported strategy to offset the loss of acquiring new customers at a fraction of your cost of operating a retail store at that location",
    "crumbs": [
      "Retail"
    ]
  },
  {
    "objectID": "qmd/retail.html#sec-retail-lscore",
    "href": "qmd/retail.html#sec-retail-lscore",
    "title": "Retail",
    "section": "Lead Scoring",
    "text": "Lead Scoring\n\nAlso see\n\nAlgorithms, Marketing &gt;&gt; Propensity Model and Uplift Score Model\n\nUse case: identify customer cohorts that are unlikely to become paying customers and eliminate the low efficient outreach\nMap User Journey\n\nAlso see Marketing &gt;&gt; Customer Journey\nExample: B2B\n\n\nQuestions\n\nFind customer’s last touchpoint before conversion\n\ne.g. Proof of concept in B2B\n\nBrainstorm on metrics that may correlate with conversion and form hypotheses\n\ne.g. Metrics that describe user behaviors in proof-of-concept\n\nWhich customer cohorts are likely to schedule sales calls with us and how do they behave without sales guidance\n\ni.e. Which cohorts are more likely to convert if they are given a sales call\nLook at how prospects behave in product trials and interact with marketing emails\n\n\nEDA\n\nPlot metrics by cohort\nCalculate correlation of metrics to conversion\n\nCohorts can be segmented by any grouping factor\n\n\nModel\n\nIf you have enough data, model conversion rate ~ predictors (beta regression?) or convert/no covert ~ predictors (logistic regression, ML, DL)\nOtherwise, weight cohorts by their correlations with conversion",
    "crumbs": [
      "Retail"
    ]
  },
  {
    "objectID": "qmd/retail.html#sec-retail-catch",
    "href": "qmd/retail.html#sec-retail-catch",
    "title": "Retail",
    "section": "Catchment",
    "text": "Catchment\n\nRefers to the sphere of influence from which a retail location, such as a shopping center, or service, such as a hospital, is likely to draw its customers.\nMajor considerations – supply factors, market demand factors, drive time from customers, transportation access (e.g. interstate) and consumer interactions\nSteps\n\nUse the customers that purchased at the store in the past 12 or 24 months (you need to be the judge on timeframe based on your business model), and map them to where they live, e.g., at census block group level.\nIf the area is massive, applying an optimization function, where you use a polygon and work to minimize the size of the area while keeping the cumulative percent of the total sales as large as possible. (70-80% of cumulative sales is typically optimal)\n\nAssume this optimization function takes into account the considerations mentioned above\n\n\nMeasuring the amount of competition in the given catchment area is useful\nCatchment areas might overlap.",
    "crumbs": [
      "Retail"
    ]
  },
  {
    "objectID": "qmd/retail.html#sec-retail-stnet",
    "href": "qmd/retail.html#sec-retail-stnet",
    "title": "Retail",
    "section": "Store Network",
    "text": "Store Network\n\nUnderstanding how stores are connected beyond a catchment area is essential to make smarter optimization decisions (i.e. closing or opening stores).\nSteps\n\nPick a store and for each customer, note which of your other stores (including ecommerce) that customer has shopped at\n\n(Typically) 3 types of customers:\n\nCustomers who shop at this store exclusively\nCustomers who spend the majority of their $ with you at this store and spend a smaller amount at other stores, including e-commerce\nCustomers who spend a small amount of their $ at this store and spend the most significant amount at other stores, including e-commerce\n\nFor the ones that have shopped at multiple locations, those are generally your more loyal and high-value customers.\n\nThis data forms the basis of the network model\n\nIf the network gets too complicated, then pruning by adding addition rules might be necessary\n\ne.g. Establishing rule for a minimum amount spent to the store network",
    "crumbs": [
      "Retail"
    ]
  },
  {
    "objectID": "qmd/saas.html",
    "href": "qmd/saas.html",
    "title": "SaaS",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "SaaS"
    ]
  },
  {
    "objectID": "qmd/saas.html#sec-saas-misc",
    "href": "qmd/saas.html#sec-saas-misc",
    "title": "SaaS",
    "section": "",
    "text": "Notes from\n\nMeaningful metrics: How data sharpened the focus of product teams\nOpen Source does not win by being cheaper\n\nOpen-Source solutions are great when a transparency problem is present\n\nSaaS tools need to process sensitive client data such as IP addresses, names, session recordings, etc., and in a world of increasing data regulations (e.g., GDPR and CCPA), it can be daunting for third parties to store that data.\nExample: PostHog\n\nOptions\n\nSelf-host an analytics solution yourself\nHire PostHog as a third party to do it, but with transparency into how that data is stored and how possibly migrating to self-hosted in the future would work.\n\nMany companies will still opt for a hosted model, but they do so now with assurances of how the software works—line by line—and the process of migrating to a self-hosted model in the future if necessary. It’s not that open-source companies win by preventing the need for a third party; they win by allowing for the open audit of how it works.\n\n\nOpen Source can lean on the community to develop connections and extensions\n\nWhile the core product is typically maintained by a central engineering team, integrations or plugins are often built by community developers and then occasionally merged into the main branch.\nConversely, closed-source solutions struggle with this because they rely on their engineering team. By tapping the community for feedback and help, open-source projects can also accelerate past closed-source solutions\n\nLarger companies aren’t worried about going out of business because a piece of software is too expensive. They might be price-conscious in terms of negotiating a contract in context of their budget, but most SaaS software is just a line item at the end of the day.\n\nWhat matters more is that the solution is (a) a good solution, (b) around for the long haul, and (c) easy to manage. And, unfortunately, deploying an open-source solution can be tricky to manage.\nMost open-source solutions aren’t replacing a top-three line item, and therefore price isn’t the north star deciding factor.\n\nMongoDB eventually switched to a special SSPL license to add specific restrictions on Cloud Providers from distributing a service without contributing to the project, which isn’t OSI approved, but is open-source practically).\n\nBut why make this point? Well, splitting the hair between profit and usage is important to measuring long-term success. If a project gets great adoption but cannot drive revenue, it will die.",
    "crumbs": [
      "SaaS"
    ]
  },
  {
    "objectID": "qmd/saas.html#sec-saas-userseg",
    "href": "qmd/saas.html#sec-saas-userseg",
    "title": "SaaS",
    "section": "User Segmentation",
    "text": "User Segmentation\n\n\nDiagram of users based on their activity on the language learning software, “duolingo.”\n\nSee article for an example of a user journey\n\nActivity States\n\nNew users: learners who are experiencing Duolingo for the first time ever\nCurrent users: learners active today, who were also active in the past week\nReactivated users: learners active today, who were also active in the past month (but not the past week)\nResurrected users: learners active today, who were last active &gt;30 days ago\nAt-risk Weekly Active Users (WAU): learners who have been active within the past week, but not today\nAt-risk Monthly Active Users (MAU): learners who were active within the past month, but not the past week\nDormant Users: learners who have been inactive for at least 30 days\n\nRates\n\nRetention Rates\n\nNew User Retention Rate (NURR): The proportion of day 1 learners who return on day 2. This day 2 transition puts these learners into another “active” state (Current User)\n\nDeactivation Rates: e.g. WAU or MAU loss rate\nActivation Rates: e.g. Reactivation or Resurrection rate\n\nProcess\n\nClassify all users (past or present) into an activity state each day\nCollect data: monitor rates of transition between states.\n\nThese transition probabilities are monitored as retention rates, activation rates, and deactivation rates\n\nUse transition probabilities to create Markov model for simulations\n\nIdentify new metrics, through simulations of the Markov model, that - when optimized - are likely to increase a North Star or Primary metric.\n\nExample\n\n\nSimulation is for 3yrs into the future\n“Lever” is the parameter being increased\n\nIn the sim, it’s increased 2% month-over-month\n\nDaily Active Users (DAU) is the North Star metric that is being opitimized (i.e. increased)\nInterpretation: Increasing Current User Retention Rate (CURR) increases DAU by 75% over the next 3yrs\n\nFormulas at time, t\n\nDAUt = ReactivatedUsert + NewUsert + ResurrectedUsert + CurrentUsert\nWAUt = ReactivatedUsert + NewUsert + ResurrectedUsert + CurrentUsert + AtRiskWAUt\nMAUt = ReactivatedUsert + NewUsert + ResurrectedUsert + CurrentUsert + AtRiskWAUt + AtRiskMAUt\nReactivatedUsert = ReactivationRatet * AtRiskMAUt-1\nResurrectedUsert = ResurrectionRatet * DormantUserst-1\nCurrentUsert = (NewUsert-1 * NURRt) + (ReactivatedUsert-1 * RURRt) + (ResurrectedUsert-1 * SURRt) + (CurrentUsert-1 * CURRt) + (AtRiskWAUt-1 * WAURRt)\nDormantUsert = (DormantUsert-1 * DormantRRt) + (AtRiskMAUt-1 * MAULossRatet)\nAtRiskMAUt = (AtRiskMAUt-1 * ARMAURRt) + (AtRiskWAUt-1 * WAULossRatet)\nAtRiskWAUt = (AtRiskWAUt-1 * ARWAURRt) + (CurrentUsert-1 * (1-CURRt)) + (ReactivatedUsert-1 * (1-RURRt)) + (NewUsert-1 * (1-NURRt)) + (ResurrectedUsert-1 * (1-SURRt))\n\n\nPerform A/B tests to see whether 1) the selected metric can be moved and 2) whether moving it actually moves the north star/primary metric",
    "crumbs": [
      "SaaS"
    ]
  },
  {
    "objectID": "qmd/sports-betting.html",
    "href": "qmd/sports-betting.html",
    "title": "Sports Betting",
    "section": "",
    "text": "Misc",
    "crumbs": [
      "Sports Betting"
    ]
  },
  {
    "objectID": "qmd/sports-betting.html#sec-spbet-misc",
    "href": "qmd/sports-betting.html#sec-spbet-misc",
    "title": "Sports Betting",
    "section": "",
    "text": "BetCris, Bookmaker, and Pinnacle are market makers. Best to use their closing lines.\n\nAlso, if at any time you find a line better at Bookmaker or another site than is offered at Pinnacle you might have a smart bet. The next step would be to return to this page and calculate Pinnacle’s No-Vig price. If the odds you’re getting are better than Pinnacle’s no-vig price chances are you have a +EV (positive expected value) wager.\n\nA move from -110 to -120 raises the break-even point from 52.38% to 54.55% (i.e Fair Value Odds but given as implied probabilities here)\nSteps to avoid getting picked off by bookmakers\n\nThese are actions that sharp bettors take that bookmakers used to help identify them.\nNotes from Eliminate These Five Sharp Betting Tells From Your Game\nSharps tend to bet early and help shape a stronger line for books to present to the less-knowledgeable public betting crowd who will bet later.\n\nSolution: Instead of always betting when the opening line is published, bet when key events for that sport take place (e.g. when NFL injury report is published, when an informative site/person posts their report/info, etc.).\n\nSharps tend to bet more underdogs than the public\n\nPublic bettors like to bet on overs or the expected favorite to win the game. As a result, sharp bettors often find value in betting unders or underdogs\nDue to betting markets becoming more efficient, the Unders/Underdogs value is dissipating. In major sports, the underdog is covering right around 50-51% of the time. As a result, finding value on Overs/Favorites is possible.\nSolution: Keep records of the types of bets you make with each book. If you’re betting more underdogs at a book, change your behavior which can be as simple as not betting there for a few days. Perhaps betting something in an unrelated sport. Or, when necessary, bet something without getting the best number in the market.\n\nSharps tend to bet in amounts that return round numbers (e.g. bet $110 to win $100)\n\nSolution: Toss in some weird bet amounts make you look like you have less of a system.\n\nSharps tend to bet straight bets only\n\nSolution: Mix in some parlays in positive expected value situations\n\nSharps tend to get good Closing Line Value",
    "crumbs": [
      "Sports Betting"
    ]
  },
  {
    "objectID": "qmd/sports-betting.html#sec-spbet-terms",
    "href": "qmd/sports-betting.html#sec-spbet-terms",
    "title": "Sports Betting",
    "section": "Terms",
    "text": "Terms\n\nClosing Line Value (CLV) - A measure of the value of a bet at the time it is placed compared to the closing odds. You can calculate it by comparing the odds you received when you placed your bet to the final odds when the event starts. If the odds you received were better than the closing odds, then the bet has positive CLV. If the odds you received were worse than the closing odds, then the bet has negative CLV. This is the formula used by the Unabated website.\n\\[\n\\text{CLV} = \\frac{p_{\\text{imp}}^{\\text{cl}} - p_{\\text{imp}}^{\\text{bet}}}{p_{\\text{imp}}^{\\text{bet}}}\n\\]\n\n\\(p_{\\text{imp}}^{\\text{cl}}\\): Implied probability of the closing line\n\\(p_{\\text{imp}}^{\\text{bet}}\\): Implied probability of your bet\n\nFirst Five - Betting an event will happen at the end of five innings of a baseball game\n\ne.g. Over 5.5 points, Braves leading, etc.\n\nImplied Probability - Is Risk/Reward. Used in betting to obtain a probability converted from a Moneyline odds\n\nFor a negative Moneyline Odds\n\\[\np_{\\text{imp}} = \\frac{\\text{Risk}}{\\text{Reward}} = \\frac{-\\text{Odds}_{\\text{mon}}}{-\\text{Odds}_{\\text{mon}} + 100}\n\\]\nFor a positive Moneline Odds\n\\[\np_{\\text{imp}} = \\frac{\\text{Risk}}{\\text{Reward}} = \\frac{100}{\\text{Odds}_{\\text{mon}} + 100}\n\\]\n\nMoneyline - A moneyline bet functions as a straight-up wager on which team will win a game. This kind of bet doesn’t involve a point spread.\n\nThe underdog on the moneyline pays out at greater odds than the favorite.\nSportsbooks generally display the moneyline, point spread, and total (Over/Under) odds for a game, but all three function as separate bets.\n\nOriginators - A sharp that generates their own odds — the quants of sports betting.\nPicking Off (aka Stake Factoring) - The limiting of the maximum wager or even banning of a sharp bettor by a bookmaker. This is done by the bookmaker to optimize profitability. Bettors can be flagged as unprofitable not only by how much or how often they win, but by betting actions determined to be predictive according to the algorithm used by the bookmaker.\nSharp Bettors (aka Sharps) - Informed, experienced, successful sports bettors. Someone who is consistently making positive Expected Value (EV) bets or bets that are expected to be profitable in the long term which is typically means winning about 55% to 60% of the time.\nTissue - A list of odds representing the chance of the various possible outcomes of a sporting event occurring.\nTotals - The Over/Under\nTout - An individual (handicapper) who sells picks often in exchange for an up-front payment. Touts either sell picks by themselves or are part of a service, which is composed of numerous touts, selling different kinds of packages.\n\nCan also refer to public handicappers in a public forum, such as in a digital or print newspaper, on social media or on broadcast outlets\nAlmost never a good idea to buy picks from a tout. Although, if they’re good and move lines, you can take advantage of the information to hedge. (See Is There Ever a Good Reason To Buy Sports Picks?)\n\nVigorish (aka Vig or Juice) - The house edge. Sports betting sites need to turn a profit, so they work a slight advantage into each bet.",
    "crumbs": [
      "Sports Betting"
    ]
  },
  {
    "objectID": "qmd/sports-betting.html#sec-spbet-calcclv",
    "href": "qmd/sports-betting.html#sec-spbet-calcclv",
    "title": "Sports Betting",
    "section": "Calculating CLV",
    "text": "Calculating CLV\n\nMisc\n\nNotes from\n\nWhat is Closing Line Value (CLV)?\nNo-Vig Fair Odds Calculator\nGetting Precise About Closing Line Value\n\nUsing CLV as a metric is only useful when the betting market is efficient. When the true price of the bet isn’t actually being discovered, any CLV you see is, at best, an informed guess. When the market is efficient, the closing line converted into probabilities is the most accurate prediction of a match up.\n\nThe efficient market hypothesis says that markets eventually reveal the true price of an asset by accounting for all relevant information.\nIn an efficient market, a negative ROI but a positive CLV says I’m on the right track.\nIf there aren’t enough bettors, in general, betting a particular sport or a particular sportsbook, then it will not be efficient\nIf there aren’t enough sharps betting a particular sport or particular sportsbook, then it will not be efficient\n\nExamples of efficient betting markets\n\nNFL\n\nExamples of inefficient betting markets\n\nNon-conference season in NCAA basketball (fewer books, low quality info about teams keeps sharps away)\nWNBA (low limits keep sharps away)\nNBA Player Props (low limits keep sharps away)\n\n\nSteps\n\nCalculate implied probabilities for moneyline odds you bet and the moneyline odds at close\nRemove the vig from the closing line implied probability\nCalculate the CLV\n\nExample: Calculate CLV\n\nWe bet the Tampa Bay Lightning at -195 (Moneyline) odds to beat the Kraken which are at +190. Right before puck drops, what’s our CLV if the line shifts to Tampa Bay -220?\nCalculate implied probabilities\n\\[\n\\begin{align}\np_{\\text{imp}}^{\\text{bet}} &= \\frac{-(-195)}{-(-195) + 100} = 0.661\\\\\np_{\\text{imp}}^{\\text{cl}} &= \\frac{-(-220)}{-(-220) + 100} = 0.688\\\\\n\\end{align}\n\\]\nCalculate the total implied probability\n\\[\n\\begin{align}\np_{\\text{imp}}^{\\text{opp}} &= \\frac{100}{190+100} =  0.345\\\\\np_{\\text{tot}} &= 0.661 + 0.345 = 1.006\n\\end{align}\n\\]\n\n\\(p_{\\text{imp}}^{\\text{opp}}\\) is the implied probability of the team you bet against (i.e. opponent).\n\nCalculate adjusted closing implied probability\n\\[\np_{\\text{adj}}^{\\text{cl}} = \\frac{0.688}{1.006} = 0.684\n\\]\nCalculate CLV\n\\[\n\\text{CLV} = \\frac{0.684 - 0.661}{0.661} = 0.035 \\;\\;\\text{or}\\;\\; 3.5\\%\n\\]\n\nExample: Calculate Fair Value Odds and Interpret\n\nCalculate adjusted implied probability for each team\n\\[\n\\begin{align}\n\\text{(Lightning)}\\: p_{\\text{adj}}^{\\text{}} &= \\frac{0.661}{1.006} = 0.657 \\\\\n\\text{(Krakens)}\\: p_{\\text{adj}}^{\\text{}} &= \\frac{0.345}{1.006} = 0.343\n\\end{align}\n\\]\nConvert to probabilities to Moneyline odds\n\\[\n\\begin{align}\n\\text{(Lightning/Favorite)}\\: \\text{Odds}_{\\text{mon}} &= -100 \\cdot \\frac{p_{\\text{adj}}}{1-p_{\\text{adj}}} = -100 \\cdot \\frac{0.657}{1-0.657} = -192\\\\\n\\text{(Krakens/Underdog)}\\: \\text{Odds}_{\\text{mon}} &= 100 \\cdot \\frac{1-p_{\\text{adj}}}{p_{\\text{adj}}} = 100 \\cdot \\frac{1-0.343}{0.343} = 192\n\\end{align}\n\\]\nThis means that any moneyline odds\n\nFor the favorite, with odds &gt; -192 (e.g. -191) has a positive Expected Value (EV)\nFor the underdog, odds &gt; 192 has a positive Expected Value (EV)",
    "crumbs": [
      "Sports Betting"
    ]
  },
  {
    "objectID": "qmd/sports-betting.html#sec-spbet-betstrat",
    "href": "qmd/sports-betting.html#sec-spbet-betstrat",
    "title": "Sports Betting",
    "section": "Betting Strategies",
    "text": "Betting Strategies\n\nRound Robin\n\nNotes from The Round Robin Bet: The Variance-Reducing Parlay Play\nParlay betting strategy\nProbably won’t reach the dexponential returns of a parlay, but it’s a solid workhorse than can help bettors reduce the substantial variance inherent in parlay betting while still keeping them alive in the hunt for a more significant payday.\n\nVariance in Parlay Betting\n\nParlays are hard to hit. In a five-leg parlay, there are 31 ways to lose and only one way to win.\nIf every leg of a 5-leg parlay had a 50% probability of hitting, you’d win 3.13 percent of the time — for 3-leg, 12.5% of the time.\nBetting round robins just reduces the variance while maintaining the edge for sportsbook. On the flip side, they’re good for the player because they reduce variance.\n\nDrawbacks\n\nLikely less of a payout than a regular parlay\nYou’re putting substantially more money on the line\n\nIf you want to keep to your original stake, you have to divide it by the number of combinations you’re betting.\n\n\n\nYou create combinations of a group of bets where the total number of combinations is:\n\\[\n_nC_r = \\frac{n!}{r!(n-r)!}\n\\]\n\n\\(n\\): The total number of bets\n\\(r\\): The number of bets in each combination\n\nChoosing the optimal number of bets for each combination\n\nIf you want to maximize plus-EV spots in a way that decreases fluctuations in your bankroll, choosing round robins with fewer combinations can be a viable path. (e.g. 2 at a time)\nNo matter how many legs per combination you choose, you’ll have to win the same proportion of bets to be profitable.\n\ne.g. For five bets, whether you use 2-leg or 3-leg parlays per combination, you still have to win 4 out 5 bets to be profitable. (See examples)\n\n\nExample: 3-Bet Round Robin by Twos (i.e. 3 bets chosen 2 at a time)\n\nBet Team A & Team B\nBet Team A & Team C\nBet Team B & Team C\n\nExample: 5 bets chosen 3 at time\n\nBets\n\nMinnesota Twins moneyline (+102)\nNew York Yankees moneyline (-158)\nDodgers-Cubs Over 11 (-110)\nRockies-Philles first 5 Over 5.5 (+106)\nLuke Weaver (CIN) Over 3.5 strikeouts (-126)\n\nTogether as a five-leg parlay, these bets would pay +2226. Bet $10 to win $222.67.\nTotal number of combinations\n\\[\n_5\\text{C}_3 = \\frac{5!}{3!(5-3)!} = \\frac{120}{6(2)} = 10\n\\]\nRound Robin (Bets, payout):\n\nTwins-Yankees-Cubs Over, which pays $52.97\nTwins-Yankees-Rockies Over, $57.95\nTwins-Yankees-Weaver, $49.16\nTwins-Cubs Over-Rockies Over, $69.44\nTwins-Cubs Over-Weaver, $59.17\nTwins-Rockies Over-Weaver, $64.64\nYankees-Cubs Over-Rockies Over, $54.22\nYankees-Cubs Over-Weaver, $45.91\nYankees-Rockies Over-Weaver, $50.33\nCubs Over-Rockies Over-Weaver, $60.54\n\nIf every leg comes in the max payout is $564.34. Hit four legs but not Weaver and that would pay $234.58.\n\nIf Weaver and the Rockies miss, you’d only get back $52.97 for the one combination that connected or you could get back as much as $69.44 if Yankees and Weaver don’t cover.\n\n\nExample: 5 bets chosen 2 at a time\n\nSame bets as previous example\nTotal number of combinations remains the same\nFor a straight 5 team parlay, assume payout of $253 for a $10 stake\n\nI googled the average payouts to get these numbers\n\nRound Robin (Bets, Assuming payout of $2.60 per $1 bet for each leg):\n\nTwins-Yankees, which pays $26.00\nTwins-Cubs Over\nTwins-Rockies Over\nTwins-Weaver\nYankees-Cubs Over\nYankees-Rockies Over\nYankees-Weaver\nCubs Over-Rockies Over\nCubs Over-Weaver\nRockies Over-Weaver\n\nIf every leg comes in, the max payout is $260\nIf one bet misses(e.g. Twins), the payout is $156.\nIf two bets miss (e.g. Twins, Yankees), the payout is $78 which is still a loss and is probably about $10 less than the 3-leg approach, but substantially less of a payout if you only miss either 0 or 1 bets.",
    "crumbs": [
      "Sports Betting"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Creative Commons Attribution-NonCommercial 4.0 International Public License",
    "section": "",
    "text": "Attribution-NonCommercial 4.0 International"
  },
  {
    "objectID": "LICENSE.html#attribution-noncommercial-4.0-international",
    "href": "LICENSE.html#attribution-noncommercial-4.0-international",
    "title": "Creative Commons Attribution-NonCommercial 4.0 International Public License",
    "section": "",
    "text": "Creative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n\nConsiderations for licensors: Our public licenses are intended for use by those authorized to give the public permission to use material in ways otherwise restricted by copyright and certain other rights. Our licenses are irrevocable. Licensors should read and understand the terms and conditions of the license they choose before applying it. Licensors should also secure all rights necessary before applying our licenses so that the public can reuse the material as expected. Licensors should clearly mark any material not subject to the license. This includes other CC-licensed material, or material used under an exception or limitation to copyright. More considerations for licensors.\nConsiderations for the public: By using one of our public licenses, a licensor grants the public permission to use the licensed material under specified terms and conditions. If the licensor’s permission is not necessary for any reason–for example, because of any applicable exception or limitation to copyright–then that use is not regulated by the license. Our licenses grant only permissions under copyright and certain other rights that a licensor has authority to grant. Use of the licensed material may still be restricted for other reasons, including because others have copyright or other rights in the material. A licensor may make special requests, such as asking that all changes be marked or described. Although not required by our licenses, you are encouraged to respect those requests where reasonable. More considerations for the public."
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-noncommercial-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-noncommercial-4.0-international-public-license",
    "title": "Creative Commons Attribution-NonCommercial 4.0 International Public License",
    "section": "Creative Commons Attribution-NonCommercial 4.0 International Public License",
    "text": "Creative Commons Attribution-NonCommercial 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-NonCommercial 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nNonCommercial means not primarily intended for or directed towards commercial advantage or monetary compensation. For purposes of this Public License, the exchange of the Licensed Material for other material subject to Copyright and Similar Rights by digital file-sharing or similar means is NonCommercial provided there is no payment of monetary compensation in connection with the exchange.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part, for NonCommercial purposes only; and\nB. produce, reproduce, and Share Adapted Material for NonCommercial purposes only.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties, including when the Licensed Material is used other than for NonCommercial purposes.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\nIf You Share Adapted Material You produce, the Adapter’s License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database for NonCommercial purposes only;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  }
]