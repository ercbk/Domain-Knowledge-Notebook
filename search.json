[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Domain Knowledge",
    "section": "",
    "text": "Preface\nThese notes were transferred from Evernote to Quarto, so some of notes may be difficult to read as the editing of the note format is still an ongoing process.\nNotes without numbers in sidebar should be a good indicator of a note that I’ve finished formatting.\nIf you see any mistakes or have any questions, please open an issue at the github repository."
  },
  {
    "objectID": "qmd/agriculture.html#sec-ag-misc",
    "href": "qmd/agriculture.html#sec-ag-misc",
    "title": "Agriculture",
    "section": "Misc",
    "text": "Misc\n\nCommodity prices normally forecast using a cobweb model which leads to price risk\nVegetable price series are much more volatile than other commodities\n\nSeasonality regulates the supply and demand\nPerishable nature of the produce adds complications in stabalizing the price"
  },
  {
    "objectID": "qmd/banking-credit.html#sec-bank-cred-misc",
    "href": "qmd/banking-credit.html#sec-bank-cred-misc",
    "title": "Banking/Credit",
    "section": "Misc",
    "text": "Misc\n\nLogistic Regression models typically with 8 to 10 predictors are common\n\n“To adopt a new algorithm, it not only had to outperform regression. The improvement also had to justify the effort of explaining the algorithm.”\n\nUsing SHAP, PDPs, etc. just adds more complexity because then you would have also needed to explain the method used to explain the model\n\n\nDS Use Cases\n\nCredit risk — predict default due to financial distress\nFraud — predict if customers do not intend to repay a loan\nPre-areas — identify customers in financial distress\nChurn — identify customers who intend to leave the bank\nMarketing — identify the best customers to promote a product to"
  },
  {
    "objectID": "qmd/banking-credit.html#sec-bank-cred-fraud",
    "href": "qmd/banking-credit.html#sec-bank-cred-fraud",
    "title": "Banking/Credit",
    "section": "Fraud",
    "text": "Fraud\n\nMisc\n\nFraud Score - Values that indicate how risky a user action is. Scoring determined by fraud rules.\nComputing ROI for a fraud model\n\nAssume the cost of fraud is the cost of the transaction\nCalculate the total cost of all the fraudulent transactions in the test dataset.\n\nCalculate the cost based on the model predictions.\n\nFalse Negatives: Observed frauds that weren’t predicted are assigned a cost equal to the value of the transaction.\nFalse Positives: Legitimate transactions that were marked as fraud are assigned $0 cost.\n\nThis likely isn’t true. There is the cost of having to deal with customers calling because the transaction was declined or the cost sending out texts for suspicious transactions, but this cost is very small relative to the cost of a fraudulent transaction.\nZhang, D. , Bhandari, B. and Black, D. (2020) Credit Card Fraud Detection Using Weighted Support Vector Machine. Applied Mathematics, 11, 1275-1291. doi: 10.4236/am.2020.1112087.\n\nOther costs can include deployment (e.g. DL model vs logistic regression)\nROI of the new model = costs of old model - cost of new model\nExample: article\n\n\nMetrics\n\nFN: Predicting “not fraud” for a transaction that is indeed fraud\n\nA false negative more costly than false positive\n\nRecall (aka sensitivity): Ratio of correct fraud predictions (TP) out of all fraud events (TP + FN)\nFN Rate: Ratio of fraud events  the model failed to predict out of all fraud events\n\nComplement of Recall, (FN/(TP+FN))\nMost expensive to organizations in terms of direct financial losses, resulting in chargebacks and other stolen funds\n\nFP Rate: Rate at which a model predicts fraud for a transaction that is not actually fraudulent\n\nFP / (FP + TN)\nMeasures the incovenience for the customer that the model inflicts\nSeems to be a metric to monitor by group to see if the model is ethically biased\n\n\nModel Monitoring\n\nFar more false positives in production than the validation baseline\n\nResults in legitimate purchases getting denied at the point of sale and annoyed customers.\n\nA dip in aggregate accuracy\n\nInvestigate prediction accuracy by group\nExample: The fraud model isn’t as good at predicting smaller transactions relative to the big-ticket purchases that predominated in the training data\n\nFeature performance heatmaps can be the difference between patching costly model exploits in hours versus several days\nScenario Examples\n\nPrediction Drift\n\nPossible Drift Correlation: An influx and surge of fraud predictions could mean that your model is under attack! You are classifying a lot more fraud than what you expect to see in production, but (so far) your model is doing a good job of catching this. Let’s hope it stays that way.\nReal-World Scenario: A hack of a health provider leads to a surge of identity theft and credit card numbers sold on the dark web. Luckily, the criminals aren’t novel enough in their exploits to avoid getting caught by existing fraud models.\n\nActuals Drift (No Prediction Drift)\n\nPossible Drift Correlation: An influx of fraud actuals without changes to the distribution of your predictions means that fraudsters found an exploit in your model and that they’re getting away with it. Troubleshoot and fix your model ASAP to avoid any more costly chargebacks.\nReal-World Scenario: A global crime ring sends unemployment fraud to all-time highs using new tactics with prepaid debit cards, causing a dip in performance for fraud models trained on pre-COVID or more conventional credit card data.\n\nFeature Drift\n\nPossible Drift Correlation: An influx of new and/or existing feature values could be an indicator of seasonal changes (tax or holiday season) or in the worst case be correlated with a fraud exploitation; use drift over time stacked on top of your performance metric over time graph to validate whether any correlation exists.\nReal-World Scenario: An earlier holiday shopping season than normal takes hold, with bigger ticket purchases than prior years. It might be a sign of record retail demand and changing consumer behavior or a novel fraud exploitation (or both)."
  },
  {
    "objectID": "qmd/budget-allocation.html#sec-budallo-misc",
    "href": "qmd/budget-allocation.html#sec-budallo-misc",
    "title": "Budget Allocation",
    "section": "Misc",
    "text": "Misc"
  },
  {
    "objectID": "qmd/budget-allocation.html#sec-budallo-perform-cuts",
    "href": "qmd/budget-allocation.html#sec-budallo-perform-cuts",
    "title": "Budget Allocation",
    "section": "Performance-based Budget Cuts",
    "text": "Performance-based Budget Cuts\n\nNotes from: The Politics of Performance Measurement - Scenario: “Criminal Justice Division (CJD) of the Texas Governor’s Office received news all government agencies dread: budgets were to be cut. CJD oversaw a grant program that funded specialty courts throughout the state, however it was now being told that the program’s budget of $10.6m would be reduced 20% to $8.5m by 2018.”\n\nHow should these cuts be distributed among grant holders?\nGoal: Develop a data collection and performance assessment process to allocate budget cuts in a manner widely accepted\n\nOptions\n\nCut across the board. The Advisory Council would employ the same scoring method as the previous year but reduce each grant amount by 20%.\n\nThis option would leave long-running grantees scrambling to make up for this shortfall by reducing services, laying off staff, or spending more of their limited local funds. Worse, it would punish all grantees equally — our most successful programs would be arbitrarily defunded.\n\nFewer grants. Grants were scored based on the quality of their application and all grants that passed a certain threshold got funded. The Advisory Council would employ the same scoring method as the previous year but instead of funding the top $10.6m worth of grants, they would fund the top $8.5m worth.\n\nThis seemed a less bad option than cutting across the board, but we would still run into the problem of arbitrarily defunding successful programs. Grants near the bottom of the Advisory Council’s cutoff that got funded the previous year would be denied renewal only because the goalposts had moved.\n\n\n\n\nTargeted funding. The Advisory Council would incorporate performance data and statewide strategic plan alignment into their scoring method and make cuts accordingly.\n\nEngage stakeholders and define performance\n\nConvene a strategy session with the stakeholders to discuss how to proceed as part of a broader strategic plan\n\nAchieve consensus on high-level goals (e.g. fund strategically, focus on success, build capacity)\nLarger plan agreed upon that would also include: capacity building, training and technical assistance, helping courts obtain non-CJD sources of funding, and steering grantees toward established best practice."
  },
  {
    "objectID": "qmd/economics.html#sec-econ-misc",
    "href": "qmd/economics.html#sec-econ-misc",
    "title": "Economics",
    "section": "Misc",
    "text": "Misc\n\nDynamic stochastic general equilibrium (DSGE) models, which are popular in macroeconomic modeling, are garbage (article)\n\nEven under extremely ideal conditions they don’t retrieve the parameters and using them to forecast is no better than chance."
  },
  {
    "objectID": "qmd/economics.html#sec-econ-terms",
    "href": "qmd/economics.html#sec-econ-terms",
    "title": "Economics",
    "section": "Terms",
    "text": "Terms\n\nAdverse Selection - a market situation where buyers and sellers have different information. The result is that participants with key information might participate selectively in trades at the expense of other parties who do not have the same information\n\ne.g. A person waits until he knows he is sick and in need of health care before applying for a health insurance policy. The buyer has more knowledge (i.e., about their health). To fight adverse selection, insurance companies reduce exposure to large claims by limiting coverage or raising premiums\n\nIntertemporal Price Discrimination - charging a high price initially, then lowering price after time passes.\n\nA method for firms to separate consumer groups based on willingness to pay\ne.g. last minute travel bookings (opposite direction since last minute bookings usually cost more)\n\nPrice Elasticity of Demand (PED)- the percent change in demand given the percent change in price assuming that everything else doesn’t change\n\nHow sensitive the quantity demanded is to its price. When the price rises, quantity demanded falls for almost any good, but it falls more for some than for others.\nThe elasticity of a good or service can vary according to the number of close substitutes available, its relative cost, and the amount of time that has elapsed since the price change occurred.\nWhen the price of a good or service has reached the [point of elasticity, sellers and buyers quickly adjust their demand for that good or service.\nAn inelastic product is one that consumers continue to purchase even after a change in price\nProducts or services that are elastic are either unnecessary or can be easily replaced with a substitute.\n\nSecond Degree Price Discrimination - charging a different price for different quantities at the same time"
  },
  {
    "objectID": "qmd/economics.html#sec-econ-pricelas",
    "href": "qmd/economics.html#sec-econ-pricelas",
    "title": "Economics",
    "section": "Price Elasticity",
    "text": "Price Elasticity\n\nBy identifying the price elasticity of demand, you can try to determine the amount of price you can increase without hurting the demand, as well as check at what point an increase in price starts to affect the market.\nPrice is NOT the only variable that influences whether you purchase a product or service. Therefore, looking at quantity purchased at each price to determine price elasticity is not enough.\nGuidelines\n\nIf the PED is greater than one (PED &gt; 1), it is known as “elastic”, meaning changes in price causes a significant change in demand.\nIf the PED is equal to 1 (PED = 1), then this means any change in price causes equivalent changes in demand.\nIf the PED is less than one (PED &lt; 1), it is known as “inelastic”. This means changes in price don’t affect the demand that much.\nIf the PED is equal to 0 (PED = 0), known as “perfectly inelastic”, meaning any change in price doesn’t cause a change in demand.\n\nProcess\n\nFilter on the specific subset of sales data relevant to the dimension which you are estimating elasticity (e.g. if estimating the price elasticity for red wine, filter on only red wine sales)\nPerform a log transformation on the future sales target variable and on the current price feature\nTrain a multivariable linear regression model to accurately predict future sales\nThe price elasticity estimate will be the coefficient of the log transformed, price feature\nRepeat steps 1–4 for each elasticity estimate"
  },
  {
    "objectID": "qmd/environment.html#sec-environ-misc",
    "href": "qmd/environment.html#sec-environ-misc",
    "title": "Environment",
    "section": "Misc",
    "text": "Misc"
  },
  {
    "objectID": "qmd/environment.html#sec-environ-metrics",
    "href": "qmd/environment.html#sec-environ-metrics",
    "title": "Environment",
    "section": "Metrics",
    "text": "Metrics\n\nExisting Tree Canopy: The amount of urban tree canopy present when viewed from above using aerial or satellite imagery.\n\nETC % = tree canopy / land area\n\nPossible Tree Canopy - Vegetated: Grass or shrub area that is theoretically available for the establishment of tree canopy.\n\ne.g. residential areas\n\nPossible Tree Canopy - Impervious: Asphalt, concrete or bare soil surfaces, excluding roads and buildings, that are theoretically available for the establishment of tree canopy without having to remove paved surfaces\n\ne.g. any areas with no trees, buildings, roads, or bodies of water\nPossible-Vegetation category should serve as a guide for further analysis, not a prescription of where to plant trees since other factors, such as land use, social, and financial (e.g. golf courses, agricultural and recreational fields), are involved.\n\nNot Suitable: Areas where it is highly unlikely that new tree canopy could be established (primarily buildings and roads).\nRelative tree canopy change - change of tree canopy over a period of time\n\ne.g (for 1 hexagon) relative tree canopy change % = (tree_canopy_area_2019 - tree_canopy_area_2012) / tree_canopy_area_2012 - Acre gain per \n\nCanopy height - proxy for tree age\n\nSteps\n\nSegment tree canopy into polygons approximating individual trees\nAttribute each polygon with a height from both the starting date to end date (e.g. 2012 and 2019 ) LiDAR data\n\nInterpretation example\n\nTrees in the 0-60 foot height class experienced gain, while there was minimal gain in the other taller height classes.\n\nTherefore, many new trees planted and canopy expanding on existing trees.\nDiverse height structure corresponds to a healthy and diverse tree age distribution\nVery mature trees in the 130 height class points to the height potential for certain tree species provided the right conditions"
  },
  {
    "objectID": "qmd/environment.html#sec-environ-canopy",
    "href": "qmd/environment.html#sec-environ-canopy",
    "title": "Environment",
    "section": "Canopy Assessment",
    "text": "Canopy Assessment\n\nNotes from Lousiville Tree Canopy Assessment 2012-2019\nTree benefit: reducing stormwater runoff near streets and decreasing the urban heat island effect\nAbove surface factors such as sidewalks to utilities can affect the suitability of a site for tree planting.\nImportant to preserve trees in the 10-50 foot height range, so they can grow into the 60+ foot range while planting a variety of new trees to continue the lifecycle\nLosses are generally easier to detect than gains as losses tend to be due to a large event, such as tree removal, whereas gains are incremental growth or new tree plantings, both of which are smallerin size\nFactors that can affect change in tree canopy\n\nNatural\n\nInvasive species\nNatural disasters such as storms\nClimate change may cause trees to grow more quickly but could also result in inhospitable conditions for native species\n\nAnthropogenic\n\nPreservation and conservation efforts, the strength of tree ordinances, and the impacts of new development\nTree removal due to homeowner preferences and not being replaced by new trees\nProximity to roads: Regular salting, compaction, limited space, clearance pruning, and plow collisions\n\n\nData sources\n\nLiDAR\n\nFeatures distinguished by their spectral (color) properties\nTrees and shrubs can appear spectrally similar or obscured by shadow, LiDAR, which consists of 3D height information enhances the accuracy of the mapping\nResolution of 30-meters\n“LiDAR datasets were acquired under leaf-off conditions and thus tend to underestimate tree canopy slightly” (i.e. Fall or Winter?)\nLiDAR and imagery datasets are not directly comparable due to differences in the sensor, time of acquisition, and processing techniques employed.\nResources:\n\nPaper summarizing their tree canapy mapping approach\nGetting to Land Use/Land Cover\nThreshold Classification in eCognition\nObject-based approach to LiDAR\n\n\n\n%  using 500-acre hexagons\n\nUse LiDAR Hill shade map with % canopy change to highlight local areas\n\nLand Use Categories\n\nOverall: residential, commercial, and recreational\nMetric change by category | acres lost (orange)/gained (green) by category\n\n\nChange per Council District (by metric)"
  },
  {
    "objectID": "qmd/epidemiology.html#sec-epidemi-misc",
    "href": "qmd/epidemiology.html#sec-epidemi-misc",
    "title": "Epidemiology",
    "section": "Misc",
    "text": "Misc"
  },
  {
    "objectID": "qmd/epidemiology.html#sec-epidemi-dismap",
    "href": "qmd/epidemiology.html#sec-epidemi-dismap",
    "title": "Epidemiology",
    "section": "Disease Mapping",
    "text": "Disease Mapping\n\nGoals\n\nProvide accurate estimates of mortality/incidence risks or rates in space and time\nUnveil underlying spatial and spatio-temporal patterns\nDetect high-risk areas or hotspots\n\nRisk estimation using metrics such as Standardized Mortality Ratio (SMR) when analyzing rare diseases or low-populated areas are highly variable over time, so it’s diffficult to spot patterns and form hypotheses\n\nSMR = Observed number of cases / Expected number of cases\n\nSMR &gt; 1: risk is greater than the whole region under study\nGuessing “Expected number of cases” is the average number of cases for the whole study region\n\n\nStatistical models smooth risks by borrowing information from spatio-temporal neighbors\n\nThe smoothed gradient over the entire study region makes it easier to detect patterns and form hypotheses than highly variable, local area metric estimates (e.g. SMR in a low populated county)\n\nTraditional Models\n\nTypes\n\nMixed Poisson with conditional autoregressive (CAR) priors for “space” and random walk priors for “time” that include space ⨯ time interactions (Knorr-Held, 2000, Bayesian modeling of inseperable space-time variation in disease risk)\nReduced rank multidimensional P-splines (Ugarte et al, 2017, One-dimensional, two-dimensional, and three dimensional B-splines to specify space-time interactions in Bayesian disease mapping)\n\nIssues\n\nEstimating the cov-var matrix becomes intractable with big data and many areas since the covariance must be estimated between each pair of areas\nCAR models assume the same level of spatial dependence between all areas which isn’t likely.\n\n\n{bigDM}\n\nScalable non-stationary Bayesian models for high-dim, count data\nDependencies\n\nUses {future} for distributed computing\nIntegrated, nested laplace approximation (INLA) method through {R-INLA}\n\nK-order neighborhood model\n\nBreaks up local spatial or spatio-temporal domains so that estimations can distributed and local area dependencies (neighborhoods) can be accounted for.\n“Areas” are usually districts, counties, provinces, etc.\n\nPackage does provide a method to create a “random” area grid\n\nMight be useful to compare a random grid model with the e.g. county model to see how much county boundaries influence the estimates\n\n\nEach local area model includes k adjacent areas which creates a partition\n\nThe local area estimate is smoothed by taking information from the adjacent areas\nAdjacent areas also have estimate posteriors computed\nEach area will have multiple posterior estimates from local area models where the area is the local area or where it is the adjacent area\n\nMerge or don’t merge estimate posteriors for each area\n\nMerge: use weights proportional to the conditional predictive ordinates (CPO) ???\nDon’t Merge: Use the posterior marginal risk estimates of an area corresponding to the original submodel.\n\ni.e. use the posterior where the area is the “local area” in that local area model and not an adjacent area.\n\nPrimary functions\n\nCAR.INLA() fits several spatial CAR models for high dim count data\nSTCAR.INLA() fits several spatio-temporal CAR models for high dim count data"
  },
  {
    "objectID": "qmd/finance.html#sec-finance-misc",
    "href": "qmd/finance.html#sec-finance-misc",
    "title": "Finance",
    "section": "Misc",
    "text": "Misc\n\nOpenBB - Open source bloomberg terminal (overview article)\n\nFree data and seemingly hundreds of different tools including some decent forecast methods.\n\nBeware of survivorship bias when backtesting algorithms\n\nExample: S&P 500\n\nA trader is creating an algorithm to predict prices of all the stocks in the S&P 500. If the trader uses the current roster of companies, the list only includes companies that have made it to now without shutting down or losing so much value they drop off the list. The trader should use the list of companies as it was at the start of the training data.\n\n\n435 choices for start and end dates of each monthly investment cycle\n\ni.e. Easy to have a selective endpoints fallacy for an investment strategy.\n\nStrategies\n\nHedgefund - factor rotations (longer term)\nSomething about futures - factor tilts (shorter term)\n\nResources - Advances in Financial Machine Learning - de Prado (R &gt;&gt; Documents &gt;&gt; Financial)\nMisleading chart (Thread)\n\n\n\nNo transaction costs\nNo fees\nNo taxes\nUS “exceptionalism”\nOverlapping time period analysis\n\nExample: if I look at 1920-1950 and then 1921-1951, twenty eight of the thirty data points are overlapping. So there’s not a lot of “unique” data in this set.\n\nNominal vs real returns - Time weighted vs dollar weighted returns"
  },
  {
    "objectID": "qmd/finance.html#sec-finance-terms",
    "href": "qmd/finance.html#sec-finance-terms",
    "title": "Finance",
    "section": "Terms",
    "text": "Terms\n\nBasis Point - 1/100th of a percentage point\nBasis Risk - the risk that an asset and a hedge will not move in opposite directions as expected; “basis” refers to the discrepancy.\nCapital expenditure (CapEx) - money that is spent to acquire, repair, update, or improve a fixed company asset, such as a building, business, or equipment. For assets to fall under the CapEx destination, the investments must have a useful life of one year or more. A CapEx is amortized, or its value is deducted a little each year based on the total cost and its expected useful life.\n\nUseful life refers to the estimated and generally agreed upon shelf life of a specific business asset.\n\nAccording the IRS, Car’s useful life is 5yrs and new building’s is 39yrs\n\nAlso see What Is a Capital Expenditure (CapEx)? Definition and Guide (example, calculations, relation to operating expenditure (OpEx)\n\nCost of Carry or Carrying Charge - the cost of holding a security or a physical commodity over a period of time. The carrying charge includes insurance, storage and interest on the invested funds as well as other incidental costs\n\nFor a stock, it’s is the opportunity cost of the capital that goes into it plus the risk you take on for holding it (this is what the idea of risk neutral valuation is based on).\n\nDerivatives are securities that move in correspondence to one or more underlying assets. They include options, swaps, futures and forward contracts. The underlying assets can be stocks, bonds, commodities, currencies, indices or interest rates. Derivatives can be effective hedges against their underlying assets, since the relationship between the two is more or less clearly defined (if they’re negatively correlated? Or maybe if the underlying asset goes down, there’s a lag between the asset going down and the derivative going down. Therefore, you can sell the derivative before it goes down. Thus, hedging your risk). Knowing the value of an underlying asset helps traders determine the appropriate action (buy, sell, or hold) with their derivative.\nExchange Traded Fund (ETF) - A mutual fund that may be traded daily like a stock or bond.\nFalse Strategy Theorem - Gives the threshold for which a Sharpe Ratio would be significant.\n\nGiven a sample of estimated performance statistics (e.g. sharpe ratios), {Sk} for k = 1, …, K, where each S ∈ N(0, 1) \\[\n        \\mathbb{E}[\\max_{k} {S_k}] \\approx (1-\\gamma) Z^{-1} [1-\\frac{1}{K}] + \\gamma Z^{-1} {1 - \\frac{1}{Ke}}\n        \\]\n\nZ-1 is the inverse of the standard Gaussian cdf\ne is the exponential constant (i.e. 2.71…)\nγ is the Euler-Mascheroni constant (approx. 0.5772156649…)\n\nUseful for backtesting multiple strategies and deciding whether the strategy with the maximum sharpe ratio is significant (mitigates multiple testing bias)\n\nFutures are an obligation to the buyer and a seller. The seller of the future agrees to provide the underlying asset at expiry, and the buyer of the contract agrees to buy the underlying at expiry. The price they receive and pay, respectively, is the price they entered the futures contract at. Most futures traders close out their positions prior to expiration since retail traders and hedge funds have little need to take physical possession of barrels of oil, for example. But, they can buy or sell the contract at one price, and if it moves favorably they can exit the trade and make a profit that way. Futures are a derivative because the price of an oil futures contract is based on the price movement of oil, for example.\nHedge - an investment that is made with the intention of reducing the risk of adverse price movements in an asset. Normally, a hedge consists of taking an offsetting or opposite position in a related security. An example could be investing in both cyclical and counter-cyclical stocks.\nHedge Ratio (delta) - The effectiveness of a derivative hedge, delta, is the amount the price of a derivative moves per $1 movement in the price of the underlying asset.\nLeg - one part or one side of a multistep trade. Legs should be exercised at the same time in order to avoid any risks associated with fluctuations in the price of the related security. So a purchase and sale should be made around the same time to avoid any price risk. Strategy often associated with derivatives trading.\nLimited Liability Corporation (LLC) - A corporation is a business organization that issues stock to its shareholders. A limited liability company is a business organization composed of members with membership interests. a type of legal entity that can be used when forming a business that offers protection to the owner(s) from personal liability for debts and other obligations that a business might incur. In other words, the personal assets of the owner cannot be used for legal claims against the business.The differences don’t really matter much at the taxation or day-to-day corporate level except in scale: LLCs tend to be smaller than corporations (more or less; a lot of people form small business corporations for good reasons and ignorant ones). (See Differences between a LLC and S Corp)\nMarket Capitalization - the total dollar market value of a company’s outstanding shares. Calculated by multiplying the total number of a company’s shares by the current market price of one share.\nMarket Momentum - measure of overall market sentiment that can support buying and selling with and against market trends. In general, Momentum = Today’s price - Price from X days ago. Positive: bullish, Negative: bearish. More sophisticated indicators can be calculated, see https://www.investopedia.com/terms/m/marketmomentum.asp#:~\nOptions - an option on stock XYZ gives the holder the right to buy or sell XYZ at the strike price up until expiration. The underlying asset for the option is the stock of XYZ. The writer must either buy or sell the underlying asset to the buyer on the specified date at the agreed-upon price. The buyer is not obligated to purchase the underlying asset, but they can exercise their right if they choose to do so. If the option is about to expire, and the underlying asset has not moved favorably enough to make exercising the option worthwhile, the buyer can let the expire and they will lose the amount they paid for the option.\n\nPut option - if Morty buys 100 shares of Stock plc (STOCK) at $10 per share, he might hedge his investment by buying an American put option with a strike price of $8 expiring in one year. This option gives Morty the right to sell 100 shares of STOCK for $8 any time in the next year. Let’s assume he pays $1 for the option, or $100 in premium. If one year later STOCK is trading at $12, Morty will not exercise the option and will be out $100. He’s unlikely to fret, though, since his unrealized gain is $100 ($100 including the price of the put). If STOCK is trading at $0, on the other hand, Morty will exercise the option and sell his shares for $8, for a loss of $300 ($300 including the price of the put). Without the option, he stood to lose his entire investment.\nCall options - contract giving the owner the right, but not the obligation, to buy a specified amount of an underlying security at a specified price within a specified time. The specified price is known as the strike price and the specified time during which a sale is made is its expiration (expiry) or time to maturity. As the price of the stock goes up, the value of the call option contract goes up. The contract can be sold at any time or you can purchase the stock at the guaranteed price on the expiration date. The price of the call option is called the premium.\n\nIf Apple is trading at $110 at expiry (aka expiration date), the strike price is $100, and the options cost the buyer $2, the profit is $110 - ($100 +$2) = $8. If the buyer bought one contract that equates to $800 ($8 x 100 shares), or $1,600 if they bought two contracts ($8 x 200). If at expiry Apple is below $100, then the option buyer loses $200 ($2 x 100 shares) for each contract they bought.\nSuppose that Microsoft shares are trading at $108 per share. You own 100 shares of the stock and want to generate an income above and beyond the stock’s dividend. You also believe that shares are unlikely to rise above $115.00 per share over the next month. You take a look at the call options for the following month and see that there’s a 115.00 call trading at $0.37 per contract. So, you sell one call option and collect the $37 premium ($0.37 x 100 shares), representing a roughly four percent annualized income. If the stock rises above $115.00, the option buyer will exercise the option and you will have to deliver the 100 shares of stock at $115.00 per share. You still generated a profit of $7.00 per share, but you will have missed out on any upside above $115.00. If the stock doesn’t rise above $115.00, you keep the shares and the $37 in premium income.\n\n\nOutstanding Shares - the number of stocks that a company has issued. This number represents all the shares that can be bought and sold by the public, as well as all the restricted shares that require special permission before being transacted.\nRelative Performance - the price ratio of two stocks\nSharpe Ratio (annualized) - measures the performance of an asset relative to violitility (i.e. riskiness)\n\n(expected excess returns relative to a risk free asset (e.g. treasury bond) / sd of those expected excess returns) * √number_of_observations_in_a_year\nSharpe ratios above 1.0 are generally considered “good,” as this would suggest that the portfolio is offering excess returns relative to its volatility\n\nEven if your sharpe ratio is above 1 it may not be good if it is below the average sharpe ratio of peer group portfolios.\n\nMultiplying by √number_of_observations_in_a_year makes “annualizes” the sharpe ratio and makes sharpe ratios comparable\nShould NOT be thought of as t-stats for testing significance of the sample mean (i.e. p-values for estimates) since it doesn’t account for the number of observations\n\nSee Sharpe Ratio (deflated)\nInvestment professionals often use a rule of thumb of dividing the sharpe ratio by 2 when backtesting to avoid overfitting, there is no statistical basis for this.\n\n\nSharpe Ratio (probabilistic) - allows you test the significance of the Sharpe Ratio under assumptions of ergodicity and stationarity (Paper)\nSharpe Ratio (deflated) - the probability that an observed Sharpe Ratio was drawn from a distribution with positive mean after controlling for sample size (aka backtest length), skewness, kurtosis, and number of strategy variations explored.\n\nCombines probabilistic sharpe ratio and false strategy theorem\nPaper: The Deflated Sharpe Ratio: Correcting for selection bias, backtest overfitting and non-normality\n\n\nShows that if a strategy has a maximum sharpe ratio of 1 but had 3 variations backtested, it’s deflated sharpe ratio drops below the 95% CI for a sharpe ratio = 1.\n\nReturns from investment strategies often exhibit autocorrelation, fat tails, and negative skewness which further “deflates” the deflated sharpe ratio\n\nSpread (Bid-Ask): the difference between two prices, rates or yields. - the gap between the bid and the ask prices of a security or asset, like a stock, bond or commodity. - the gap between a short position (that is, selling) in one futures contract or currency and a long position (that is, buying) in another. This is officially known as a spread trade\nVolatility:\n\nParkinson Range (PR) = ln(closing_price) - ln(opening_price)\n\nYield a return measure for an investment over a set period of time, expressed as a percentage.\n\nIncludes price increases as well as any dividends paid, calculated as the net realized return divided by the principal amount (i.e. amount invested).\nHigher yields are perceived to be an indicator of lower risk and higher income, but a high yield may not always be a positive, such as the case of a rising dividend yield due to a falling stock price."
  },
  {
    "objectID": "qmd/finance.html#sec-finance-6040",
    "href": "qmd/finance.html#sec-finance-6040",
    "title": "Finance",
    "section": "60/40 Portfolio Strategy",
    "text": "60/40 Portfolio Strategy\n\nMisc\n\n{{QSTrader}}\nNotes from - The 60/40 Benchmark Portfolio\n\nDescription\n60/40 US Equities/Bonds strategy is a simple long-term investment approach that is widely utilised in the investment industry. It seeks to ensure that at any point during the lifetime of the investment that 60% of account equity is invested in one or more assets representing a broad selection of US equities (such as an S&P500 ETF), while 40% of account equity is invested in one or more assets representing a broad selection of US treasury bonds (such as a treasury bond ETF).\nSince the actual percentage allocations of each asset class can deviate over time due to relative growth of the respective assets a ‘rebalance’ approach is often carried out. This means that trades are issued on a relatively infrequent basis to buy/sell amounts of each asset class to periodically bring the account equity allocations back into the 60/40 split. For the particular strategy implemented here we are using an end of month rebalance frequency."
  },
  {
    "objectID": "qmd/finance.html#sec-finance--momassallo",
    "href": "qmd/finance.html#sec-finance--momassallo",
    "title": "Finance",
    "section": "Momentum Tactical Asset Allocation Strategy",
    "text": "Momentum Tactical Asset Allocation Strategy\n\nMisc\n\n{{QSTrader}}\nNotes from\n\nSystematic Tactical Asset Allocation: An Introduction - Tutorial\n\n\nDescription\n\nThe US sector momentum strategy is a long-only dynamic tactical asset allocation strategy that attempts to exceed the performance of simply going long the S&P500.\nAt the end of every month the strategy calculates the holding period return (HPR) based momentum of all of the SPDR sector ETFs (the ticker symbols of which begin with the prefix XL) and selects the top N to invest in for the forthcoming month, where N is usually between 3 and 6.\nIn the implementation given here the HPR momentum is calculated over the previous 126 days (approximately six months of business days) and the top three sector ETFs are chosen for the portfolio. The portfolio allocation is equally weighted between each of these three sector ETFs. Irrespective of changes in signal the portfolio is rebalanced once per month to weight the assets equally."
  },
  {
    "objectID": "qmd/finance.html#sec-finance-meanrev",
    "href": "qmd/finance.html#sec-finance-meanrev",
    "title": "Finance",
    "section": "Mean Reversion Strategy or Pairs Trading",
    "text": "Mean Reversion Strategy or Pairs Trading\n\nMatch two trading vehicles that are highly correlated, trading one long and the other short when the pair’s price ratio diverges “x” number of standard deviations - “x” is optimized using historical data. If the pair reverts to its mean trend, a profit is made on one or both of the positions.\nChart relative performance (price ratio)\n\nThe center white line represents the mean price ratio over the past two years. The yellow and red lines represent one and two standard deviations from the mean ratio, respectively.\nThe potential for profit can be identified when the price ratio hits its first or second deviation. When these profitable divergences occur it is time to take a long position in the underperformer and a short position in the overachiever. (i.e. shorting the stock that’s rising in relation to the other, and buying the other stock that hasn’t risen yet)\nThe revenue from the short sale can help cover the cost of the long position, making the pairs trade inexpensive to put on.\nPosition size of the pair should be matched by dollar value rather than number of shares; this way a 5% move in one equals a 5% move in the other.\nAs with all investments, there is a risk that the trades could move into the red, so it is important to determine optimized stop-loss points before implementing the pairs trade.\n\nChart spreads between returns\nLook for stocks that move closely together and their:\n\nSpreads zig-zag around zero\nPrices separately (marginally) are normally distributed and joint-normally distributed (assumes linear correlation)\n\nIf both, then they move up AND down together (symmetric)\nIf only marginally, then they probably only move in one direction together (asymmetric)\n\n\nIf your two assets returns (differenced prices are stationary) do not correlate within a [0.7..0.8] correlation coefficient range at least 70% of the times (70% of all observations) then you’re probably dealing with a very bad hedging instrument\n\nLook to cover the remaining data points and research why correlations broke down during those periods. If you can derive a mathematical relationship you could possibly formulate an approach in which you make adjustments to the hedge ratio through an adjustment in your beta/correlation coefficient.\nFigure out the optimal time interval to retrain model due to data drift\nRolling 20-day correlation is a common way to monitor stock correlation\n\nCopulas\n\nSee Association, Copulas\nMeasures non-linear association\nFor Value-at-Risk (VAR) calculations, Gaussian copula is overly optimistic and Gumbel is too pessimistic\nCopulas with upper tail dependence: Gumbel, Joe, N13, N14, Student-t.\nCopulas with lower tail dependence: Clayton, N14 (weaker than upper tail), Student-t.\nCopulas with no tail dependence: Gaussian, Frank.\n\nCointegration allows us to construct a stationary time series from two asset price series, if only we can find the magic weight, or more formally, the cointegration coefficient β. Then we can apply a mean-reversion strategy to trade both assets at the same time weighted by β. There is no guarantee that such β always exists, and you should look for other asset pairs if no such β can be found.\n\nSee Forecasting, Statistical &gt;&gt; Terms\nCointegrated assets share common nonstationary components, which may include trend, seasonal, and stochastic parts\nMight have low correlation, and highly correlated series might not be cointegrated at all.\nDescribes a long-term relationship between the prices (correlation describes a short-term relationship between the returns). The resulting stationary series is the spread between the prices of both assets\nShould have similar risk exposure so that their prices move together\nGood candidates for cointegrated pairs could be:\n\nStocks that belong to the same sector.\nWTI crude oil and Brent crude oil.\nAUD/USD and NZD/USD.\nYield curves and futures calendar spreads"
  },
  {
    "objectID": "qmd/healthcare.html#sec-health-misc",
    "href": "qmd/healthcare.html#sec-health-misc",
    "title": "Healthcare",
    "section": "Misc",
    "text": "Misc\n\nNotes from\n\nPaper: Scalable and accurate deep learning with electronic health records by Google AI’s team\nHow to Encode Medical Records for Deep Learning\n\nMakes data suitable for a RNN (see Processing for details)\n\n\nAlso see\n\nEpidemiology &gt;&gt; Disease Mapping\n\nBias in Healthcare Claims Data (article, (mini) paper)\n\nImbalance of patients or members represented in large healthcare datasets can make your results non-generalizable or (in some cases) flat out invalid.\n\nGoals should be: investigate the biases, mitigate as well as possible, and decide which insights are still valuable or meaningful despite these nuances\n\nCharacteristics of data that lead to biases\n\n\nOnly includes any members/patients who actually had an incident/event for which the insurer processed a subsequent claim\n\nTends to over/underrepresent certain groups who are more likely to have chronic health problems, adverse social determinants of health, seek care,\n\nGroups are reflective of the type of demographics your organization tends to serve or that you have a larger book of business in\n\n\nCan cause over-and-under-representation of certain diseases\n\nEven companies within the same insurance sector may not have similar populations. Company business practices affect the types of populations within their data.\n\n\nOur [patients / members / employees / residents / etc.] may not act similarly or even represent [all patients / other groups / other types of employer s / other regions / etc.]\n\n\nData is years old. Effects sizes in data that’s 5 or 6yrs old may not be valid now.\n\n\nTypes\n\n\nUndercoverage bias - occurs when a part of the population is excluded from your sample.\n\nHistorical bias - occurs when socio-cultural prejudices and beliefs are mirrored into systematic processes [that are then reflected in the data].\n\nSystemic biases - result from institutions operating in ways that disadvantage certain groups.\n\n\nSolutions\n\n\nAugment or compare with outside data:  data sharing/collaboration or tapping into additional feeds such as health information exchange (HIE)\n\nSocial Determinants of Health (SDOH) - (see definition below) compare socio-economics and demographics of your dataset with  Census estimates and the SDOH variability seen for said zip codes.\n\n\nWill determine the amount of reweighting/normalizing the data requires\n\n\n\nPopulations within Healthcare Claims Data\n\n\nLines of business (LOB) such as patients with coverage from a government payer (Medicaid, Medicare); commercial lines (employer groups, retail or purchased via the exchange); self-insured (self-funded groups, usually large employers paying for their own healthcare claims of employees), etc.\n\nSub-lines of business, or groups. For instance, Medicaid may consist of multiple sub-groups representing different levels of eligibility, coverage, benefits, or types of people/why they qualified for the program (Temporary Assistance for Needy Families (TANF) vs. Medicaid expansion for adults)\n\nDemographics (certain groups, males or females, certain regions, etc.)\n\nConditions (examining certain chronic conditions, top disease states of interest or those driving the highest costs to the system, those that Medicare focuses on, etc.)\n\nSub-groups of some combination of the above, for instance: Medicaid, TANF, looking at mothers vs. newborns\n\nCross-group comparison of some combination of the above, for instance: Medicaid, TANF, new mothers and their cost/utilization/outcomes trends vs. Commercial, self-insured, new mothers and their cost/utilization/outcomes\n\n\nICU’s data is usually the most complete and available for research compared to other healthcare data\nRecognized cutpoints - 25 kg/m2 to define “overweight” based on body mass index.\nDS Use Cases:\n\nClinical Outcomes\n\nMortality (death) events\nEarly warning score (EWS) - likelihood of death\n\nFeatures: respiratory rate, oxygen saturation, temperature, blood pressure, heart rate, and consciousness rating\nEach variable has a normal range as established by common medical knowledge. A score is computed based on a lookup table to characterize how far away the variable is from its normal range. If the sum of all scores surpasses a threshold, it means a high likelihood of death\n\nEarly warning system for patient deterioration (CHARTwatch at Toronto hospital system, video)\n\nDeployed to General Internal Medicine ward (GIM)\nSlides with links to papers that discuss the model on the youtube website\nPredicts risk score of a patient “at risk”\n\n“At Risk” - transfer to ICU or transfer to Palliative Care unit or Death (composite endpoint, see Terms)\nProbability score then thresholded into “high,” “medium,” and “low” risk (color coded) labels\n\nFeatures: laboratory values, vital measurements, and demographics\nPrediction delivery\n\nEmail to nurses, palliative care unit, etc.\n\nTable with name, bed #,… , Status (aka prediction) for each patient\n\n“Front-End Tool” - software for people that sign out patients, has general patient information, includes column for Status\nPhone alerts for patients with High risk labels get sent to clinicians\n\n\n\nResource Utilization\n\nPlanning for hospital bed capacity\nScore for long length of stay (i.e. larger than 7 days)\n\nCommonly computed 24 hours after admission\nLogistic regression model (with proper regularization techniques)\nFeatures: age, gender, condition category, diagnosis code, hospital service received, and lab tests of vital signs to produce a probability number for long length of stay.\n\nAssignment/Scheduling\nForecast daily Emergency Department arrivals\n\nQuality of Care\n\n30 days readmission after discharge\nHospital score for readmission\n\nTypically calculated at discharge time\nFeatures: hemoglobin level, sodium level, type of the admission, number of previous admissions, length of stay, whether the hospital stay is cancer related, and whether medical procedures were performed during the stay\nBased on established medical knowledge, the values of each factor is translated to a risk score, and the sum of which depicts the overall risk of readmission\n\n\nMedical Imaging AI tools\nDashboard to compute dosage (e.g. blood thinner) based on patient health factors"
  },
  {
    "objectID": "qmd/healthcare.html#sec-health-terms",
    "href": "qmd/healthcare.html#sec-health-terms",
    "title": "Healthcare",
    "section": "Terms",
    "text": "Terms\n\nBiomedical Informatics - involves carrying out analysis on large-scale biological datasets in order to understand and profer solutions to health-related problems. Focuses on the optimal use of biomedical information, data, and knowledge for problem-solving and decision-making by employing computational and traditional approaches\nCalibration - the agreement between the estimated and the “true” risk of an outcome. A well-calibrated model is one that minimizes residuals, which is equivalent to saying that the model fits the test data well. (This is just GoF. How well the model generalizes.)\nClinical Data Science - focuses on applying data science to healthcare with the goal of improving the overall well-being of patients and the healthcare system\nDiscrimination - the ability of a model to rank patients according to risk (Often measured by AUROC)\nElectronic Health Record (EHR) -  comprehensive collection of all information by the individuals involved in patient care. This includes records from clinicians, laboratories, radiology imaging, health insurance, socio-demographics, genetic sequencing data, etc.\nElectronic Medical Record (EMR) - patient medical and treatment history within a single practice. EMR is the electronic version of the traditional paper records found in clinicians’ offices.\nEndpoint - Outcome variable measured in a medical study. e.g. Death, stroke, or quality of life are good endpoints. Blood tests and images on scans are not good endpoints.\n\nA composite endpoint is one that consists of two or more events\n\nExample: death due to cardiovascular causes or hospitalization due to heart failure\n\nSo the binary outcome would be a 1 if either of those events took place or a 0 if they did not. Or in a survival model, time until either of those events.\nIssues\n\nSee The All-Important Endpoint of a Medical Study for details\n\nSee Harrell in the comments for the solution\n\n\n\n\n\nFHIR - Fast Healthcare Interoperability Resources - open healthcare data standard (also sets standards for other things)\n\nOne JSON data schema per healthcare concept — e.g. Patient, Observation, Condition, MedicationRequest, etc.\n\nHealthcare Analytics - analytics activities that can be undertaken as a result of data generated from core areas of healthcare including claims and cost data, pharmaceutical and research & development data, clinical data, patient behavior & sentiment data (narrower in scope compared to clinical data science)\nLife Expectancy - a snapshot of the current mortality (“expenctancy” comes from “expected value”)\n\nAssumes that assumes that the observed age-specific death rates at the time of birth for a cohort stay unchanged for their entire lifetimes.\n\nLine of Business (LOB) - a statutory set of heath insurance policies\nProgression-Free Survival (PFS) - The length of time during and after the treatment of a disease, such as cancer, that a patient lives with the disease but it does not get worse. In a clinical trial, measuring the PFS is one way to see how well a new treatment works.\nProgression-Free Survival Rate - The percentage of people who did not have new tumor growth or cancer spread during or after treatment. The disease may have responded to treatment completely or partially, or the disease may be stable. This means the cancer is still there but not growing or spreading.\nSocial Determinants of Health (SDOH) - conditions in the environments where people are born, live, learn, work, play, worship, and age that affect a wide range of health, functioning, and quality-of-life outcomes and risks. (US Health and Human Services article)\n\nFive Categories: Economic Stability, Education Access and Quality, Health Care Access and Quality, Neighborhood and Built Environment, Social and Community Context"
  },
  {
    "objectID": "qmd/healthcare.html#sec-health-preproc",
    "href": "qmd/healthcare.html#sec-health-preproc",
    "title": "Healthcare",
    "section": "Prepocessing",
    "text": "Prepocessing\n\nCategoricals and Codes\n -\n\nReconciling codes from various sources of data can be challenging because some coding systems are proprietary\n\ne.g. “Heart failure” may be “123” in one coding system and “1a2b3c” in another\nSolution: Tokenize data and create embeddings\n\nWorks as long as each healthcare dataset uses a consistent set of coding systems for itself\n\n\nTokenization\n\nText\n\nJust split it by whitespaces.\nExample: “high glucose” becomes [“high”, “glucose”]\n\nCodes\n\nEmbedding\n\nFor every field in every healthcare concept, we build a vocabulary of a predefined size\n\nCreating a global vocabulary doesn’t work because different fields carries distinct healthcare semantics\n\nTrain an embedding for each token\nEmbeddings are learnt jointly with the prediction tasks\nChoose the same embedding dimension for all fields within a given healthcare concept (easier for aggregating)\n\nAggregate to a common time-step\n\ne.g. 1 hour or a few hours or can be tuned as a hyperparameter\nTake the average of all field embeddings in an instance (embeddings for a time step) to form the aggregated embedding for that instance\n\nCan also use median, etc. instead of mean\n\nIf there are multiple instances of a concept, we can further average the instance embeddings to form the embedding for that concept\nSince the healthcare concepts are a predefined enumerated list, we can concatenate the concept embeddings together to form a fixed sized example.\nIf a concept does not appear in the time-step, we just set its embedding to all 0s.\n\nAppend timestamp feature to embedding\n\nThe timestamp of the instances / training examples are not evenly spaced. When a particular event occurs and has significant clinical meaning, it’s not captured by the embeddings.\nTake the average of the timestamps of all instances in the time-stamp, and append it to the end of the fixed size embedding we obtained via fields -&gt; instance, and instances -&gt; concept aggregation\n\nThis part isn’t completely clear to me. Need to check the Google AI paper\nI think it means average all the timestamps within each aggregated embedding and append it to the embedding\n\nSeems like the averaged timestame would need to be transformed into a numerical before being appended."
  },
  {
    "objectID": "qmd/healthcare.html#sec-health-fairpriv",
    "href": "qmd/healthcare.html#sec-health-fairpriv",
    "title": "Healthcare",
    "section": "Fairness and Privacy",
    "text": "Fairness and Privacy\n\nMisc\n\nNotes from - Algorithmic Bias in Healthcare and Some Strategies for Mitigating It\nDeidentified clinical data sets are collections of observational patient data that have been stripped of all direct Patient Health Information (PHI) components. IRB permission is not necessary for access to deidentified clinical data sets.\nClinical data sets with HIPAA restrictions include observational patient information such as dates of admission, discharge, service, and birth and death as well as city, state, zip codes with five digits or more, and ages expressed in years, months, days, or hours. Without a patient’s consent or a HIPAA waiver, HIPAA-restricted clinical data sets may be used or shared for research, public health, or healthcare operations.\n\nBiases to fairness\n\nHistorical bias - when the data collected to train an AI system no longer reflects the current reality\n\ne.g. even though the gender pay gap is still an issue, it was worse in the past.\n\nRepresentation bias - depends on how the training data is defined and sampled from the population.\n\ne.g. the data used for training the first facial recognition system, mostly relying on white faces, which lead the model to have a hard time recognizing black faces and other dark-skinned faces.\n\nMeasurement bias - occurs when training data features or measurements differ from real-world data\n\ne.g. where the data for image recognition is mainly collected from one type of camera while real-world data is from multiple types of cameras.\n\nCoding/human bias - this happens mostly when scientists dive deep into a project with their subjective thoughts about their study\n\ne.g. “non-white patients receive fewer cardiovascular interventions and fewer renal transplants”, and “Black women are more likely to die after being diagnosed with breast cancer”. Source\n\n\nStrategies to mitigate bias\n\nCollecting and using diverse training data:\n\nData on-representative of the real-world population is a common cause of bias.\nCollect and use diverse training data that accurately reflects the demographics, backgrounds, and characteristics of the population the algorithm will be used on.\n\nTest the algorithm for bias:\n\nCan be done using a variety of methods, including conducting bias audits and using fairness metrics to measure the algorithm’s performance.\n\nUse algorithmic fairness techniques:\n\nPre-processing algorithms that adjust the data to reduce bias\nIn-processing algorithms that make adjustments during the training process\nPost-processing algorithms that adjust the output of the algorithm to make it fairer\n\nEnsure transparency and accountability:\n\nProvide clear explanations of how the algorithm works\nRegularly review and update the algorithm to remove any biases that may have been introduced\nProvide mechanisms for individuals to challenge the decisions made by the algorithm.\n\nEngaging with diverse stakeholders\n\ne.g. individuals and communities that may be affected by the algorithm\nUnderstand their perspectives and incorporate their feedback into the design and implementation of the algorithm.\nCan help ensure the algorith accurately reflects the needs and concerns of the population it will be used on.\n\n\nExamples of Algorithmic Bias\n\nUnitedHealth Group\n\nDeveloped a commercial algorithm in order to determine which patients would require extra medical care (patients with the greatest medical need).\nA bias in the algorithm reduced the number of black patients identified for extra care by more than half, and falsely concluded that black patients are healthier than equally sick White patients.\nRace correlated with other factors such as historical healthcare expenditures to evaluate future healthcare needs, which made it reflect economic inequality rather than the true medical needs of patients.\n\nDrug discovery for Covid-19\n\nAn AI system was developed to triage patients and expedite the discovery of a new vaccine\nThe AI system was able to predict with 70 to 80% accuracy which patients are likely to develop severe lung disease\nThe triage process was solely based on the symptoms and preexisting conditions of patients, which can be biased because of the disparities based on race and social economic status."
  },
  {
    "objectID": "qmd/insurance.html#sec-insur-misc",
    "href": "qmd/insurance.html#sec-insur-misc",
    "title": "Insurance",
    "section": "Misc",
    "text": "Misc\n\nDS Use Cases\n\nPredict insurance claims frequency (see bkmk)"
  },
  {
    "objectID": "qmd/insurance.html#sec-insur-risk",
    "href": "qmd/insurance.html#sec-insur-risk",
    "title": "Insurance",
    "section": "Risk",
    "text": "Risk\n\nWhen analyzing data, beware of survivorship bias\n\nExample: Real Estate Investment\n\nAn analyst is studying housing prices over time in a certain region. They use a current map and so only consider neighborhoods that have survived without major incidents (like natural disasters, economic decline, etc.). They will probably underestimate the risk and overestimate the return of real estate investment in that region.\n\n\nLimiting exposure\n\nFrom http://ronaldrichman.co.za/2021/02/24/x-is-not-fx-insurance-edition/\nSeverity\n\nCapping the payout of a policy\n\ne.g. only paying a maximum amount if tragedy strikes\n\n\nFrequency\n\nSetting a threshold to which the policy only pays out after the threshold has been passed\n\nKeeps the insurance company from being needled to death by administrative costs of frequent policy payouts\ne.g. minor doctor appointments\n\n\nReinsurance\n\nPolicies that produce an option-like exposure, where one can pass risk above a fixed level of losses to the counterparty for a fixed premium (excess of loss). Other options are to share risks in more or less equal proportions.\n\nAllows insurers take on risky (and potentially more profitable) policies by taking on an insurance policy themselves for the excess risk\n\nairplanes, volatile manufacturing, etc.\n\n\n\n\nAnalysis\n\nFit one distribution to the smaller and more frequent attritional losses, and another disruption to the extreme losses, with the latter distribution often motivated by extreme value theory\n\nThis approach ignores the fact the each loss has an upper bound determined by the limits on the policy generating the loss. Also, since these extreme losses follow a very heavy tailed distribution, naïve estimators of the statistical properties of these losses are likely to be biased\n\nShadow Moments\n\nTransform the data to a new domain that is unbounded, parameterizing EVT distributions in this domain, and then translating the implications of these models back to the original bounded domain\nCirillo, P., & Taleb, N. N. (2016). On the statistical properties and tail risk of violent conflicts. Physica A: Statistical Mechanics and Its Applications, 452, 29–45. https://doi.org/10.1016/j.physa.2016.01.050\nCirillo, P., & Taleb, N. N. (2020, June 1). Tail risk of contagious diseases. Nature Physics, Vol. 16, pp. 606–613. https://doi.org/10.1038/s41567-020-0921-x"
  },
  {
    "objectID": "qmd/insurance.html#sec-insur-mba",
    "href": "qmd/insurance.html#sec-insur-mba",
    "title": "Insurance",
    "section": "Market Basket Analysis",
    "text": "Market Basket Analysis\n\nSupport: What percent of patients have disease 1 and disease 2?\nConfidence: Of the people w/disease1, what percent also have disease 2?\nLift: How much more likely are you to have disease 2 if you already had disease 1 (and vice versa)"
  },
  {
    "objectID": "qmd/kpis.html#misc",
    "href": "qmd/kpis.html#misc",
    "title": "1  KPIs",
    "section": "1.1 Misc",
    "text": "1.1 Misc\n\nAlso see\n\nJob, Organizational and Team Development &gt;&gt; Developing a data strategy &gt;&gt; Objectives and Key Results (OKRs)\nProduct Development &gt;&gt; Metrics\n\nLeaders should focus on 3 to 6 KPIs that will drive growth.\nWhen a KPI’s growth begins to stagnate, break up the KPI monolith into smaller, more meaningful (and hopefully, easier to optimize) segments"
  },
  {
    "objectID": "qmd/kpis.html#terms",
    "href": "qmd/kpis.html#terms",
    "title": "1  KPIs",
    "section": "1.2 Terms",
    "text": "1.2 Terms\n\nOperationalization - a process of defining the measurement of a phenomenon that is not directly measurable (e.g. happiness), though its existence is indicated by other phenomena. It is the process of defining a fuzzy concept so as to make the theoretical concept clearly distinguishable or measurable, and to understand it in terms of empirical observations."
  },
  {
    "objectID": "qmd/kpis.html#general",
    "href": "qmd/kpis.html#general",
    "title": "1  KPIs",
    "section": "1.3 General",
    "text": "1.3 General\n\nKPI characteristics\n\nMonotonic: you always want to drive them higher or lower\nTypically a summary statistic or rate\n\nQuestion that you should ask when determining which metrics should be your KPIs What do you want to know?\n  How much do you need to know?\n\n  Why do you want to know this?\n\n  What is the value or impact between not knowing and knowing?\n“Input –&gt; Output –&gt; Outcome” framework\n\nInput Metrics\n\nProvide information about the resources used to create a system or process\nUseful for evaluating the efficiency of a system or process\nWhat you control: amount of time you spend on a task, the quantity of materials used to produce something, etc.\nOutputs should be highly responsive to your inputs\n\nOutput Metrics\n\nProvide information about the immediate results of a system or process\nUseful for evaluating the performance of a system or process\n\nAnswers whether the system or process is producing the desired results.\n\nIt should be clear how much your output would change for one additional unit of input.\nVery actionable but not fully under your control\nThere is a causal link between your output and your outcome, so difficult to discover\n\nMay require experimentation to determine causality\n\nTypically tracked daily\n\nOutcome Metrics\n\nNorth Star metrics (See Product Metrics &gt;&gt; Types)\nWhat you are aiming to move with all your activities.\nTypically requires multiple outputs to move\nTypically tracked monthly or quarterly\n\nExample: Educational Program\n\nThe # of teachers, their average seniority, the funding of the school (inputs) help drive the grades of the students and their consistency over time higher (output) which ultimately lead to more students graduating high school (outcomes).\n\n\nIndustry examples\n\nFinancial — Revenue growth, cash flow, burn rate, gross profit\nCustomer — Engagement rates, net promoter scores, acquisition costs, conversion rates\nSupport & Service — Turnaround time, mean time to resolve, SLA compliance, quality\nEmployee — Attrition and retention, satisfaction, engagement\nGovernance, Risk, & Compliance — Percent compliance to process, audit compliance, non-security incidents Issues\nAvoid incentivizing the wrong behavior\n\nAs soon as someone’s performance is linked to a metric — it is fair game to expect them to try to move the metric, so it is up to the metric designer to make sure the ‘rules of the game’ are clearly articulated.\nExample: Reducing the number of support tickets opened via email\n\nPotential Solution: Engineer makes it as hard as possible to contact the email support\nUnintended Consequence: Increasing the number of ‘negative’ social media interactions\n\nExample: You don’t want to push your salespeople to sell without caring about the retention rate.\n\ne.g Car salesmen selling a lemon to a customer. Customer unlikely to return to buy another car.\n\nSolutions\n\nPair metric with a guardrail metric (see Product Metrics &gt;&gt; Types)\n\nExamples\n\nquantity (sales) + quality (retention rate)\nshort term metric (inventory level) + long term metric (# of shortages)\n\n\nDesign a compound metric\n\nBy taking into account multiple measurement, it cannot be easily gamed\nCan also include metrics you don’t want move negatively (e.g. cost metrics, guardrail metrics, health metrics)\nFor experiments that use this metric, you then create a binary decision rule to decide if your experiment is successful or not.\n\n\n\nBeware of Deceptive Metrics\n\nCorrelation does not equal causation\nExample: Someone discovers that the # of transactions is correlated w/higher revenue\n\nCompany decides to make # of transactions a metric and starts trying to increase it.\n\ne.g. re-targeting former customers, offering discounts, etc.\n\nRevenue doesn’t substantially increase after considerable effort and resources\nAnalysis reveals that the revenue is being mostly driven by a few whales buying high-ticket items while the team’s increase in transactions was mostly low-ticket items\nSo, the strategy should’ve been to target more whales and not just trying to increase the # of transactions\n\n\n\nChecks\n\nMetric is precise (i.e. measurement isn’t noisy)\nMetric is accurate (i.e. it properly depicts the phenomenon it is supposed to depict).\n\nNext Steps\n\nFigure out the best way to calculate these metrics\nCreate dashboards to monitor them over time\nSet up alerts when thresholds are crossed\n\nanomaly detection"
  },
  {
    "objectID": "qmd/kpis.html#product-metrics",
    "href": "qmd/kpis.html#product-metrics",
    "title": "1  KPIs",
    "section": "1.4 Product Metrics",
    "text": "1.4 Product Metrics\n\nWhy are they important?\n\nMetrics help companies have clarity, alignment and prioritization in what to build.\nMetrics help a company decide how to build the product once they’ve prioritized what to build.\nMetrics help a company determine how successful they are and hold them accountable to an outcome.\n\nTypes\n\nNorth Star Metric (NSM)\n\nLong-term metric focused on the desired outcome for the entire company\nExamples of potential NSMs\n\nInstagram: Monthly Active Users\nSpotify: Time Spent Listening\nAirbnb: Booked Nights\nLyft: Rides per week\nSlack: Daily Active Users\nCustomer happiness (e.g. revenue, Net Promoter Score (NPS), and customer satisfaction)\nQuora: Questions Answered\n\n\nPrimary\n\nAsk\n\n“what is the desired outcome for this product?” (keeping the company’s mission in mind)\n\nExample: number of high quality sellers that join the platform as a result of the email outreach\n\n“What do our customers care about, and how do we solve it as fast as possible?”\n\nExample\n\nEngineering, product, and marketing agree that onboarding is a pain point, you decide to build goals and KPIs around making it easier for new customers to get started.\nAlign the company around the shared goal of reducing new tool onboarding from five days to three days\n\nData team gathers metrics on usage and helps build A/B tests\nEngineering team modifies the product\nMarketing team creates nurture campaigns\n\n\n\n\nMap this outcome to a metric that is meaningful, measurable, and movable\n\nMeaningful\n\nShould reflect the way a company intends to drive value for its customers (based on the company’s mission)\nAvoid vanity metrics.\n\nExample: Quora uses push notifications to alert users when they would be best suited to answer a question. However, the number of times users click on the notification and open the app is a vanity metric. It may make Quora feel good to see open rates going up. But Quora didn’t release push notifications to drive open rates. The outcome they were aiming for was to get high quality answers to questions asked on their platform. So an outcome oriented metric would focus on answer rates, not open rates.\n\n\nMeasurable\n\nEach component of that metric’s formula should be a datapoint that you can collect with high confidence and precision. And remember to add a time frame to your metric.\nSome aspects of a company’s or product’s mission/goal aren’t measurable directly but may be measured through latent variables, proxy, or highly associated variable or group of variables\n\nMoveable\n\nBasically means the metric shouldn’t be so noisy as to inhibit being able to measure changes when you take actions to change it.\nThe metric should measure something that as directly under control of the company as possible.\n\nExample: Page Load Time - time difference between a user clicking on a link or typing the URL in the browser and the page being rendered to the user.\n\nThis is affected substantially by the user’s computer, ISP, browser, etc. along with the app/website code. So asis, it wouldn’t be a good metric to track.\n\n\n\nOther Desirables\n\nQuick Feedback\n\nActions taken to influence the metric should be (almost) immediately observable\nExample: Measuring (subscription) retention would require you to wait until the end of the month in order to be able to judge the effect of your action. Daily Active Users might be a better alternative\n\nEasily Interpretted\n\nThe role of a metric is to align teams around a specific goal so that they can take the right steps towards achieving that goal. If people can’t understand the metric, they can’t take the right steps to optimize for it.\nExamples:\n\nComplicated: the median of a weighted combination of viewing time, comments, likes, shares per user (with each action having different weights based on its importance)\nSimpler: average watch time per user.\n\n\nNot Gameable\n\nI.e. picking an easily moveable metric that has no real value leads to bad incentives.\n\n\n\n\nSupporting/Tracking/Input Metrics\n\nLeading indicators that the NSM or the primary metric is moving in the right direction\nInputs into the NSM and are directly correlated to its value\nAlso tells you where your efforts to move your NSM may be falling short.\nExamples\n\nNumber of emails sent\nThe number of people that opened the email\nNumber of businesses that signed up to sell on the platform\n\n\nCounter Metrics/Guardrails\n\nOther outcomes that the business cares about, which may be negatively affected by a positive change in the primary metric (or NSM)\nThey exist to make sure that in the pursuit of your primary metric, you are not doing harm to another aspect of the business\nExample (email marketing to obtain quality sellers for Amazon)\n\nif your primary metric focuses on product quantity, your guardrail metric might be around product quality\nThe guardrail, average number of purchases a user makes in a day, ensures that the influx of sellers that the primary metric optimizes for doesn’t result in consumers becoming so overwhelmed by choice, that they end up not buying anything at all\n\n\n\nUser journey: AARRR (or Pirate Metric) Framework\n\nAwareness: How many people are aware your brand exists?\n\nmetric examples: number of website visits, social media metrics (number of likes, shares, impressions, reach), time spent on a website, email open rate\n\nAcquisition: How many people are interacting with your product?\n\nA lead is any potential user who’s information you’ve been able to capture in some shape or form.\n\ne.g. people who give you their email addresses when they sign up for your mailing list are considered to be leads.\n\nA qualified lead when they show additional interest in your product beyond giving you their information.\n\ne.g. in addition to signing up for your mailing list they also watch a webinar or sign up for a demo\n\nmetric examples: number of leads, number of qualified leads, sign ups, downloads, install, chatbot interactions\n\nActivation: How many people are realizing the value of your product?\n\ntypically in the form of an action taken x times with in a period of y days\nWhen the activation hurdle is crossed, an individual goes from unknown entity to actual user.\nExample (Dropbox): Their activation metric is the number of users that have stored at least one file in one dropbox folder on one device\nmetric examples: number of connections made, number of times an action is performed, number of steps completed\n\nEngagement: What is the breadth and frequency of user engagement?\n\nDepth of their usage: How often are they using your product? Is it above or below the average users frequency?\nBreadth of their usage: Are they performing every action that’s possible with your product? Are they favoring some more than others? What if you have multiple products? Are they using all of them?\nmetric examples: daily, weekly and monthly active users, time spent in a session, session frequency, actions taken in the product\n\nRevenue: How many people are paying for your product?\n\n** Remember, a business should optimize for the value they bring to their customers, not the revenue they generate. And, if their customers are deriving a lot of value from the business, willingness to pay will be a natural byproduct\nmetric examples: % of paid customers; average revenue per customer; conversion rate of trial to paid customers; number of transactions completed; shopping cart abandonment rates; ad-metrics like click-through-rate and conversion rate (crucial for ads based businesses)\n\nRetention/Renewal: How often are your people coming back?\n\nmetric examples: % of users coming back to your platform each day, month, year (product dependent); churn rates; customer lifetime value\n\nReferral: How many customers are becoming advocates?\n\nmetric examples: Net Promoter Score, viral coefficient i.e. the average number of people that your users refer you to\n\nAlso see\n\nSurvey, Design &gt;&gt; Response Scales &gt;&gt; Net Promoter Score\nAlgorithms, Product &gt;&gt; Net Promoter Score"
  },
  {
    "objectID": "qmd/location-selection.html#sec-locsel-misc",
    "href": "qmd/location-selection.html#sec-locsel-misc",
    "title": "Location Selection",
    "section": "Misc",
    "text": "Misc\n\nCustomer research, market expertise & experience, and competitor location analysis can all help inform the important criteria for your business\nTool to calculate population density within a certain radius of a location\nMight be more useful to aggregate smaller geographies into overlapping circular areas to compare candidates\n\n\nWould have to decide how to handle geographies that are only partially enclosed in a circlular area\n\nUse a percentage?\nInclude the whole thing?"
  },
  {
    "objectID": "qmd/location-selection.html#sec-locsel-fact",
    "href": "qmd/location-selection.html#sec-locsel-fact",
    "title": "Location Selection",
    "section": "Factors",
    "text": "Factors\n\nUnderstanding of the demographic or economic factors that must be in place to be successful\nExamples of questions\n\nDo you need a large population?\nHigh income population?\nHigh presence of certain age brackets?\nDo you rely on office worker foot traffic?\nIs the presence of certain business types important (restaurants, healthcare facilities)?\n\nNon-data factors\n\nAppropriate accessibility (car traffic/foot traffic, street frontage)\nSignage\nAvailability and size of space\nCost/affordability"
  },
  {
    "objectID": "qmd/location-selection.html#sec-locsel-locprof",
    "href": "qmd/location-selection.html#sec-locsel-locprof",
    "title": "Location Selection",
    "section": "Location Profiles",
    "text": "Location Profiles\n\nThese are created for existing stores and locations or potential new stores\nExample: Workforce and Demographic \n\nOther potential variables\n\nCustomer median driving distance\n\nMay also inform on the correct census geography to use\n\nDistance_to highways, business district, etc."
  },
  {
    "objectID": "qmd/location-selection.html#sec-locsel-anal",
    "href": "qmd/location-selection.html#sec-locsel-anal",
    "title": "Location Selection",
    "section": "Analysis",
    "text": "Analysis\n\nUse thresholds for any profile variables to help narrow the group of potential candidate locations to a managable number\n\nMight be useful to fit a decision tree to develop rules to use as thresholds\nExample\n\nZip Code Population of 25,000+\n\nMay want to use census geographies other than zip code\n\nCity Population of 150,000+\nGrowing Population\nHousehold Income of $75,000+\nHigh percentage of the population in the workforce\nHigh economic activity\nPrimary industry of employment in White Collar\nPercentage millennial population\nRestaurant density\n\n\nScore candidate locations\n\nCreate weights for important profile variables and then calculate scores for each candidate location\n\nMethods for creating weights\n\nWing it with domain knowledge\nCoefficients from a regularized regression of KPI ~ standardized_profile_vars could be used as weights\n\nOr feature importance, shapely values, etc. from tree model\n\nCorrelation or association statistics as weights\n\n\nOrder scores highest to lowest\n\nIf more than one location is considered, then group_by a suitably-sized geography\n\n\nCluster candidate location profiles with current successful stores\n\nCandidate locations that are in the same cluster as your stores are the ones that should be considered\nProminent features of the cluster(s) may indicate which profile variables are more important than others\n\nTake top-n candidates and dig deeper:\n\nCompetitor analysis\n\nExample questions\n\nHow many competitors exist is location?\nWhere are they located?\nHow satisfied are consumers with the options that exist today?\n\nWhich competitors are most popular, suggesting we may want to look in other areas?\ne.g. Google Map, Yelp, etc. reviews of competitors at this location\n\n\n\nMapping may illuminate other considerations\n\ne.g. One location has large swaths of uninhabitable land — is there enough population density for us to be successful?\n\nHow close are these locations to your other stores?\n\nCould one leach customers from the other?\n\nExamine profiles of final candidates\n\nWhat are the primary differences?\nWhat are the best features?"
  },
  {
    "objectID": "qmd/logistics.html#sec-log-misc",
    "href": "qmd/logistics.html#sec-log-misc",
    "title": "2  Logistics",
    "section": "2.1 Misc",
    "text": "2.1 Misc\n\nGoal: develop a replenishment policy that will minimize your ordering, holding and shortage costs.\nOrdering Costs ($ per order): fixed cost to place an order due to administrative costs, system maintenance or manufacturing costs\nHolding Costs ($ per unit per unit of time): all the costs required to hold your inventory (storage, insurance, and capital costs)\nShortage/Stock-out Costs ($ per unit): the costs of not having enough inventory to meet the customer demand (Lost Sales, Penalty)\nForecast PI widths serve as a proxy for inventory holding costs and provides valuable input for setting a target service level\nPackages\n\n{planr} - uses opening inventory, sales forecasts and supply variables to calculate projected inventory and projected coverage calculations (article)\n\nQuestion: “Due to the complexity/cost of maintaining inventory management system. Would it be sufficient to just set a safety stock level and replenish once the SKU dips below that level?”\n\nThe more products you handle, the more an inventory management system matters. If you have 1000s or 10s of 1000s of different products, it makes a large difference whether you do demand forecasting along with implementing the Order-Up-To Level Policy, etc.\n\nVariables of interest that need to be forecasted for various decisions\n\nStock/No Stock - should we continue to stock a product or discontinue the product and just let the current stock dwindle to zero.\n\nMean of Demand\n\nReplenishment - how much product should we restock\n\nMean and Variance of Demand\nForecasts in conjunction with a hypothesized demand distribution (parametric) vs build-up of the empircal distribution via bootstrapping (non-parametric)\n\nReturns - customer returning products\n\nNet Demand which is equal to Demand - Returns\n\nCan be forecasted itself or by forecasting Demand and Returns separately.\n\n\nLast Time Buy (LTB) - the supplier’s “last call” for a part or component. The final chance an enterprise will have to buy the part before the supplier stops producing it.\n\nRate of Demand Decline"
  },
  {
    "objectID": "qmd/logistics.html#sec-log-terms",
    "href": "qmd/logistics.html#sec-log-terms",
    "title": "2  Logistics",
    "section": "2.2 Terms",
    "text": "2.2 Terms\n\nActive References - a product may have multiple SKUs. All SKUs would be needed in order to calculate stats for that product. The active reference for a product is one SKU that encompasses all other SKUs.\n\ne.g. Last season’s dress “C” has been replaced by the new dress “D”. Even though both dresses are identical, they have different SKUs. Dress D’s SKU will be the active reference. Therefore, if the retailer sold two units of C in the past and three units of D this week, Nextail will show D having sold five units.\nAlso seen this term used when referring to all unique SKUs on an order sheet as the total active references.\n\nLinked Lines (aka [Silent Switches) - refer to when several products in a retailer’s inventory are commercially equivalent. In other words, when identical products are identified by multiple SKUs.\n\nExample: continuity products are ordered over multiple seasons or years or when large orders are split among different suppliers\n\nOrder Components\n\n[Order: The “shopping basket” full of items you’ve just purchased.\n[Lines: The different products within your order, recognized by warehouses as each individual Stock Keeping Unit (SKU) or Universal Product Code (UPC) number.\n[Units: The quantity of each line.\n\nReference - id for a product (e.g. SKU)\nRotations - speed at which products enter and exit the warehouse\n\nHigh rotation: Units enter and exit continuously. These items are in high demand.\nMedium rotation: Units enter and exit in smaller volumes than those in High Rotation.\nLow rotation: These are the items that spend the most time in the warehouse, and are in low demand.\n\nSKU - Stock Keeping Unit - Usually a bar code that has all the information to distinguish it from another product. These attributes can include manufacturer, description, material, size, color, packaging, and warranty terms. When a business takes inventory of its stock, it counts the quantity it has of each SKU.\nSupply Chain - a network of processes and stock locations built to deliver services and goods to customers.\nWastage - where supply greatly outstrips demand, and the product expires\nS&OP - Sales & Operations Planning"
  },
  {
    "objectID": "qmd/logistics.html#sec-log-demfcast",
    "href": "qmd/logistics.html#sec-log-demfcast",
    "title": "2  Logistics",
    "section": "2.3 Demand Forecasting",
    "text": "2.3 Demand Forecasting\n\nMisc\n\nAlso see DoorDash &gt;&gt; Forecast Supply and Demand\nReinforcement Learning for Inventory Optimization Series III: Sim-to-Real Transfer for the RL Model | by Guangrui Xie | Jan, 2023 | Towards Data Science\n\nMetrics for judging demand forecasts\n\nchange in the number of Stock-Out days\npercent change in Service Level\nchange in Profit\nchange in wastage\nchange in inventory costs\n\nNotes from Are Your Demand Forecasts Hurting Profits and Service Levels\n\nBlue Dot Thinking’s WasteNot API service ($) WasteNot (logistics analytics firm) uses prophet for demand forecasting which consistently under-predicts, so they use an adjustment for better forecasts\nPrediction adjustment - (e.g. for perishables) calculate an optimal “Buffer Multiplier” value — each predicted value from the statistical forecast is multiplied by the multiplier, resulting in a higher number of units replenished each day\n\nVariables used:\n\nunit_sale_price/unit_cost\nhistorical variability\nshelf-life (seconds)\nstock levels They didn’t show the formula, but I’m guessing their modeling residuals with these variables.\n\n\n\nNotes from Case Study: Applying a Data Science Process Model to a Real-World Scenario A VERY detailed article that goes through a scenario of step-by-step planning and execution of changing a manual stock replenishment process to an automated one\n  As a guide, they use [DASC-PM](https://medium.com/towards-data-science/dasc-pm-a-novel-process-model-for-data-science-projects-9f872f2534b1) (DAta SCience - Process Model) - a structured and scientific process for project management\n\nProject manager tries to examine whether the project can fundamentally be classified as feasible and whether the requirements can be carried out with the available resources.\n\nExpert Interviews: Is the problem in general is very well suited for the deployment of data science and are there corresponding projects that have already been undertaken externally and also published?\nData science team: Are there a sufficient number of potentially suitable methods for this project and are the required data sources are available?\nIT department: check the available infrastructure and the expertise of the involved employees. Demand forecasting model\nRequirements:\n\nAccuracy of 75%. This means that the forecasts for quantities of each product should deviate from actual requirements by no more than 25%.\nProduce monthly planning cycles and quantify the need for short-term and long-term materials\n\nData sources: Order histories, inventory and sales figures for customers, and internal advertising plans\nFeatures: seasonality, trends, and market developments\nForecasts regenerated every month\n\nForecasts loaded into internal planning software\n\nProjections will be analyzed and, if need be, supplemented or corrected.\n\nPlanners can make their corrections during the first four working days of the month.\nThe final planning quantity will ultimately be used by the factories for production planning.\n\nExample: IBM Planning Analytics\n\nAllows for the creation of flexible views where the users can personally choose their context (time reference, product groups, etc.)and adjust calculations in real-time.\nSounds like expensive optimization software with a snazzy UI\n\n\nFinal plans are loaded into the Data Lake after processing by the planning teams so they can be referenced in the future.\nUser Integration\n\nUsers are included in the development from the beginning to ensure technical correctness and relevance and to ensure familiarity with the solution before the end of the development phase.\nSimple line and bar charts for processes and benchmarks are used, along with tables reduced to what is most important.\nPlanners get training sessions to help them interpret the forecasts and classify their quality.\nComplete documentation is drafted\n\nTechnical part:  data structures and connections\nContent part: jointly prepared with the users to describe the usage of the data product\n\n\nPost-Development Phase (i.e. maintenance of the project)\n\nConstant automated adjustment of the prediction model to new data\nVarious parameters such as the forecast horizon or threshold values for the accuracy of the prediction can be made by the planners\nProblems occurring after the release of the first version are entered via the IT ticket system and assigned to the data science area\nAt regular intervals, it is also checked whether the model still satisfies the expectations of the company or whether changes are necessary.\n\n\nForecasting shocks is difficult for an algorithm\n\nNotes from Why Good Forecasts Treat Human Input as Part of the Model\nPreprocessing\n\nIt can be better to smooth out (expected) shocks in the training data and then add an adjustment to the predictions during the dates of the shocks.\nThe smoothed out data will help the algorithm produce more accurate predictions for days when there isn’t an expected shock.\n\ne.g. kalman filter with parameters for seasonality, trends from {{tsmoothie}}\n\nNothing special about this smoother. Probably just the method in tsmoothie that performed best for them (which is a good reason to use it).\n\n\nApproaches for manually replacing oulier values\n\nReplace the outlier week with the most recent week prior to the dip as a proxy for what should have happened.\nUse domain knowledge\n\ne.g. Marketing departiment is expecting 10% growth the week after it’s new promotion\n\nReplace with one of the previous methods, then apply a smoothing algorithm\n\nShows the Kalman filter by itself fails to replace the outlier dip with the normal, expected peak, but the manual adjustment + Kalman gives us what we want\n\n\nExamples of shocks that may need training data to have manual adjustments and not be smoothed by an algorithm\n\none-time spikes due to abnormal weather conditions\none-off promotions\n\nreplace the outlier week with the most recent week prior to the dip as a proxy for what should have happened.\n\na sustained marketing campaign that is indistinguishable from organic growth.\n\n\nPrediction Adjustment\n\nExtreme events (i.e. unprecedented promotions or weather), where the expected impact is well outside of any historical data, may prove impossible for forecasting methods to produce an adequate forecast\n\nSince it’s outside the range of historical data, building a more complex model or just including weather or promotional features won’t help.\n\nRequires using domain expertise of the event to use to adjust the predictions\nMake sure to build-out the code infrastructure so that each manual adjustment should just feel like adding another input to the model.\n\n\nIntermittent Demand\n\nMisc\n\nAlso see ADAM ebook chapter, post, real world use case on adding an occurrence variable to an ETS model to handle intermittent data\n\nThe use case paper also includes a GAMLSS with truncated Normal distribution model with code that performed well\n\nUses {probcast} which has functions around gams, gamlss, and boosted gamlss models from {mgcv}, {mboost}, {gamlss}, etc.\n\n\nNotes from\n\nCMAF FFT: Intermittent Demand Forecasting (Video)\n\nFrom authors of “Intermittent Demand Forecasting. Context, Methods and Applications” (see your book shelf)\n\n\nPreferrable to avoid intermittence by aggregating data to a category that’s higher in the product hierarchy or lengthen the frequency.\n\nNot always possible. For example, different SKUs have different lead times, so aggregating products into categories with the same protection intervals can be complicated.\n\n\nProtection Interval is your horizon, which equals the length of the review period + length of the lead time\n\nSee Order-Up-To (OUT) Level Policy for details\n\nCompound Distributions\n\nTarget\n\nIncidence/occurrence (of Demand): Poisson and Bernoulli\nDiscrete positive Demand interspersed with zeros: Neg. Binomial\n\nSounds like zero-inflated, censored distribution\n\n\nOptions\n\nDiscrete Compound Poisson (aka Stuttering Poisson): Poisson + Geometric\nNegative Binomial: Poisson + Logarithmic\n\nLumpy data\n\nData that’s intermittent and has extreme variability in demand sizes\nNegative Binomial built to handle this type of data\n\n\nForecasting Mean Demand\n\nParametric\n\nExponential Smoothing\n\nPopular method but bad for intermittent forecasting\n\nADAM extends ETS by adding an occurence variable to the model, so this might not be the case.\n\nBiased on “issue points” (see video for more details)\n\nCroston Method\n\nInversion Bias: we believe the mean demand is higher than predicted\n\nSBA (Syntetos-Boylan Approximation)\n\n*Recommended*\nCorrects Croston bias\nSupported by empirical evidence\n\nTemporal Aggregation: Overlapping vs Non-Overlapping\n\nself-improving mechanism\nSee video for details (although it wasn’t discussed in great detail)\n\n\nNon-Parametric (aka Empirical)\n\nBootstrapping\n\nresample in blocks or resample independently\n\nindependently is how you normally see bootstrapping\nWith blocks, bin your sequential data into non-overlapping partitions and resample independently within each partition.\n\n\n\n\nForecasting Demand Variance\n\nUsing the variance of the forecast error of the protection interval to estimate the variance of demand over the protection interval\n\nClassical method is to calculate the variance of the errors over each review period and aggregate to the get the variance over the protection interval, but the above method is better empirically and theoretically\n\n\nDiagnostics\n\nDo NOT use metrics based on absolute errors (e.g MASE, MAPE) by themselves\n\nMinimization of these metrics, by themselves, can result (i.e. over half of your values are zeros) in always recommending 0 value forecasts\nCan be used in conjunction with bias correction measures\n\nScaled Mean Squared Error\nRelative Root Mean Squared Error\nLook at the predictive distributions instead of just the point forecasts\nAccuracy Implication Metrics - measure the effect of the forecast on inventory\n\nY Axis: 1 - Fill_Rate; X Axis: Average on-hand inventory\nEach method is a forecasting model\nThink the Fill Rate is estimated from a simulation using the method’s forecast and a range of average SOH values as parameters"
  },
  {
    "objectID": "qmd/logistics.html#sec-log-safstk",
    "href": "qmd/logistics.html#sec-log-safstk",
    "title": "2  Logistics",
    "section": "2.4 Safety Stock (aka Buffer Stock)",
    "text": "2.4 Safety Stock (aka Buffer Stock)\n\nMisc\n\nNotes from Article\nCalculation of Safety Stock can be more useful than trying to improve forecast accuracy for intermittent(or sporadic) product time series (lotsa zeros).\n\nSafety stock - extra inventory held by a retailer or a manufacturer in case demand increases unexpectedly. This means it’s additional stock above the desired inventory level that you would usually hold for day-to-day operations.\nReplenishment cycle - cycle between replenishment (i.e. restocking) orders\nFill rate (aka Fulfillment Rate)- the percentage of orders that you can ship from your available stock without any lost sales, backorders, or stockouts.\n\nFill Rate = (Total Orders Shipped / Total Orders Placed) x 100\nOn average, companies typically maintain a fill rate of about 85%-95%. But ideally, you should strive for a fill rate between 97% and 99%.\n\nReasons for safety stock:\n\nDemand uncertainty - Every retailer and manufacturer will have products that sell well all year round and products that fluctuate in demand.\nLead time uncetrainty - deliveries arriving earlier or later than expected, a safety stock formula will help you to cover unexpected delays and demand fluctuation to maintain a consistent output.\n\nlead time (aka performance cycle)- time required between the creation of a replenishment order and the effective store replenishment\n\nUsually a distribution and not a constant, so it needs to be recorded in order to get a sample standard deviation thats used the safety stock formula\nFactors:\n\ndeciding what to order or produce\napproval time\nsubmitting a purchase requisition\nemailing vendors\nmanufacturing and processing of the product\ndelivery time from vendor\nincoming inspection time\ntime it takes to put on the shelf\nany additional time required to return to the start of the next cycle Stockouts - out of stock events\n\n\n\nSafety stock determinations are not intended to eliminate all stockouts—just the majority of them Usually caused by:\n\nChanges in consumer demand\nIncorrect stock forecasts\nVariability in lead times for raw materials\n\nCosts due to stockouts\n\nLoss of revenue\nLoss of gross profit\nLoss of customers\nReduced market share\nPoor efficiency\nStrained supplier and retailer relationships\n\n\n(Cycle) Service level (Z) - the probability that the amount of inventory on hand during the lead time is sufficient to meet expected demand – that is, the probability that a stockout will not occur.\n\nHigher service level –&gt; more safety stock\nIndependently choose a service level for groups of products based things such as strategic importance, profit margin, or dollar volume.\n\nThe retail industry aims to maintain a typical service level of between 90% and 95% depending on the product\n\nExample: \n\nAt 95 percent service level, expect:\n\n(D1) for 50 percent of replenishment cycles, not all cycle stock will be depleted and safety stock will not be needed\n(D2) for 45 percent of replenishment cycles, the safety stock will suffice.\n(D3) and for 5 percent of replenishment cycles, expect a stockout.\n\nKind of a confusing diagram but I think the y-axis is total stock and x-axis represents time (kinda sorta)\n\nStock dwindles as product is sold as the cycle ends, then stock is replenished after an order and begins to dwindle again.\n\n\n\nSafety Stock equations\n\nWhen the demand interval doesn’t equal lead time interval\n\nused to mitigate demand variability and lead time variability (σLT see next formula) is very small or zero\nSafety stock = Z × √[PC / T1] × σD\n\nPC = performance cycle, another term for total lead time\nT1 = time increment used for calculating standard deviation of demand\nσD = standard deviation of demand.\n\n(Partial) Example: if the standard deviation of demand is calculated from weekly demand data and the total lead time including review period is three weeks.\n\nPC = 21 days (3 weeks)\nT1 = 7 days (weekly data)\nSafety Stock = Z * √3 * σD\n\nExample: Desired service level = 95%; seven-day manufacturing time and the one day needed to arrive at the warehouse; Standard deviation of weekly demand = 10 rolls\n\nSafety stock = 1.65 * √[8/7] * 10 rolls\n\nSafety stock = 18 rolls\n\n\n\n\n\nWhen the lead time varies and demand variability (σD see previous formula) is very small or zero\n\nSafety Stock = Z × σLT × Davg\n\nZ is the z-score of the service level (1-sided, upper-tail)\n\ne.g. 95% service level –&gt; Z = 1.64; 90% service level –&gt; Z = 1.28\nqnorm(0.95) = 1.644854, qnorm(0.90) = 1.281552\n\nσLT is the standard deviation of the lead time\nDavg is the demand forecast\n\nDepends what the frequency of the series is, but you want an estimate of the total demand between orders\nExample: If orders are made monthly and you forecast weekly sales, then your horizon is likely monthly and you sum around 4 weeks of point estimates to get D.\n\n\n\nWhen both demand variability and lead time variability are present\n\ndemand and lead time variability are independent\n\nSafety stock = Z × √[(PC/T1  ×  σD2 ) + (σLT × Davg)2]\n\ndemand and lead time variability are not independent\n\nSafety stock = (Z × √[PC/T1] × σD) + ( Z × σLT × Davg)\n\nDemand variability is the dominant influence on safety stock requirements.\n\nWith the recognition of what factors dominate an equation, it becomes easier to focus improvement efforts\n\n\n\nIssues\n\nSometimes recommended safety stock volumes are larger than business leaders are comfortable having\n\nalternative/supplement: order expediting\n\nReduce safety stock volume by keeping small amounts of expensive products and rely on air freight to cover peaks in demand. The cost of shipping a small percentage of total demand via air can be minimal compared to the cost of carrying large amounts of safety stock of the valuable material on an ongoing basis.\n\nalternative/supplement: make-to-order (MTO) or finish-to-order (FTO) production environment\n\nIf lead times allow, MTO eliminates the need for most safety stock. Meanwhile, FTO allows for less differentiation in safety stock than finished-product inventory, which lowers demand variability and reduces safety stock requirements.\nFTO and MTO also are well suited for situations where customers are willing to accept longer lead times for highly sporadic purchases."
  },
  {
    "objectID": "qmd/logistics.html#sec-log-eoq",
    "href": "qmd/logistics.html#sec-log-eoq",
    "title": "2  Logistics",
    "section": "2.5 Economic Order Quantity (EOQ) (aka Wilson Formula)",
    "text": "2.5 Economic Order Quantity (EOQ) (aka Wilson Formula)\n\nthe ideal order quantity a company should purchase to minimize inventory costs such as holding costs, shortage costs, and order costs\n\nUsually used for purchase ordering (not production)\nAssumes demand, ordering, and holding costs remain constant over time\n\nGoal: minimize the cost of ordering and holding stock, while still meeting demand and service level requirements\nCosts of ordering:\n\nplacing your order\ndelivery\ntransportation\nreceiving the order\n\nCosts of holding stock:\n\npaying for stock in advance\nwarehousing\nstorage\ndepreciation"
  },
  {
    "objectID": "qmd/logistics.html#sec-log-outpol",
    "href": "qmd/logistics.html#sec-log-outpol",
    "title": "2  Logistics",
    "section": "2.6 Order-Up-To (OUT) Level Policy",
    "text": "2.6 Order-Up-To (OUT) Level Policy\n\n\nTime intervals (i.e. Review Interval) trigger a replenishment, not reorder points\n\nLength depends on the industry\nExamples\n\nManufacturing: 1 month\nRetail: 1 week\n\n\nMisc\n\nNotes from\n\nCMAF FFT: Intermittent Demand Forecasting (Video)\n\nFrom authors of “Intermittent Demand Forecasting. Context, Methods and Applications” (see your book shelf)\n\nShould have more details on OUT replenishment model\n\n\nUT-Dallas “Basestock Model CH. 13” Slides\n\nBased on Cachon & Terwiesch book, “Matching Supply with Demand” (link)\n\n\n\nInventory Position (IP) = Stock-on-Hand - backorders + On-Order-Inventory\nAfter every Review Interval, the OUT gets optimized according to Replenishment variables\n\nMean and Variance of Demand are estimated\nForecasts in conjunction with a hypothesized demand distribution (parametric) vs build-up of the empircal distribution via bootstrapping (non-parametric)\n\nSee Demand Forecasting &gt;&gt; Intermittent Demand &gt;&gt; Forecasting Mean Demand\n\nGiven updated variables, place order that raises IP to OUT level (S)\n\nProtection Interval is period that you should have enough inventory to cover.\n\nThe forecast horizon which equals the Review Interval + Lead Time\n\nEvaluate service at each level of an order: orders, lines, units.\n\nLines (SKU Level)\n\nWhether to use Cycle Service Level (CSL) or Fill Rate (ReadyRate (?) is also a possibility)\n\nCSL - probability of not going out of stock -  not realistic but easy to calculate\nFill Rate measures true service offered to customers, but more involved in its application\nAlso see Safety Stock for more details on CSL and Fill Rate"
  },
  {
    "objectID": "qmd/logistics.html#sec-log-reordpt",
    "href": "qmd/logistics.html#sec-log-reordpt",
    "title": "2  Logistics",
    "section": "2.7 Reorder Point",
    "text": "2.7 Reorder Point\n\nThe reorder point is the threshold amount of inventory at which you need place an replenishment order\n\nonce an item’s stock falls below PAR level, an optimised order quantity is generated\n\nComponents used to determine a reorder point\n\nSafety Stock\nReorder Point Formula\nPeriodic Automatic Replacement (PAR)\n\nReorder Point = Safety Stock + (Davg × Lead time)"
  },
  {
    "objectID": "qmd/logistics.html#sec-log-dim",
    "href": "qmd/logistics.html#sec-log-dim",
    "title": "2  Logistics",
    "section": "2.8 Decision Impact Metrics",
    "text": "2.8 Decision Impact Metrics\n\nOther methods for determining forecast performance have serious flaws\n\nReasons industry benchmarks for forecast accuracy shouldn’t be used\n\ndiversity of business strategies (size of portfolio, product & brand positioning)\nthe level at which the forecast accuracy is measured and even the metric’s definition itself (especially with value-weighted formulas) may differ\n\nForecast Value Added (FVA)\n\nFVA: The change in a forecasting accuracy metric that can be attributed to a particular process or participant in the forecasting process.\n\nExample\n\nStatistical model has 5% FVA vs Naive forecast\nAdjusted Statistical Forecast has 2% FVA vs Naive Forecast\n\nForecast gets manually adjusted by management\n\n\n\nReasons why FVA shouldn’t be used\n\nThe difference in Forecast Accuracy (FA) metrics (e.g. MAPE) of the production forecasting algorithm and the naive forecasting algorithm is usually weighted by portfolio revenue, volume or number of items\nA FA metric is not a key business performance indicator (KPI)\n\nFA has little correlation with business performance.\nImproving FA does not mean you are generating value.\nCosts may increase, decrease or remain the same as accuracy changes.\n\nFA metrics can contradict each other\n\nSwitching from one FA metric to another could profoundly alter your FVA results.\n\n\n\n\nConstraints and business rules have to be considered and not just a forecast metric\n\ne.g. allowed pack sizes, the minimum order quantity, the storage finite capacity, holding/excess costs, shortage costs, fixed costs, etc.\n\nForecast Accuracy (FA) metrics are difficult to understand for normals\n\nUsing Decision Impact (DI) metrics can help eliminate:\n\n“The forecast is always wrong!”, “The forecast is too this”, “The forecast is not enough that”, “What does 70% FA mean?”, “Is this good or bad?”\n\n\nProbabilistic Forecasts + DI metrics\n\nIdentify parameters (i.e. whatever value you’re forecasting) with the highest economic risk\n\nEconomic risk is related to the size of the PI of the forecast\n\nInvestigate potential causes for the risk and find solutions\nExample\n\nCalculate\n\nDIa min - the costs associated with the forecast at the 5% percentile\nDIa max - the costs associated with the forecast at 95% percentile.\n\nCalculate economic risk\n\n\nDecision-based Components - components that takes into account the decision being made using the forecast\n\nExample: One weather forecast predicts 4 inches of rain but another forecast predicts 0 inches of rain. The next day it rains 1 inch.\n\nIf the decision being made was whether to take an umbrella, the first forecast is the best forecast even though its error is worse than the second forecast.\n\nComponents\n\nDecision Function - for any forecast input, simulate the decision process and evaluate the quality of the final decision\nDecision Impact (DI) - metric that defines how decision quality is measured\n\nusually in terms of financial cost\nA “North Star” type metric\n\n\n\nDecision Cost function: The cost function is used to score each stock replenishment decision based on its true business impact usually in terms of financial cost\n\nExample: Walmart retail data (M5)\n\nThe Decision Cost is the sum of following factors:\n\nOrdering, shipping and handling cost (Fixed Costs)\n\nFulfilling an order generates costs for the ordering, preparation, expedition, transportation, etc.\nLet’s assume these costs represent $40 per range of $1000 of purchase value.\n\nHolding cost (Excess Costs)\n\nHolding costs are associated with the storage of unsold inventories.\nLet’s assume the annual holding cost is 10% of the inventory value (valued at purchase price), i.e. 0.19% per week.\n\nShortage cost\n\nWhen demand exceeds the available inventory, both the demand and customer goodwill may be lost.\nAs retailers propose a wide range of similar products, a part of the demand is carried to other products.\nLet’s assume that only half of the sales will effectively be lost. The shortage cost could then be measured as 50% of each lost sale gross margin.\n\n\n\nOther Considerations\n\nReplenishment Strategy\n\nLeadtime\nOrder Cycle\nReplenishment Policy\nSafety Stock\n\nAdditional Product Information\n\nGross Margin\nPack Sizes\nInitial Inventory\n\nexample: using the safety stock value\n\n\nComputational Costs\n\nAlgorithm training time\nCompute size\nData pipeline\n\n\n\nTypes of forecasts that are required\n\n“actual” forecast: forecast from a candidate model that will potentially go into production\n“naive” forecast: forecast from a simple method (e.g. seasonal-naive, simple moving average, etc.) or a previous used method\n\nSteps\n\nDecide how best to measure the Decision Impact as related to the forecast\n\ne.g. financial cost\n\nFormulate a decision cost function\n\ne.g. fixed costs + excess costs + shortage costs (see above)\n\nGenerate forecasts on a test/assessment set.\nCalculate decision impact costs:\n\nActual cost (DIa): cost after applying the decision cost function to the “actual” forecast (see above)\nNaive cost (DIn): cost after applying the decision cost function to the “naive” forecast (see above)\nOracle cost (DIo): cost after applying the decision cost function to the observed values\n\nThis would be the costs of a forecast in which we had perfect knowledge. (So probably just fixed costs)\n\n\nCalculate Decision Impact metrics:\n\nEarned Value (aka Forecast Value Added) (DIna)\n\nDIna = DIn - DIa\n\nUnearned Value (aka yet-to-be earned value)(DIao)\n\nThe value that could still be gained by improving forecasts\nDIao = DIa -DIo\n\nTotal Earnable Value (DIno)\n\nThe range of earnable value\nDIno = DIn - DIo\n\nProportion of the Earned Value\n\nDIna/DIno\n\nProportion of yet-to-be earned value\n\nDIao/DIno\n\n\n\nDI metrics for all generated forecasts can be used to calculate a quarterly, semestrial, or annual ROI for the product planning department\n\nROI = sum(DIna)- (cost of people, tools, etc. used by department to generate forecasts)"
  },
  {
    "objectID": "qmd/logistics.html#sec-log-pffpp",
    "href": "qmd/logistics.html#sec-log-pffpp",
    "title": "2  Logistics",
    "section": "2.9 Profit Functions for Perishable Products",
    "text": "2.9 Profit Functions for Perishable Products\n\nThe newsvendor problem is a class of problems, where the product can only be sold one day, after which it goes to waste. So this is appropriate, for example, for perishable products in retail\nNotes from An Integrated Method for Estimation and Optimisation; associated paper\nIf we order more than needed, we will have holding costs. In the opposite case, we will have shortage costs.\n\nBased on these costs and the price of product, we can find the optimal amount of product to order, that will give the maximum profit.\n\nInstead of a two-stage problem (see DoorDash section): optimising the forecast model via MSE or any other conventional loss and then solving the optimisation problem, we could estimate the model via maximisation of the specific profit function, thus obtaining the required number of product orders directly.\nCalculate profit as a linear\n\nπ(_q__t_,_y__t_)={_p__y_t−_v__q_t−_c__h_(_q__t_−_y__t_),_p__q_t−_v__q_t−_c__s_(_y__t_−_q__t_),for ​_q__t_≥_y__t_for ​_q__t_&lt;_y__t_,\n\nTerms\n\nyt is the actual sales\np is the price of the product\nqt ​is the order quantity\nv is the cost of production\nch is the holding cost\ncs is the shortage cost\n\n\nlibrary(greybox)\n# Generate artificial data\nx1 &lt;- rnorm(100,100,10)\nx2 &lt;- rbinom(100,2,0.05)\ny &lt;- 10 + 1.5*x1 + 5*x2 + rnorm(100,0,10)\nourData &lt;- cbind(y=y,x1=x1,x2=x2)\n# Define price and costs\nprice &lt;- 50\ncostBasic &lt;- 5\ncostShort &lt;- 15\ncostHold &lt;- 1\n# Define profit function for the linear case\nlossProfit &lt;- function(actual, fitted, B, xreg){\n    # Minus sign is needed here, because we need to minimise the loss\n    profit &lt;- -ifelse(actual &gt;= fitted,\n                    (price - costBasic) * fitted - costShort * (actual - fitted),\n                    price * actual - costBasic * fitted - costHold * (fitted - actual));\n    return(sum(profit));\n}\n# Estimate the model\nmodel1 &lt;- alm(y~x1+x2, ourData, loss=lossProfit)\n# Print summary of the model\nsummary(model1, bootstrap=TRUE)\nCoefficients:\n            Estimate Std. Error Lower 2.5% Upper 97.5% \n(Intercept)  36.5177    14.2840    2.7783    51.4844 *\nx1            1.3622    0.1622    1.1909      1.7528 *\nx2            3.3423    2.7810    -6.5997      5.9101\n\nInterpretation: with the increase of the variable x1, the orders should change on average by 1.36\nPlot\n\nplot(model1, 7)\n\n\nFigure above corresponds to the orders (purple) and would cover roughly 90.91% of cases (black), so that we would run out of product in approximately 10% of cases, which would still be more profitable than any other option.\nNonlinear case\n\nSee link to associated paper above\nThe only thing that would change is the loss function, where the prices and costs would depend non-linearly on the order quantity and sales."
  },
  {
    "objectID": "qmd/logistics.html#sec-log-drdsh",
    "href": "qmd/logistics.html#sec-log-drdsh",
    "title": "2  Logistics",
    "section": "2.10 DoorDash",
    "text": "2.10 DoorDash\n\nNotes from https://towardsdatascience.com/managing-supply-and-demand-balance-through-machine-learning-70d4f0808617\n2-stage solution\n\nforcasting supply and demand\noptimizing supply of workers with demand of food orders\n\nDefine the problem\n\nSupply and demand imbalance  (i.e. drivers and food orders)\nEffects\n\nFor consumers, a lack of driver availability during peak demand is more likely to lead to order lateness, longer delivery times, or inability to request a delivery and having to opt for pick up.\nFor Dashers, a lack of orders leads to lower earnings and longer and more frequent shifts in order to hit personal goals.\nFor merchants, undersupply leads to delayed deliveries, which typically results in cold food and a decreased reorder rate.\n\n\nOptimization Strategies\n\nBalancing at the delivery level means every order has a Dasher available at the most optimal time\n\nconsumer preferences and other changing conditions in the environment, such as traffic and weather, make it difficult to balance supply and demand at the delivery level\n\nBalancing at the market level means there are relatively equal numbers of Dashers and orders in a market but there are not necessarily optimal conditions for each of these groups at the delivery level.\n\nMetric\n\nNumber of hours required to make deliveries during a time period\n\nOptimize keeping delivery durations low and Dasher (drivers) busyness high\nAble to account for regional variation driven by traffic conditions, batching rates, and food preparation times.\n\nUnits: hourly or day-parts (e.g. breakfast, lunch, dinner)\n\nThere’s too much variation during a day in order to aggregate to a higher level metric. Demand and supply would be artifically smoothed.\n\nExample:\n\nSunday at dinner time in New York City, and we estimate that 1,000 driver hours are needed to fulfill the expected demand. We might also estimate that unless we provide extra incentives, only 800 hours will likely be provided organically. Without mobilization actions we would be undersupplied by about 200 hours.\n\n\nOptimization\n\nAdjust supply of drivers by incentivizing with pay bonues during high demand hours\n\nForecast Supply and Demand\n\nLightGBM\nPredictors\n\nInformation about population size, general traffic conditions, number of available merchants, climate, and geography\ncharacter variables were replaced with embedding vectors\n\nSupply\n\nCounterfactual to understand how to make tradeoffs between supply and costs\n\nHow will supply levels change if we changed incentive levels so that we can ?\nI assume “incentive_level” is a variable in the supply forecast model, so this could just be adjusted in “newdata” and a  prediction (or maybe during a training session?) made to see the effects on supply.\n\n\n\nDecision Making\n\nconsumes supply and demand predictions and attempts to generate a set of optimal actions\nMixed-Integer Programming (MIP)\n\nlinear optimization (see bkmks, ompr pkg)\neasy to formalize, implement, and explain to stakeholders\nCustom objective function for minimizing undersupply with several constraints.\n\nCan be configured to favor either profitability (profit per customer?) or growth (increase in orders per customer?)\n\nor maybe its about drivers — profitability (just enough incentive to get just enough drivers?) or growth (not sure what the “driver” angle is here)\n\nConstraints\n\nNever allocate more than one incentive in a particular region-time unit.\nNever exceed the maximum allowable budget set by our finance and operations partners.\nregional\n\ndifferent budgets, custom penalties, exclusion criteria for which units should not be included in the optimization, or incentive constraints that are guided by variability of the inputs.\n\n\n\nOptimizer must account for uncertainty\n\nCity B’s forecast distribution has substantial uncertainty, and it’s mean says it will have enough drivers (i.e. oversupply)\nCity A’s forecast distribution is more certain, and it’s more likely to be undersupplied.\nW/o taking uncertainty into account, the optimizer will not take into account that there’s a sizeable chance B will be undersupplied\nTaking uncertainty into account sometimes causes an over-allocation of resources to these uncertain regions (small areas, fewer orders, fewer drivers, larger variance)\nThey do something with resampling to solve this but I didn’t quite understand it."
  },
  {
    "objectID": "qmd/logistics.html#sec-log-supch",
    "href": "qmd/logistics.html#sec-log-supch",
    "title": "2  Logistics",
    "section": "2.11 Supply Chain",
    "text": "2.11 Supply Chain\n\nMisc\n\nNotes from: 4 Smart Visualizations for Supply Chain Descriptive Analytics\n\nFlow Distribution of units between production areas and markets\n\nData \n\nSource: the production facility name (left-side)\nTarget: the market supplied (right-side)\nUnits: the number of items flowing (width of bars)\n\nInterpretation\n\nIndia is the biggest country for production output\nJapan market demand is mainly supplied locally\nUSA and Germany do not have local production facilities\n\n\nNetwork Optimization\n\nx-axis: each column represents a demand scenario (i.e. there are 50 demand scenarios in this example)\ny-axis: are the production/supply locations\nA blue box means that that location is included in the optimal configuration of locations for that scenario\n\ne.g. In scenario 1, having a low capacity facility in India and a high capacity facility in India is optimal for this scenario.\n\nI think this viz can be done with {waffle} using geom_wafflewithout theme_enhance_waffle\nSimulate how the variability of demand in various markets (e.g. 50 scenarios) affects the optimal distribution of production/supply locations\n\nHopefully a configuration of locations will be optimal for a preponderance of scenarios. Assuming each scenario is equally important, that configuration of locations is the optimal choice.\n\nOr I guess you could weight each scenario by frequency or something. Maybe you have a distribution of scenarios from which you drawing from.\n\n\nLinear programming\n\nAlso see Optimization, general\nSet decision variable, objective function\nlist the constraints according to the demand for each market\nSolutions are indicator variables for production/supply locations and whether they are 1 or 0.\n\nThere should be a boolean variable for a high capacity location and low capacity location in each country\nFor each variable, 1 indicates that location should be built or that it should be in operation at that particular capacity\n\n\n\nPareto Plot\n\nData\n\n“BOX” is the number of box/packs picked of that product (“SKU”) for that order (“ORDER_NUMBER”) on that date (“DATE_FORMAT”)\n\nPreprocessing\n\nSum the number of boxes picked per SKU\nSort your data frame by descending order on BOX quantity\nCalculate the cumulative sum of BOX\nCalculate the cumulative number of SKU"
  },
  {
    "objectID": "qmd/logistics.html#sec-log-warmang",
    "href": "qmd/logistics.html#sec-log-warmang",
    "title": "2  Logistics",
    "section": "2.12 Warehouse Management",
    "text": "2.12 Warehouse Management\n\nMisc\n\nPicking operations account for the largest proportion of the total warehouse costs (up to more than 60%). This is why the design of these areas is of such importance.\nThe closer the high demand or large goods are to the loading and unloading docks, the lower the handling costs.\n\nMaterial Flow Types\n\nSimple flows: To understand how these movements work, we can examine the simplest possible flow, which takes place when units sent by the supplier are used, without dividing these up.\nMedium flows: Movements start to become more complex with this type of flow. It is normally found in warehouses with single or combined picking operations, generally with the supply of full pallets.\nComplex flows: There are warehouses with different working areas, depending on the types of product and their consumption. They normally have intermediate handling areas and can require various operations that in turn need flows of a certain (and at times great) complexity. This diagram shows an example of this type of facility and the loading movements that occur there.\n\nWarehouse Optimization\n\nA: High Rotation, B: Medium Rotation, C: Low Rotation\n(Left) Pareto Plot shows how High Rotation products are classified as those accounting for 20% of total products but also 80% of sales (point on the curve)\n(Right) Shows how the “A” products have been positioned closest to the loading and unloading area."
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-misc",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-misc",
    "title": "Low Hanging Fruit",
    "section": "Misc",
    "text": "Misc\n\nMost companies don’t have a holistic view of their business performance\n\nLarge proportion of the budget is spent on campaigns that generate little to no return\nMonitoring business performance in real-time and swiftly halting initiatives with negative returns will give a company more time and cash to survive the unstable economy.\n\nWhy a business needs statistics\n\nIdentifying Opportunities\n\nFind new markets, promote better customer retention, increase sales, and identify sales opportunities\nIncrease efficiency by finding duplication in the market or pinpointing areas that you want to eliminate from your current strategic plan\n\nUnderstanding customer behavior\n\nLooking at their buying patterns and how they use your products or services\nCan make decisions on the type of products or services you should offer to your customers\nIdentify new opportunities for product development by looking at areas that may require further research and study\n\nDetermining the correct target market\n\nIdentify the best possible choice for your business because all decisions must be made around this key area\nHelp determine whether your current target market is as profitable as it should be\n\nEvaluating products or services\n\nDetermine what your customers are using or how they are accessing your products to find new ways to improve or alter a product or service you offer\n\nMaking better decisions\n\nAllows you to make better decisions about business changes, hiring new employees, marketing, and advertising strategies"
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-wyo",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-wyo",
    "title": "Low Hanging Fruit",
    "section": "What you offer",
    "text": "What you offer\n\nSee Job, Organizational and Team Development\nIn the beginning the value comes from creating a more automated, streamlined, and reliable process and developing metrics that better measure aspects of the business.\nInsights are an iterative process which depends heavily on the quality, quantity, and predictiveness of the data\nA data scientist automates and improves data-centric processes such as collecting, storing, accessing, analyzing, and reporting."
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-raaa",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-raaa",
    "title": "Low Hanging Fruit",
    "section": "Report Automation and Alerts",
    "text": "Report Automation and Alerts\n\nSee Job, Reports\nAutomate daily, monthly, quarterly etc. reports\nTypical components: analysis, report template, automation script, shiny dashboard\nAlerts\n\nSend text or email (i.e. RPushbullet (text) or blastula (email))\nUseful when dashboards designed to monitor things, aren’t being used. This way a manager is only bothered when something is wrong\nExamples\n\nWhen an overtime limit is reached, send alert to department head\nWhen inventory reaches a limit, send alert"
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-churn",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-churn",
    "title": "Low Hanging Fruit",
    "section": "Churn",
    "text": "Churn\n\nSee Algorithms, Marketing &gt;&gt; Churn\nWhy are customers leaving and how much is it costing\nLogistic regression if just a probability of whether a customer will churn is wanted\nDecision tree if it needs to presented to an executive or stakeholder not familiar with statistics"
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-mom",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-mom",
    "title": "Low Hanging Fruit",
    "section": "Monitoring Operational metrics",
    "text": "Monitoring Operational metrics\n\nSee\n\nJob, Organizational and Team Development &gt;&gt; Developing a data strategy &gt;&gt; OKRs\nKPIs\n\nThis is about understanding the levers that drive your business, then using them to improve operations. A key aspect is making data available and understandable to those who are making daily decisions.\nMonitoring business performance in real-time allows businesses to swiftly halt initiatives with negative returns\nMetrics are also ways for organizations to align stakeholders around one vision of the world and one common goal.\nMetrics help companies have clarity, alignment and prioritization in what to build.\nMetrics help a company decide how to build the product once they’ve prioritized what to build.\nMetrics help a company determine how successful they are and hold them accountable to an outcome.\nExamples:\n\nDaily updates about key metrics.\nIs there a drop in conversion rate?\nAre we meeting our KPIs?"
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-pdf",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-pdf",
    "title": "Low Hanging Fruit",
    "section": "Product Demand Forecasting",
    "text": "Product Demand Forecasting\n\nSee Logistics &gt;&gt; Demand Forecasting\nhttps://www.remixinstitute.com/blog/automated-demand-forecasts-using-autocatboostcarma-in-r/#.XX_q6ShKi1s (some pitch material)\n“Over 20% of Amazon’s North American retail revenue can be attributed to customers who first tried to buy the product at a local store but found it out-of-stock”\nA demand forecasting system can help save a lot of dollars in term of workforce management, inventory cost and out of stock loss.\n\nSee inventory bkmk folder, SCPerf pkg. Has inventory/supply chain functions that utilize demand forecast as a variable.\n\nSales forecasts by store\nMonthly units sold by SKU (item)\neCommerce\n\nDaily or Weekly Visits by Channel, Source, and/or Medium using Google Analytics data\nDaily Customers, New Customers, Revenue, and Units Sold by Channel"
  },
  {
    "objectID": "qmd/low-hanging-fruit.html#sec-lhf-custseg",
    "href": "qmd/low-hanging-fruit.html#sec-lhf-custseg",
    "title": "Low Hanging Fruit",
    "section": "Customer Segmentation",
    "text": "Customer Segmentation\n\nSee Marketing &gt;&gt; Workflow &gt;&gt; Find the Ideal Customer Profile (ICP) and target them\nIncreases marketing efficiency by helping to indicate which campaigns are more likely to succeed with certain groups of customers\nThe company can achieve a higher return on ad spend with a smaller marketing budget, yielding a higher profit margin and more room for market expansion.\nForm marketing hypotheses based on cluster characteristics and test these hypotheses by varying campaigns based on the customer’s cluster membership.\n\nRecommend a product they’re likely to purchase, a multi-buy discount, or on-boarding them on a loyalty scheme (e.g. rewards program)\nOnce you know who your customers are and what their value is to your business, you can:\nPersonalize your products and services to better suit your customers’ needs\nCreate Communication Strategies tailored to each segment\nFocus Customer Acquisition to more profitable customers with messages and offers more likely to resonate with them\nApply Price Optimization to match customer individual price sensitivity\nIncrease Customer Retention by offering discounts to customers that haven’t purchased in a long time\nEnhance Customer Engagement by informing them about new products that are more relevant to them\nImprove your chance to Cross-sell and Up-sell other products and services by reaching out for the right segment when they’re more likely to respond\nTest which type of incentive a certain segment is more likely to respond to (e.g. pricing discounts, loyalty programs, product recommendation, etc.)"
  },
  {
    "objectID": "qmd/manufacturing.html#sec-manuf-misc",
    "href": "qmd/manufacturing.html#sec-manuf-misc",
    "title": "Manufacturing",
    "section": "Misc",
    "text": "Misc"
  },
  {
    "objectID": "qmd/manufacturing.html#sec-manuf-distrmanuf",
    "href": "qmd/manufacturing.html#sec-manuf-distrmanuf",
    "title": "Manufacturing",
    "section": "Distributed Manufacturing (DM)",
    "text": "Distributed Manufacturing (DM)\n\nNotes from Online Scheduling Approach for Distributed Additive Manufacturing\nGlobal Mass Production vs Global Distributed Production\n\nDynamic Allocation of Production Orders (POs)\n\n\nProduction order inputs delivery location, spare part to be manufactured, quantity needed\nThe only thing being locally modeled seems to be production time\n\nTravel Time from PC to Delivery Location is a request from an API\n\nSeems like there should be a cost element here. Freight charges if delivery is outside the company. If manufacturing and delivery is local though, then maybe the only real cost is gas and that would indirectly included in the Travel Time calculation\n\nQueue time should be something that’s monitored and can be looked up in a table\nSet-up time should be something that’s considered a constant depending on the spare part\n\n\nImplementation\n\n\nFASTEN is a manufacturing software company\n\nNot sure if this tool is used to feed simulated POs to the pipeline or if it’s used to optimize the objective function or both\n\nComponents in this project make sense to me but these arrows don’t all make sense to me\n\nBenchmark\n\nThe null model was one where the PC with shortest distance from PC to delivery location was chosen every time\nThe metric was Average Wait Time to receive the spare part after being ordered"
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-misc",
    "href": "qmd/marketing.html#sec-mark-misc",
    "title": "Marketing",
    "section": "Misc",
    "text": "Misc\n\nNotes from\n\nhttps://hbr.org/2021/07/why-you-arent-getting-more-from-your-marketing-ai?ab=seriesnav-spotlight\nhttps://hbr.org/2021/07/how-to-design-an-ai-marketing-strategy?utm_source=Data_Elixir&utm_medium=social\n\nMost marketing AI addresses segmentation, targeting, and budget allocation.\nTools to engage customers: Salesforce, Marketo, Braze, Facebook Ads, or Google Ads\n\nGoogle and Facebook are perennially at the top of charts for highest return on ad spend\n\nwww.singular.net may be useful resource to check from year to year\n\nExample: run a Facebook Ad campaign for these at-risk customers\n\nManual: Create a SQL query, pull a list from the data warehouse, download it to CSV, and upload it into Facebook Ads or build an integration\nAutomate:\n\nReverse ETL\n\nI think this process is described in the “Manual” part (above)\n\nPlatforms: Flywheel\n\n\n\nFacebook enables smart mobile user acquisition for mobile brands with 2 methods:\n\nApp Event Optimization (AEO)\n\nLooks for new-to-you people who are similar to customers you already have in your apps at defined stages\n\nValue Optimization (VO)\n\nLooks for people who will spend a certain amount of money in your mobile app"
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-terms",
    "href": "qmd/marketing.html#sec-mark-terms",
    "title": "Marketing",
    "section": "Terms",
    "text": "Terms\n\nAction-Based - Clicks, impressions, and GRP data\nAdstock - An average decaying function for the carryover effect (see Channel Attribution section)\nAdvanced Mobile Measurement (AMM) - Facebook’s tracker. Allowed advertisers to access click-through conversion data through their MMP\nAffiliate marketing - A type of performance-based marketing in which a business rewards one or more affiliates for each visitor or customer brought by the affiliate’s own marketing efforts.\nCarryover Effect - The lag between the time a consumer is touched by an ad and the time a consumer converts because of the ad\nClick-in - When a user click on something to reach your website/app\nClick-out - The last click a user makes that take them to another website/app\nClearing Price - The final price paid for an impression.\nCPC - Cost Per Click - refers to the cost an advertiser pays each time an online user clicks on his or her digital ad\nCPM - Cost Per Thousand Impressions is the amount advertisers are willing to pay for every one thousand impressions they receive\n\\[\nCPM = \\frac{\\text{Campaign Budget} \\cdot 1000}{\\text{Number of Desired Impressions}}\n\\]\neCPM or effective CPM is the predicted revenue earned by a publisher for every one thousand impressions; metric for ad testing\n\\[\neCPM = \\frac{\\mbox{Estimated Earnings} \\cdot 1000}{\\mbox{Total Impressions}}\n\\]\n\nFactors that affect eCPM (for all, more is better)\n\nMonthly website traffic\nNumber of ad networks\n\nFind ad networks offering better deals](https://www.adpushup.com/blog/the-best-ad-networks-for-publishers/) for different geographical locations\n\nViewability Score\n\nMultiple links in this article\n\n\n\nFloor prices - Are traditionally used by publishers to increase the closing price of their auctions. Think these are implemented by publishers with second-price auctions when they feel reductions are too large.\nGRP - Gross Rating Point. A standard measure in advertising, it measures advertising impact.\n\nOne GRP is one percent of all potential adult television viewers (or in radio, listeners) in a market\nCalculatations:\n\nPercent of the Target-Market-Reached multiplied by the Exposure Frequency.\n\\[\n\\begin{align}\n&GRP (\\%) = \\frac{100 \\cdot \\mbox{Impressions} \\;(\\#)}{\\mbox{Defined Population}\\; (\\#)} \\\\\n&GRP (\\%) = 100 \\cdot \\mbox{Reach}\\; (\\%) \\cdot \\mbox{Average Frequency}\\; (\\#)\n\\end{align}\n\\]\nExamples:\n\nif you get advertise to 30% of the target market and give them 4 exposures, you would have 120 GRP.\nIf an average of 12% of the people view each episode of a television program, and an ad is placed on 5 episodes, then the campaign has 12 × 5 = 60 GRPs.\nIf 50% view three episodes, that’s 150 GRPs\n\n\n\nHeader Bidding - Just the name for technical auctioning process behind bidding on ad space on a website.\n\nUsed to be done by waterfall archetecture where ads would pass from one publisher’s website to another until it reached one with a price floor that was below the advertiser’s bidding price\n\nIdeal customer profile (ICP) - Customers who bring in the most long-term value for a company\nIdentifier for Advertisers (IDFA) - a random device identifier assigned by Apple to a user’s device. Advertisers use them to precisely target and track users within apps on iOS devices\nImpression - An instance of each time your ad is shown on a search result page or other site on the Google Network. (i.e. number of people your ad reaches)\n\nEach time your ad appears on Google or the Google Network, it’s counted as one impression.\nIn some cases, only a section of your ad may be shown. For example, in Google Maps, we may show only your business name and location or only your business name and the first line of your ad text.\nYou’ll sometimes see the abbreviation “Impr” in your account showing the number of impressions for your ad.\n\nInventory - the amount of ad space (or the number of advertisements) that a publisher has available to sell. While the term originated from print, it has grown to encompass ad space on the web and on apps and mobile ads\nPrice Floor - The minimum price a publisher will accept for its inventory — ignoring all bids below that price.\nReduction - Money saved in a second-price auction; difference between the bid price and the clearing price\nRPM - similar to eCPM; it’s the amount earned by publishers per thousand pageviews\nTouchpoint - any time a potential customer or customer comes in contact with your brand–before, during, or after they purchase something from you. Interactions with marketing campaigns and the home page provide rich information about who they are and what they like\n\nExamples:\n\nBefore Purchase: Social media, Ratings and reviews, Testimonials, Word of mouth, Community involvement, Advertising, Marketing/PR, visits your website\nDuring Purchase: Store or office, Website, Catalog, Promotions, Staff or sales team, Phone system, Point of sale\nPost Purchase: Billing, Transactional emails, Marketing emails, Service and support teams, Online help center, Follow ups, Thank you cards\n\n\nUTM - Urchin Traffic Monitor - used to identify marketing channels or results from ad campaigns\n\nName comes from Urchin Tracker, a web analytics software that served as the base for Google Analytics.\ne.g. http://yourwebsite.com/your-post-title/?utm_source=google\n\nutm code = string after “?”\n\nseparate each UTM parameter with the ‘&’ sign.\n\nThis person clicked a google ad to get to your site\n\nGoogle URL builder tool\nSee article for me details, best practices, etc.\nParameter types\n\nutm_source - traffic source (e.g. google, facebook, twitter, etc.)\nutm_medium - type of traffic source (e.g. CPC, email, social, referral, display, etc.)\nutm_campaign - campaign name, track the performance of a specific campaign\nutm___content - In case you have multiple links pointing to the same URL (such as an email with two CTA buttons), this code will help you track which link was clicked (e.g utm_content=navlink )\nutm_term - track which keyword term a website visitor came from. This parameter is specifically used for paid search ads. (e.g. utm_term=growth+hacking+tactics)"
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-usecases",
    "href": "qmd/marketing.html#sec-mark-usecases",
    "title": "Marketing",
    "section": "Use Cases",
    "text": "Use Cases\n\nChatbots for lead development, customer support, and cross-selling or upselling\nInbound call analysis and routing, and customer comment and email analysis, classification, and response\nMarketing campaign automation (including emails, landing page generation, and customer segmentation)\nChannel Attribution\n\nMarketing mix modeling (MMM)\nMulti-touch Attribution Modeling (MTA)\nUnified Approach Online product merchandising\n\nPricing Product or service recommendations and highly personalized offers\nProgrammatic digital ad buying ( digital ads are served up almost instantaneously to users)\nSales lead scoring\nSocial-media planning, buying, and execution\nSocial-media sentiment analysis\nTelevision ad placement (partial)\nWeb analytics narrative generation\nWebsite operation and optimization (including testing)\nSales propensity models in customer relationship management (CRM) systems\nAutomated Tasks\n\nSend a welcome email to each new customer\nChatbots on social media platforms\n\nStart with a stand-alone non-customer-facing task-automation app, such as one that guides human service agents who engage with customers.\n\nLess-capable bots can irritate customers. It may be better to have such bots assist human agents or advisers rather than interact with customers.\n\n\nOnce companies acquire basic AI skills and an abundance of customer and market data, they can start moving from task automation to machine learning\n\nApps\n\nStand-alone applications continue to have their place where integration is difficult or impossible, though there are limits to their benefits.\nExample\n\nUsing IBM Watson’s natural language processing and Tone Analyzer capabilities (which detect emotions in text), the application delivers several personalized Behr paint-color recommendations that are based on the mood consumers desire for their space. Customers use the app to short-list two or three colors for the room they intend to paint.\n\n\nIntegrated ML/DL\n\nNetflix - recommendations are on a separate standalone app where the customer needs to request recommendations\nML that coaches a customer service rep through a call"
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-metrics",
    "href": "qmd/marketing.html#sec-mark-metrics",
    "title": "Marketing",
    "section": "Metrics",
    "text": "Metrics\n\nConversion Rates, Bounce Rates, and Average Basket Sizes\nReturn on Ad Spend (ROAS)\n\\[\n\\mbox{ROAS} = \\frac{\\mbox{Sales Revenue}}{\\mbox{Advertising Budget Spend}}\n\\]"
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-custlift",
    "href": "qmd/marketing.html#sec-mark-custlift",
    "title": "Marketing",
    "section": "Conversion Lift Tests",
    "text": "Conversion Lift Tests\n\nAlso see\n\nData Science &gt;&gt; Algorithms, Marketing\nData Science &gt;&gt; Business Plots\n\nRCTs on a customer sample that tries to understand the impact of an ad campaign by randomly showing it to one group of users, with-holding it from another, and looking for a difference in behaviour between the two groups over some predetermined period of time.\n\nExample:\n\nA typical ecommerce brand might want to understand whether their Facebook ads are driving sales that wouldn’t have happened anyway, and so they’d want to track purchases as a KPI in their lift test.\n\n\nGold standard of incrementality measurement\n\nIncrementality Tests also help with re-engagement strategies to highlight the optimal day, post-install, to re-engage users and to ensure the highest incremental lift from your marketing efforts\n\nAlternatives\n\nA/B tests\n\nNo way to measure incrementality\nThe difference seems to be semantics IMO.\n\nWith A/B, the control group gets a “different treatment” and with conversion lift tests, the control group gets no treatment\n\n\nBrand Lift Tests\n\nseek to measure a campaign’s impact on brand metrics, not conversions\n\nGeo-experiments (see below))\n\nCapable of measuring incrementality. Viable alternative to lift tests\n\n\nHow do you not serve ads to an audience (i.e the control group), yet still “own” the ad real-estate?\n\nMethodologies:\n\nIntent-to-treat (ITT) – This method calculates the experiment results based on the initial treatment assignment and not on the treatment that was eventually received (meaning you mark each user for test/control in advance and do not rely on attribution data. You have the “intent” to treat them with ads / prevent them from seeing ads, but there’s no guarantee it will happen).\nGhost ads/bids – This is another example of a randomly split audience, but this time it is done just before the ad is served. The ad is then withheld from the control group, simulating the process of showing the ad to the user, known as ad serving, without paying for placebo ads. This is a tactic mostly used by advertising networks carrying out their own incrementality tests.\nPublic service announcements (PSAs) – These are in place to show ads to both the test and control group however, the control group is shown a general PSA while the test group is shown the variant. The behaviors of users in both groups are then compared to calculate incremental lift.\n\n\nCost per Incremental Conversion (CPiP)\n\nMetric used to determine whether the treatment’s effect (e.g incremental conversion) is enough to warrant implementing the treatment in production\n\nHigher is worse\n\nExample:\n\neCommerce brand spent $100k on their campaign.\nThey measured\n\n7,500 sales from their campaign’s treatment group (both during and for some fixed time after the campaign)\n5,000 sales from their campaign’s control group\n7500 - 5000 = incremental sales\n\nCPiP = $100,000 / (7500 - 5000) = $40\n\nIf the CPiP &lt; Customer Lifetime Value (CTV) (See Algorithms, Marketing &gt;&gt; Customer Lifetime Value) and this margin is acceptable at which to acquire customers, then the treatment (e.g. ad campaign) can move to production\n\nIncremental Return on Advertising Spend (iROAS)\n\\[\n\\mbox{iROAS} = \\frac{\\mbox{Treatment Group Revenue} - \\mbox{Control Group Revenue}}{\\mbox{Treatment Spend}}\n\\]\n\nLess than 100% you can redistribute budgets to better-performing campaigns and channels\nEqual to or higher than 100% you know you are not cannibalizing organic traffic and that your ads are effective"
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-geoexp",
    "href": "qmd/marketing.html#sec-mark-geoexp",
    "title": "Marketing",
    "section": "Geo-experiments",
    "text": "Geo-experiments\n\nA quasi-experimental methodology where non-overlapping geographic regions (geos) are randomly assigned to a control or treatment group\n\nExample: ads are served only in the geos of the treatment group while users in geos of the control group won’t be exposed to the advert\n\nAll the major ad networks/tracking platforms allow for targeting ads to the relevant level of location (neighbourhood, city, state, etc.)\n\nConversions are measured at a geo-level.\n\nSteps\n\nDecide geo-level based on market geography\n\ncountry \\(\\rightarrow\\) states\nstate \\(\\rightarrow\\) zip codes, Designated Market Area (DMA)\nDon’t select geos that are too small as people may travel across geo-boundaries and the volumes of conversions may be too low\n\nPerform a preliminary analysis of the geos\n\nDetermine factors that differ between geos that may influence the experiment\nFind geos most similar to each other so the experiment isn’t biased even after randomization\n\nAssignment\n\nRandomly assign treatment and contol to geos\n\nAnalysis\n\nUse difference in differences or synthetic controls to measure incremental conversions\nExample DiD"
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-chanatt",
    "href": "qmd/marketing.html#sec-mark-chanatt",
    "title": "Marketing",
    "section": "Channel Attribution",
    "text": "Channel Attribution\n\nMisc\n\nAlso see UTM parameters in Terms on how to use URLs to track different ad accounts.\n\nWhen you start tracking the UTM parameters and tying it to the account creation, you can now measure the conversion rate from ad click to purchase.\nThe main KPI is now end-to-end customer acquisition cost, rather than cost per click.\n\nModel should reflect these characteristics\n\nThe concave shape of diminishing returns\n\n\nAs the market becomes saturated by your ads, additional ad spend generates less return\nsales ~ ln(spend) and you’d have make sure you change the 0s to a really small number or add 1 , e.g y = ln(x+1).\n\nBy adding 1, zeroes will be zeros on the log-scale since log 1 = 0\n\n\nCarryover Effect\n\nThe lag between the time a consumer is touched by an ad and the time a consumer converts because of the ad.\nAdstock is a decaying function that describes the average carryover effect\n\n\nThe advertising effect of this example channel was about 40% in the initial week, then decrease to 20% in the following week, and so on\n\nNot sure how this “effect” is measured. Maybe it means a 40% increase in conversions during the initial week of the ad buy. Maybe this is measured experimentally. Maybe it’s something like a ratio of conversions after clicks on digital ads/paid search to total clicks on the ad.\n\nCalculate a “spend_with_adstock” variable\n\n\nDecay rate is specific to an ad channel and business type\n\nOnline ads have a more immedicate effect since users are usually ready to buy\nOffline ads have a longer delay since it takes time to respond\n\n\n\n\n\nTypically dealing with sparse data\n\nThere can be a big spike in spending due to a new launch but no consistent spending afterward.\nFor certain channels, the budget could be turned off for some time and back on due to market dynamics or business strategy changes.\nIn some cases, we can pool together smaller locations that are believed to behave similarly to each other and estimate them using a hierarchical Bayesian model (the same logic can be applied to similar channels)\n\nIssues\n\nSelf-selection/endogeneity\n\nExample: Paid Search\n\nA user already has the brand she wants to purchase in mind and searches the brand name online. Then she clicked the paid search ad and made a purchase. This would incur ad spend, however, the purchase would not be incremental sequence of steps (i.e customer journey) because she would have purchased anyway. So the ad didn’t induce the purchase since the user was already ready to purchase before encountering the ad.\n\nUpper funnel channels can get less credit than downstream channels\n\ne.g. A user saw an ad on TV first and wanted to make a purchase online. He then searched for the product on Google and bought it from the paid search ad. The model could attribute more credit to the search if not treated well.\n\nSolutions\n\nInformative priors from reliable experimentation on channels with high selection bias to guide the model and prevent the model from being biased towards endogenous channels.\nInstrumental variables to better control for the bias (although instrumental variables may be hard to find or construct)\nSelection Bias Correction approach developed by Google (Paper).\n\nGoogle team used DAGs of the search ad environment and derived a statistically principled method for bias correction\nAdjusted for search queries to close backdoor paths.\nAfter the adjustment, the coefficient for the search channel is much less than that from the naive model and is aligned with the result from the experimentations.\n\n\n\nMulti-Collinearity\n\ne.g. Two channels might be launched together to reinforce each other or to support a product launch\nSolutions: variable reduction techniques: PCA, ElasticNet, etc.\n\n\n\n\n\nUnified Approach\n\nUse experimentation results as priors for MMM and use MTA results to validate and calibrate MMM\n\nExperiment examples: Conversion Lift Tests, Geo-Experiments, A/B Tests, RCTS, etc.\nUse trustworthy priors obtained from experiments or external sources to inform a bayesian model\n\nA bayesian model with time-varying coefficients can handle violatility of marketing effects (particularly for new companies)\n\n\n\n\n\nMarketing Mix Modeling (MMM)\n\nSales (or ) ~ Channel_Spend (+ Trend + Seasonality(s) + Campaigns + Events + Holidays + Weather +… etc.)\n\nOther outcome variables\n\nKPI or google search volume or website visits or account sign-ups\n\nOther explanatory variables:\n\nmedia cost for each channel\nproduct pricing\neconomic indicators\nsynergistic effect between different media (i.e. interaction)\ncompetitor activities\n\n\nBetter for measuring performance of offline traditional Out-of-Home (OOH) media like TV, radio and print which is unlike online advertising where we have access to metrics like clicks, clickthrough rate, impressions, etc.\nRequires relatively large budgets and longer data history to have reliable reads. Sparse data and short data history could make the results biased and unstable\n\nTypically requires at least 2 years of weekly data (longer is better) and a good volume of media spend.\nFor small data situations, reduce the number of features by combining smaller channels and prioritizing bigger channels\n\nTends to underestimate upper funnel channels and overestimate lower funnel channels.\nLess likely to get more granular level insights, and will not show the nuances of the user journey\n\n\n\nMulti-Touch Attribution Modeling (MTA)\n\nAlso see\n\nMulti-touch attribution: The fundamental to optimizing customer acquisition (only skimmed, haven’t taken notes yet)\n\nUnlike MMM, it acknowledges that the customer journey is complex and customers may encounter more than one (or even all) of our marketing channel activities in their journey\nRequires metrics like clicks, clickthrough rates, impressions, etc.\n\nTherefore, only suited for digital media\nMTA data aims to model the entire user ad journey. However, in reality, the real-world data can be partial and only includes part of the touch points due to tracking difficulty.\n** MTA is also subjective to user data privacy initiatives, such as Apple no IDFA and Facebook no AMM. In the foreseeable future, Google will also join the force. Without user-level tracking data, MTA models cannot be built. **\n\nMost MTA methods utilize click data, not impressions, which tends to give more credit to more clicky channels, like search. This can also make the analysis biased.\nWeighting is assigned to these channels based on their touchpoints to determine each channel’s contribution / involvement towards the sale\n\nWeighting methods:\n\nLast Click Attribution and First Click Attribution\n\nassigns 100% weighting to the last/first channel that was clicked on\nNaive default model used on many analytics platforms\n\nGoogle switched from this model to “data driven attribution” (see below) in Google Analytics 4 (GA4)\n\n\nTime Decay - weighting is distributed based on the recency of each channel prior to conversion. The most weighting will be given to the last channel prior to conversion (PPC). The least weighting is assigned to the first channel.\nPosition-Based - the first and last channels each receive 40% of the weighting, with the remainder 20% distributed equally to the middle channels.\n\nSee https://support.google.com/analytics/answer/10596866?hl=en#zippy=%2Cin-this-article  for explanations of other rules based models in GA4\n\nData-Driven Attribution (DDA) - weighting is assigned objectively by an algorithm based on the probability of conversion, given the touchpoints. Methods like\n\nGoogle Search Ads 360 - Markov Chain, game-theory approaches using Shapley values\nGA4\n\nAnalyze the available path data to develop conversion rate models for each of your conversion events\nUse the conversion rate model predictions as input to an algorithm that attributes conversion credit to ad interactions\n\n\n\n\n\n\n\nMobile Channel Attribution\n\nAdvertising Identifiers - helps mobile marketers attribute ad spend.\n\nGoogle’s advertising identifier (GAID)\nExample:\n\nWhen a company like Lyft or Kabam runs user acquisition campaigns to gain new mobile customers, a mobile measurement partner like Adjust, Singular, Kochava, or AppsFlyer can help them connect a click on an ad with an eventual app install on a specific device. That helps Lyft know that an ad worked, and that whatever ad network they used for it succeeded.\nif the person who installed that app eventually signs up for an account and takes a ride share, Lyft knows where and how to attribute the results of that marketing effort, and connect it to the ad spend that initiated it. Even better, from Lyft’s perspective, it can use the IDFA to tell mobile ad networks essentially: I like users like this; go find me more.\n\nApple’s SKAdNetwork \n\nPrivacy-safe framework for mobile attribution\nAllows advertisers to know which ads resulted in desired actions without revealing which specific devices — or which specific people — took those desired actions.\nYou go to an ad network like Vungle or AdColony or Chartboost — or Facebook or Google, for that matter — and kick off an ad campaign. They show your ads to potential new mobile customers, and when one clicks on it and downloads your app from the App Store, Apple itself will handle sending a cryptographically signed notification — a postback — to the ad network. That postback will not include any user or device-specific information, so while it will validate the conversion for marketing purposes, it won’t reveal any personal information of your new app user.\nCan’t be used in Conversion Lift Tests\n\nPostbacks require a ad campaign ID for the conversion to be attributed to, so it’s not possible to measure the conversion volume from a lift test’s control group (since the control group isn’t exposed to the ad)"
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-sem",
    "href": "qmd/marketing.html#sec-mark-sem",
    "title": "Marketing",
    "section": "Search Engine Marketing (SEM)",
    "text": "Search Engine Marketing (SEM)\n\nMarketing on search engines where you pay to place ads at the top so that the user sees your ads before the organic search results (e.g. Google, Bing, etc.)\nMost SEM ads work on an auction system, where you have to place a bid on each of the search terms (can be millions of terms) relevant to your business and if you win a position in the auction(there are usually 2–3 SEM ads per search, or more based on your region and the search engine you are using), then you would pay the cost equal to your bid or lower depending on which auction system the search engine follows.\nAuctions\n\nTypes\n\nFirst Price Auctions: A model wherein the buyer pays exactly the price they’ve bid on any given advertising impression. (greater transparency)\nSecond-Price Auctions: A model wherein the buyer pays $0.01 more than the second highest bid for an ad impression.\n\nExample: 3 bidders\n\nBids\n\nBidder A $2.20\nBidder B $2.80\nBidder C $2.50 first-price auction: B wins\nClearing Price will be the same as the bid- $2.80\nSecond Price auction: B wins\nClearing Price = $0.01 + Decond-Highest Bid ($2.50) = $2.51\nReduction = $2.80 (Winning Bid) - $2.51 (Clearing Price) = $0.29\n\n\n\nBid Shading\n\nTechnique buyers use in first-price auctions in an attempt to avoid overpaying\nTakes a maximum possible bid and tries to forecast the market value for a given impression, in order to determine the actual bid price to submit.\n\nOutcome: bid price; model features: site, ad size, exchange and competitive dynamics\nIf win rates decrease, the algorithm raises the price they pay\n\nSome publishers (google, bing, etc.) use intelligent price floors to counteract bid shading\n\nOptimize bidding for value vs traffic volume characteristics of key words\n\nSparsity Issues (i.e. Zero-Inflation)\n\nDue to the long tail nature of the search terms (some search terms are more popular than some other obscure ones)\nDue to the conversion ratio itself — not every click from these SEM ads converts to a purchase\nWhat should the outcome variable be? (binary: conversion/no_conversion or numeric: bid value)\n\nOther considerations\n\nHow does the effect of a competitor’s bid change based on the type of auctioning method?\nSeasonality?\n\nPre-corona vs post-corona?\n\nIs there an optimal level of investment?"
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-custseg",
    "href": "qmd/marketing.html#sec-mark-custseg",
    "title": "Marketing",
    "section": "Customer Segmentation",
    "text": "Customer Segmentation\n\nMisc\n\nAlso see\n\nCustomer Journey\nAlgorithms, Product &gt;&gt; Customer Journey\n\nSeeks to answer:\n\nWho are the most valuable customers?\nWhere do they come from?\nWhat do they look like?\nWhat and how do they like to buy?\n\nBenefits\n\nThe company can achieve a higher return on ad spend with a smaller marketing budget, yielding a higher profit margin and more room for market expansion.\nIncreases marketing efficiency by helping to indicate which campaigns are more likely to succeed with certain groups of customers\n\n\n\n\nProcess\n\nForm marketing hypotheses based on cluster characteristics and test these hypotheses by varying campaigns based on the customer’s cluster membership.\n\ne.g. Recommend a product they’re likely to purchase, a multi-buy discount, or on-boarding them on a loyalty scheme (e.g. rewards program)\n\nOnce you know who your customers are and what their value is to your business, you can:\n\nPersonalize your products and services to better suit your customers’ needs\nCreate Communication Strategies tailored to each segment\nFocus Customer Acquisition to more profitable customers with messages and offers more likely to resonate with them\nApply Price Optimization to match customer individual price sensitivity\nIncrease Customer Retention by offering discounts to customers that haven’t purchased in a long time\nEnhance Customer Engagement by informing them about new products that are more relevant to them\nImprove your chance to Cross-sell and Up-sell other products and services by reaching out for the right segment when they’re more likely to respond\nTest which type of incentive a certain segment is more likely to respond to (e.g. pricing discounts, loyalty programs, product recommendation, etc.)\n\n\n\n\nRFM Analysis\n\n\nRecency, Frequency, and Monetary Value Analysis\n\n\nData\n\nJust need a transactional database with client’s orders over time (at least 2.5 to 3 yrs of data to capture enough behavior variation).\n\n\ntime frame could be monthly or yearly or any other time frame required by the business\n\nExplicitly creates sub-groups based on how much each customer is contributing.\n\n\nRecency – How recently did the customer purchase?\n\nInvoice date can be used to calculate the recency\nNumber of days since the last purchase\n\nFrequency – How often do they purchase?\n\nInvoice numbers can be used to calculate the frequency\nNumber of invoices per month\n\nMonetary Value – How much do they spend?\n\nTotal price can be used to calculate the monetary value.\nTotal monetary value of all purchases in a month\n\n\n\n\n\nModeling\n\nAlso see\n\nAlgorithms, Marketing &gt;&gt; Customer Lifetime Value &gt;&gt; BG/NBD Gamma-Gamma model\nVideo: Bizsci\n\nlab-49-feature-engineering-customer-segmentation\nlab-58-customer-lifetime-value-rfm-calc\npython customer lifetime value, rfm + xgboost (youtube)\n\n\nClustering (k-means is popular)\n\nCluster Customer IDs based on Recency, Frequency, and Monetary Value variables\n\nAdd additional variables if relevant to helping you choose a market strategy\nIf you end up with too many variables for the amount of data you have, then perform PCA before clustering\n\n\n\n\n\nInterpret clusters and create strategy for each cluster\n\nExample\n\n\nCharts\n\nBoth charts have the same info, right shows the patterns better but you can’t read the values\nRFM variables were standardized between 0 and 100 before clustering\n\nCluster 0\n\nInterpretation — Customers who have not made purchases recently\nStrategy — Make offers to bring them back to purchasing Cluster 1\nInterpretation — Customers who have a high monitory value\nStrategy — Create a loyalty program so that they can continue spending more Cluster 2\nInterpretation — Customers who have not made purchases recently\nStrategy — Make offers to bring them back to purchasing Cluster 3\nInterpretation — Customers who are likely to churn\nStrategy — Retain them with exciting offers Cluster 4\nInterpretation — Regular customers\nStrategy — Create a loyalty program to keep them purchasing on a regular basis\n\n\n\n\n\n\n\nBehavioral Segmentation\n\nTries to understand how customers interact with your product or website\nMisc\n\nAlso see\n\nCustomer Journey\nProduct Development &gt;&gt; Behavioral Data\n\n\nData\n\nDemographics: location, age, gender, and income\nTouchpoints (see Terms): view-through, click-through, the time between each action, etc\n\nIf a user sees an ad on Instagram and quickly swipes over it, that probably indicates they are not very interested in it.\nOften impacted by advertisers’ programmatic push notifications\n\nCompany Website Navigation: visiting behaviors provide enlightening ideas about what a visitor is truly looking for and how they like to shop (See Customer Journey)\nPurchasing: what a user buys, their purchasing frequency and timing, purchasing price and discounts, etc.\nSocial media network: Is there an influencer within a customer’s network? (see bkmks &gt;&gt; Network Analysis)\nExample: Telecommunications\n\n\nCluster the data and interpret the clusters\n\nExample using the telecommunications data above\n\nThe green cluster (aka segment) is Digitally engaged customers.\n\nA market strategy could be to create a digital loyalty card and reward them based on the use of digital services. This will in turn also increase revenue for the company.\n\nThe blue cluster is moderately engaged with low tenure.\n\nA market strategy could be to offer discounts and convert them into long-term contracts.\n\nThe red cluster is basic customers with only phone service.\n\nA market strategy could be to educate them about the advantages of digital services and then upsell digital products."
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-custjourn",
    "href": "qmd/marketing.html#sec-mark-custjourn",
    "title": "Marketing",
    "section": "Customer Journey",
    "text": "Customer Journey\n\n\n\nTop Figure: Typical SaaS User Journey\nBottom Figure: General User Journey\n\nArea of the rectangle represents the amount of customers that haven’t dropped off and remain the funnel\nThe red arrow represents customers that drop off and don’t convert\nThe start of the arrow show the section of the rectangle that represents the amount customer that drop off from on point to the next.\n\nMisc\n\nAlso see Algorithms, Product &gt;&gt; Customer Journey\n\nPre Sale\n\nWhen potential customers are in the “consideration” phase and researching a product, AI will target ads at them and can help guide their search\n\nDetermine which customers are most likely to be persuadable and, on the basis of their browsing histories, choose products to show them\n\nUse extremely detailed data on individuals, including real-time geolocation data, to create highly personalized product or service offers\n\nDuring Sale\n\nUpselling and cross-selling and can reduce the likelihood that customers will abandon their digital shopping carts\n\nAfter a customer fills a cart, AI bots can provide a motivating testimonial to help close the sale—such as “Great purchase! James from Vermont bought the same mattress.”\n\n\nPost sale\n\nAI-enabled service agents (chatbots) triage customers’ requests—and are able to deal with fluctuating volumes of service requests better than human agents are\nCan handle simple queries about, say, delivery time or scheduling an appointment and can escalate more-complex issues to a human agent"
  },
  {
    "objectID": "qmd/marketing.html#sec-mark-wkflw",
    "href": "qmd/marketing.html#sec-mark-wkflw",
    "title": "Marketing",
    "section": "Workflow",
    "text": "Workflow\n\nFind the Ideal Customer Profile (ICP) and Target Them\n\n\nNotes from article\nCustomer Long Term Value\n\\[\n\\mbox{Customer's long-term value} = \\mbox{Revenue from the customer's repeated purchase} - \\mbox{customer's acquisition cost}\n\\]\nSegment in Negative, Low-Value, Medium-Value, and High Value cohorts\n\nSee Customer Segmentation &gt;&gt; RFM Analysis\nAlternatively, based on its sales and marketing attribution systems, the brand can calculate one-year revenue and acquisition costs for each customer (or visitor that doesn’t convert). With that, the brand quickly gets each customer’s value and decides who is worth its marketing budget.\nThe split between medium-value segment and high-value segment typically follows the 80/20 rule\n\nThe High-Value segment can be further segmented into three smaller cohorts using pca or clustering algorithms.\n\nSee Customer Segmentation &gt;&gt; Behaviorial Segmentation\nThe customer data used to cluster can include: interactions with marketing campaigns, browsing history, purchase history, records of using coupons, etc.\nDomain knowledge is usually required to meaningfully label these cohorts and develop marketing plans based on the cluster attributes\nExamples\n\nCohort 1 likes to buy the latest model, cohort 2 prefers to buy with discounts, and cohort 3 often makes purchases as gifts\nCohort 1 reacts positively to campaigns about cool geeky features. On the other hand, cohort 2 engages more with campaigns spotlighting the practical benefits of using a new gadget.\nCohort 1 trusts YouTubers who talk about the latest and greatest tech products and make purchases after watching unboxing videos. Cohort 2 likes to shop at Costco and buy gadgets at a discount\n\nPartnering with tech influencers helps the company attract cohort 1 while working with wholesale stores speeds up selling to cohort 2.\n\n\n\nUse experimentation on High-Value cohorts to test which acquisition/marketing campaigns have high ROAS.\n\n\nFor cohort 3, campaign A seems to have more success than campaign B\n\nFuture campaigns can now target high value cohorts with greater conversion and less expenditure.\n\n\n\nPotential Pitfalls\n\nMake sure the metric fits the business question\n\nExample: Churn\n\nAlso see Algorithms, Marketing &gt;&gt; Churn\nDon’t model who was most likely to leave, they should have asked who could best be persuaded to stay — in other words, which customers considering jumping ship would be most likely to respond to a promotion.\n\ni.e. Swing Customers - like politicians looking for swing voters because they are persuadable.\n\nIf you model the wrong objective, you can squander money on swaths of customers who were going to defect anyway and underinvest in customers they should have doubled down on.\n\nThink this has to be 2 stages\n\nFilter data before promotions over some window \\(\\rightarrow\\) model traditional churn \\(\\rightarrow\\) filter data after promotion \\(\\rightarrow\\) label which likely churns left and which ones didn’t \\(\\rightarrow\\) model churn with new churn labels using data after promotion because you want probability of churn given promotion\n\nSo you’d have 2 models: 1 to identify churners and 1 to identify swing customers from churners\nDoes a lift model help here?\n\n\nExample: Increasing user spend for a video game\n\nFinding features that increase time spent playing doesn’t efficiently raise revenue or maybe not at all if most of your players play for free.\n\ne.g. Increasing my time spent playing FOE doesn’t increase FOE’s revenue since I don’t pay for shit on that game.\n\nGuessing there’s a decent correlation between weekly total_time_spent_playing and overall revenue and they thought they could increase revenue by increasing total time_spent_playing. They should have been looking at correlations between features and spend_per_customer.\n\nMaybe just filter top spenders and see which features they prefer (i.e. correlated to)\n\n\n\nMake sure when you increase the accuracy of a model, it doesn’t hurt the bottom line due to a bad tradeoff\n\nConsider the consumer goods company whose data scientists proudly announced that they’d increased the accuracy of a new sales-volume forecasting system, reducing the error rate from 25% to 17%. Unfortunately, in improving the system’s overall accuracy, they increased its precision with low-margin products while reducing its accuracy with high-margin products. Because the cost of underestimating demand for the high-margin offerings substantially outweighed the value of correctly forecasting demand for the low-margin ones, profits fell when the company implemented the new, “more accurate” system.\n\nDetermine the optimal granularity of predictions\n\nConsider a marketing team deciding how to allocate its ad dollars on keyword searches on Google and Amazon. The data science team’s current AI can predict the lifetime value of customers acquired through those channels. However, the marketers might get a higher return on ad dollars by using more-granular predictions about customer lifetime value per keyword per channel.\n\n\n\n\nRefine the Business Question\n\nWhen defining the problem, managers should get down to what we call the atomic level — the most granular level at which it’s possible to make a decision or undertake an intervention.\nA good business question captures the full impact of the decision on the Profit and Loss (P&L), recognizes any trade-offs, and spells out what a meaningful improvement might look like.\nGradaute from an extremely vague question to a more finely grained question\n\n“How do we reduce churn?”\n“How can we best allocate our budget for retention promotions to reduce churn?”\n\nHas the retention budget been set, or is that something we need to decide?\nWhat do we mean by “allocate”?\nAre we allocating across different retention campaigns?\n\n“Given a budget of $x million, which customers should we target with a retention campaign?”\n\nDefine Missed Opportunity metrics\n\nIdentify sources of waste and missed opportunities\n\nWaste example: targeting a likely-to-churn customer with a promotion that is not persuadable\nMissed opportunity example: not targeting a likely-to-churn customer with a promotion that is persuadable\n\nCompare the distribution of success versus failure to quantify waste and missed opportunities.\nMeasure\n\nFor difficult to quantify situations use aggregate data\n\nExample of an approach for churn model example using aggregated data\n\ncustomers who received promotion: what’s the cost of the promotion incentive relative to the incremental lifetime value\n\ndifference or ratio?\n\ncustomers who didn’t receive promotion: what’s the lost profit associated with the nonrenewal of their contracts\n\n\n\n\nDetermine the cause of waste and missed opportunities\n\nIn an ideal world, what knowledge would you have that would fully eliminate waste and missed opportunities? Is your current prediction a good proxy for that?\n\nIn churn example, rather than the basic churn/no churn model, focusing on persuadability would have led to great improvements.\n\nDoes the output of your AI fully align with the business objective?\n\nChurn example: A persuadable user with low expected profitability should have a lower priority than a persuadable user with high expected profitability.\n\nAdditional factor that further optimizes the solution to the refined business question\n\n\nHow much are we deviating from the business results we want, given that the AI’s output isn’t completely accurate?\n\nQuantifying the errors of your model\nChurn example: the cost of sending a retention promotion to a nonpersuadable customer (waste) is lower than the cost of losing a high-value customer who could have been persuaded by the offer (missed opportunity). Therefore, the company will be more profitable if its AI system focuses on not missing persuadable customers, even if that increases the risk of falsely identifying some customers as being receptive to the retention offer."
  },
  {
    "objectID": "qmd/meteorology.html#sec-meteor-misc",
    "href": "qmd/meteorology.html#sec-meteor-misc",
    "title": "Meteorology",
    "section": "Misc",
    "text": "Misc\n\nExample: Extreme hot weather events in Western Europe-Atlantic region\n\nNotes from video, around ~1:07:03\nPapers\n\nGradient boosting with extreme-value theory for wildfire prediction\nSpatiotemporal wildfire modeling through point processes with moderate and extreme marks\n\nSee “Supplemental Content” tab for link to zip file with code — uses {INLA} for modeling.\n\n\nData: “ERA5 reanalysis data”\n\nMonthly\nTraining: 1979 - 2020\nValidation: 2020 - 2022 (simple train-test split?)\n\nGoals:\n\nPredict probability of an event (i.e. binary classification), intensity of event (ie regression), spatial dependence of events (i.e. spatial correlation)\nIdentify weather patterns driving extremes (i.e. predictors)\n\nGlobal thermodynamic (i.e.. climate change)\nLocal land surface: soil moisture or snow cover determin surface energy budget (i.e. wet soil buffers heat)\nRegional dynamic conditions: diabatic (clear skies), adiabatic warming (subsidence) or advection of warm air from anti-cyclonic circulation (atmospheric blocking)\n\n\nModel\n\nResponse:\n\nExtreme 2m daily temp anomalies over land\n\nPredictors\n\nAtmospheric blocking measured by variable, “Z500” (5-day avg)\nSoil moisture, “SM” (15-day avg)\n\nResolution\n\nThere’s a predictor for each 5.6 x 5.6 degree (lat-lon) grid points\n\nShows the Western Europe-Atlantic region (disregard the horizontal black line)\n\nThere’s a response variable for each 1.3 x 0.3 degree (lat-lon) grid points\n\nStudy region, larger box, is a smaller subregion within the Western Europe-Atlantic region, and the Risk area is the smaller box\nThe response variable measurements are for the grid points in the study region\n\n\nModel\n\nModel has 3 components\n\nExceedence Probability is fit first. Then, its probabilities are used in the Marginal Extermal Intensity model. Then, both results are used to model the Spatial Dependence\nExceedence Probability: indicator response for whether temp has exceeded the threshold.\n\nFits a boosted tree with mean log loss\n\nMarginal Extremal Intensity: uses a univariate loss function discussed at the beginning of the talk\n\nksi (shape?) is a hyperparameter along number of trees, m.\n\nSpatial Dependence\nPaper doesn’t come out until June or July, so will have to wait to see if there’s a package and to get further details\n\nFeature importances will be the Z500 and SM for each of the geographical location variables. These feature importances can used to figure out weather patterns in those areas that were important to occurance of the high temperature event in the risk area."
  },
  {
    "objectID": "qmd/nonprofits.html#sec-nonprof-misc",
    "href": "qmd/nonprofits.html#sec-nonprof-misc",
    "title": "Nonprofits",
    "section": "Misc",
    "text": "Misc\n\nNotes from:\n\nData Strategy for Non-Profits: Why?"
  },
  {
    "objectID": "qmd/nonprofits.html#sec-nonprof-devdatstrat",
    "href": "qmd/nonprofits.html#sec-nonprof-devdatstrat",
    "title": "Nonprofits",
    "section": "Developing a data strategy",
    "text": "Developing a data strategy\n\nWhy They Need a Data Strategy\n\nIdentify opportunities, inform decision makers on how to allocate scarce resources, and to measure and communicate impact\nProviding greater transparency and accountability to funders.\n\nThe availability of data has driven donors and grantmakers to be more conscious on the measurement of the impact that their dollars have. Non-profits which can better demonstrate their impact receive more financial support.\n\nGrowing impact of programs and services.\n\nData is utilized internally to improve services and expand impact which results in being able to generate greater impact.\n\n\n\n\nAssess Organization\n\nMission and Theory of Change\n\nA concrete outline which states\n\nThe impact that will be generated\nThe conditions needed to generate the impact\nThe programs in place to create those conditions.\n\nEach piece of the Theory of Change can then be stated in terms of a quantifiable measure of success which will serve as the starting point for developing a data strategy.\nExample\n\n\nStakeholders\n\nAnswer these questions:\n\nWho are your stakeholders?\n\nIdentify subgroups and individuals who fall into these groups\n\nDonors and Volunteers.\nManagement and Employees.\nBeneficiaries.\n\n\nWhat questions do stakeholders have that can be answered through data?\n\nDonors and Volunteers\n\nExample: A data-driven Impact Report which provides a holistic view of how the nonprofit utilizes their resources to achieve its mission.\n\nContains anectdotal stories with data that demonstrate how effectively their resources are being used\n\n\nManagement and Employees\n\nExample: more granular views on how individual programs and initiatives are performing on metrics related to their Theory of Change\n\nBeneficiaries\n\nExample: data around how projects in different sectors are performing\n\n\nHow will the data affect stakeholder decision making?\n\nDonors and Volunteers\n\nCan influence decisions around donating time and money\n\nManagement and Employees\n\nProvides visibility into how resources are allocated internally and empowers internal decision makers to evaluate how to get the most impact out of the limited resources they have\n\nBeneficiaries\n\nCan be used to garner buy in and allow the nonprofit access to communities that they would otherwise not have\n\n\n\n\nData Gap Analysis\n\nIdentify gaps between current data capabilities and those needed to answer all stakeholder questions\nContents\n\nOutline all data needs in the form of questions derived from your Theory of Change and stakeholder analysis.\nDeep dive into the required data to answer the questions and an estimate of how much that data would cost to obtain. Don’t forget that the same data could answer multiple questions.\nIdentify data that has already been collected and any existing efforts to collect additional data.\nConnect existing data and data efforts to questions and determine gaps between questions and data.\nPropose strategies to bridge data gaps and sustain data assets. Evaluate both the benefits of answering the question and costs of acquiring the data.\nPrioritize data gaps to close.\nCommunicate findings to relevant stakeholders.\n\n\n\n\n\nIdentify Key Questions\n\nShould be designed to be measurable, clear, and actionable\n\nAlso see nonprofit example in Projects, Planning &gt;&gt; General Steps to Starting a Project\n\nMake sure that you are addressing the social outcome and not program output of your organization.\n\nJust answering the question of how many people go through your program, how many volunteers you recruit / retain, are not sufficient because they focus only on outputs — but not social impact.\nYou would still need to assess whether those outputs are truly feeding into the social outcomes you want to accomplish through your organization.\n\nThe answers to key data questions should be resolved by a “North Star metric” or the single, defining outcome metric that best captures the core value that your nonprofit delivers.\n\nChasing non-critical metrics can strain resource constraints of the budget and time constraints of staff.\n\nExamples:\n\nDoctors Without Borders: Are we delivering adequate medical aid to people affected by crises (e.g., conflicts, epidemics, disasters) or exclusion from healthcare?\nAmerican Red Cross: How many lives have been saved from the blood donated by our donors?\nFeeding America: How many people have we been able to provide food for this year?\nBUILD: Are we improving the academic and professional outcomes of our students?\n\n\n\n\nOutputs vs Outcomes\n\nLogic model\n\nIt focuses on how inputs and activities translate to outputs and eventually\n\nExample"
  },
  {
    "objectID": "qmd/politics.html#sec-politics-misc",
    "href": "qmd/politics.html#sec-politics-misc",
    "title": "Politics",
    "section": "Misc",
    "text": "Misc\n\nNotes from Ethan Epping talk\n\nMost common modeling are for “support” (who they vote for) and “turn-out” (whether they vote at all) responses\nMessages around civic and community engagement is better at encouraging low propensity voters than pushing your candidate’s policy agenda\nTurnout Model\n\nFeatures: previous vote history of an individual; feature that influence turnout in general\n\nNCOA data (National Change of Address)\n\nVoter file info can be outdated\nWhen people move, nobody calls to cancel their voter registration\n\nGeocoding\nCommercially Available\n\nMobile phone data\nExperion\n\nGovernment\n\nGun and Boat Registration\nCollege student lists at public universities\n\nData collected through contact by the campaign\n\n\nSupport Model\n\nFeatures: survey data + other data\n\nDNC Stack\n\nBigQuery/SQL for a db and analytics,\nPython for engineering;\nDatascience does some R but more python\n\nVoter File\n\nList of registered voters in every state\nPublicly vailable\n\nStates have various restrictions\n\nOH and NC publish online and is free\nSome cost thousands and only available to campaigns\nNot available for commercial use though\n\n\nIncludes everything that’s on voter registration\n\nAddresses. phone numbers, etc.\nVote history (participation)\n\nIf you don’t vote in 2 straight federal elections you are usually eligible to be removed from the registry\n\n“Arbor” (sp?) tool\n\nFor registration drives\nAt census block or precinct level, take census estimate of eligible voters and subtract the number of registered voters\nHelps to figure out where to prioritize efforts (i.e. where the biggest pool of unregistered, eligible voters are)\n\n“Election Base” tool\n\nDe-aggregate results down to the census block level\n\nResolution is usually at the census block level because precinct boundaries often change\nEarly voting data\n\nWant to find voters that have requested a ballot but have yet to mail it in\n\nBest time to talk to them\n\nRejection data: talk to voters who incorrectly filled out a ballot\n\nVoter needs to talk to county clerk\nOr show up on voting day\n\n\nJobs\n\nhttps://www.progressivedatajobs.org/\n\nGuide to application process: https://guide.progressivedatajobs.org\n\nhttps://www.digidems.com/\n\nTakes tech people and gives some political training then deploys to campaigns"
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-misc",
    "href": "qmd/product-development.html#sec-proddev-misc",
    "title": "Product Development",
    "section": "Misc",
    "text": "Misc\n\nAlso see Decision Intelligence &gt;&gt; Conjoint Analysis for surveying consumers about product attributes to calculate attribute utilities and probability of purchase\nFlaws of Purchase-Funnel Based Attribution Metrics\n\nNotes from The Causal Analysis of Cannibalization in Online Products\nUsers usually take more complicated journeys than a heuristically-defined purchase-funnel can capture.\n\nExamples\n\nIf the recommendations make users stay longer on Etsy, and users click listings on other pages and modules to make purchases, then the recommendation-attributed metrics fail to capture the contribution of the recommendations to these conversions.  The purchase-funnel is based on “click”, and there is no way to incorporate “dwell time” to the purchase-funnel.\nSuppose the true user journey is “click A in recommendation → search A → click A in search results → click A in many other places → purchase A”.  Shall the conversion be attributed to recommendation or search? Shall all the visited pages and modules share the credit of this conversion? Any answer would be too heuristic to be convincing.\n\n\nViolation of causal experiment assumptions (ignorability assumption)\n\nSegments of users who follow the pattern of purchase-funnel may not be comparable between treatment and control groups, because the segmentation criterion (i.e., user journey) happens after random assignment and thus the segments of users are not randomized between the two groups.\n\nSolution involves causal mediation analysis (also see Causal Inference &gt;&gt; Mediation Analysis)\n\nDistribution of effect sizes for webpage variations using Optimizely (platform for A/B testing) data\n\n\nMedian (and average) webpage variations have roughly zero effect on webpage Engagement\nThread summarizing paper\n\n70% of effects will not show any impact on Engagement compared to a baseline\nalpha=0.05 yield an FDR of 18%-25% (i.e. much higher than alpha)\nRecommends only testing sizeable changes as these are the type of changes that yield the largest effects and fewer false discoveries (i.e. the tails of the effect size distribution)\n\nSee paper for other tests and recommendations\n\n\n\nLoyalty measure by app category\n\nTrust Thermocline - Point at which the number of customers that use your product suddenly drops off a cliff (3 nested Threads)\n\nBreaching this point typically occurs when a company continually changes the service, raises prices year after year but doesn’t notice a major change in customer subscriptions/engagement. Then customers start abruptly dropping the product.\nMost of the time the company cannot regain the customers after this point and even if they do they will never reach heights they were at previously.\nIf you’re a relatively large company you should have customer retention department/team/process in place\nSignals to watch out for\n\nWatch for grumbling and LISTEN to it.\nDon’t assume that because people have swallowed a price or service change that they’ll swallow another one.\nTreat user trust as a finite asset. Because it is.\n\nGround-level customer-facing people will be the ones receiving these signals\nMaintaining your customer base\n\nGet your customer retention people in a room with a white board and list all the issues they CONSTANTLY hear from customers but have stopped bothering to report up the chain.\nHowever painful it is to your bottom line. However politically tough it is. However complex the problem. These are the things that need to be fixed."
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-terms",
    "href": "qmd/product-development.html#sec-proddev-terms",
    "title": "Product Development",
    "section": "Terms",
    "text": "Terms\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. Low bounce rate can indicate the landing page needs improvement\nProduct Listing Page (PLP) -  webpage that lists all your products or a category of your products\nProduct Details Page (PDP) - webpage for a specific product that shows in-depth information"
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-behdat",
    "href": "qmd/product-development.html#sec-proddev-behdat",
    "title": "Product Development",
    "section": "Behavioral Data",
    "text": "Behavioral Data\n\nBehavioral data serves two main purposes for teams — understanding how the product is being used or not used (user behavior) and building personalized customer experiences across various touchpoints to influence user behavior.\n\nLaunching new features without instrumenting them (e.g. tracking) beforehand takes away the opportunity to analyze how those features are used (if at all) and to trigger in-app experiences or messages when relevant events take place (or don’t).\n\n\n\nMisc\n\nNotes from How to Collect Behavioral Data\nAlso see Marketing &gt;&gt; Customer Segmentation &gt;&gt; Behaviorial Segmentation\n\n\n\nSources\n\nPrimary: web app, mobile apps, a smart device, or a combination — powered by proprietary code\nSecondary: all external or third-party tools that your customers interact with directly or indirectly\n\ne.g. Tools used for authentication, payments, in-app experiences, support, feedback, engagement, and advertising\n\nAuth0 for authentication, Stripe for payments, and AppCues for in-app experiences\nOpening a support ticket via Zendesk, leaving feedback via Typeform, opening an email sent via Intercom, or engaging with an ad on Facebook\n\nTo collect data from secondary sources, you can either use source integrations offered by data collection tools or write your own code\n\n\n\n\nExtract Data\n\nUsing vendors are the best option. Maintenance and troubleshooting are not trivial for homemade solutions even for experienced engineers.\nCDI or ELT?\n\nCDI is best-in-class to collect behavioral data from primary or first-party data sources — web and mobile apps, and IoT devices\nELT is best-in-class to collect all types of data including behavioral data from secondary data sources — third-party tools that power various customer experiences.\n\nCustomer Data Infrastructure (CDI)\n\nCharacteristics\n\nPurpose-built to collect behavioral data from primary or first-party data sources but some solutions also support a handful of secondary data sources (third-party tools).\nData is typically synced to a cloud data warehouse like Snowflake, BigQuery, or Redshift, but most CDI solutions have the ability to sync data to third-party tools as well.\nAll CDI vendors offer a variety of data collection SDKs and APIs\nSome CDI solutions store a copy of the data, some make it optional, and some don’t.\n\nCDI Vendors\n\nSegment Connections - supports data warehouses and a host of third-party tools as destinations, as well as store a copy of your data that can be accessed later if needed.\nmParticle - offers CDI capabilities along with identity resolution in its Standard edition whereas audience building is available on the Premium plan\n\nAlso supports data warehouses and a host of third-party tools as destinations, as well as store a copy of your data that can be accessed later if needed.\n\nRudderStack Event Stream and Jitsu - Open Source; support warehouses and third-party tools but RudderStack offers a more extensive catalog of destinations\nSnowplow - open-source and unlike the others, Snowplow doesn’t support third-party tools as it is focused on warehouses and a few open source projects as destinations.\nFreshpaint that offers codeless or implicit tracking\nMetaRouter which is a server-side CDI that only runs in a private cloud instance\n\n\nELT tools\n\nThese provide more comprehensive source integrations than CDIs can\nAirbyte\n\nOpen-source\nOffers source connectors with 150+ tools like Zendesk, Intercom, Stripe, Typeform, and Facebook Ads, many of which generate event data\nOffers a Connector Development Kit (CDK) that you can use to build integrations that are maintained by Airbyte’s community members\n\nOther (all open source): Fivetran, Stitch, and Meltano\n\n\n\n\nProcess Data\n\nNotes from The Modern Customer Data Stack\nIdentity Resolution: Identifying the same users in different data sources\n\nIdentify match keys: Determine which fields or columns you’ll be using to determine which individuals are the same individual within and across sources.\n\nExample: email address and last name.\n\nAggregate Customer Records: Create a source lookup table that has all the customer records from your source tables.\nMatch & Assign a Customer ID: Take records that have the same (or in some cases, similar) match keys and generate a unique customer identifier for that matching group of customer records. Every customer id that is generated can be used to link the customer sources together going forward.\nRoll in more sources: As you get more sources you can start rolling them into the same process by setting the correct rules and precedence for the source.\n\nMaster Data Models: Creating a final/clean view of your customers and associated facts and dimensions.\n\nStart with a “Customer → Transaction → Event” framework (more details)\n\nCustomers: Create a table of your customers with the ability to quickly add new fields\nTransactions: Join key from customers table to their transaction history\nEvents: Any events you track for each customer\nOther business types may have other tables\n\ndouble-sided marketplace - tables for both sellers and buyers as different entities.\nB2B business - separate accounts and contacts entities\n\n\n\n\n\n\nAnalysis Tools\n\nGeneral characteristics\n\nOffer SDKs and APIs to collect data from your primary (first-party) data sources\n\nUsing purpose-built data collection tools (CDI and ELT) is more efficient and prevents vendor lock-in\n\nStore a copy of your data and allow you to export the data (usually for an additional fee)\n\nAmplitude, Mixpanel\n\nCan be integrated with Airbyte\n\nIndicative, Heap\nPostHog (open-source)\n\nCan be integrated with Airbyte"
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-convfun",
    "href": "qmd/product-development.html#sec-proddev-convfun",
    "title": "Product Development",
    "section": "Conversion Funnels",
    "text": "Conversion Funnels\n\n\nFigure: Typical eCommerce Funnel\n\nFunnel moves down and users drop off until only 3% of users (“% users”) reach Transaction (aka conversion)\n“CR” is the conversion rate at each point in the funnel\n\nProvide you with quantitative data about the number of customers that churn at each stage of the funnel. While this information is already valuable, paired up with qualitative data, it gives you all the insights you need to retain more customers.\nMisc\n\nAlso see\n\nMarketing &gt;&gt; Customer Journey\nAlgorithms, Product &gt;&gt; Customer Journey and Conversion Funnel\n\nTypical e-commerce user conversion rate can be benchmarked at around 2.5–3% in a regular business as usual time.\n\nBenefits\n\nProduct Managers:\n\nUseful for new feature launches. By grouping your sessions by user or device properties, you can compare the conversions between different user cohorts.\nIs the new feature sticking or not? Are your users struggling with it? Are they simply not interested? Looking at the numbers is one thing, but try getting deeper by watching session replays. Now you see what went wrong exactly.\n\nMarketers:\n\nBreak down conversion numbers according to the different acquisition channels and figure out where your most valuable users are coming from. This way, you can focus your efforts on the more relevant channels.\n\n\nSignals\n\nThe more rare an event is that’s high in an e-commerce funnel (i.e more towards Home Page), the more weight it carries in terms of purchase signaling.\nIf a user has entered the bottom of the funnel (i.e. more towards Begin Checkout) and simply dropped off, it strong reason to reach out in attempts to facilitate or promote movement down the funnel.\n\nData\n\nFunnel data at the user-level over a time period\n\n\nThe columns are in sequence according the conversion funnel (i.e. “Home” is the beginning of the funnel and “Purchase” is the end.)\nValues are counts of viewing events for webpages in the conversion funnel\n\nValues for Home must be an indicator of whether they have visited or started on the Home Page.\n\nInterpretation\n\nUser A\n\nLooks similar to a window shopper, that is engaged enough — 50% of their PLP views turn to PDP views.\nThey have not added anything to a cart but they may have had something there from previous sessions — which is indicated by 1 cart view.\n\nUser B\n\nLikely a customer who is actively trying to make a choice. They may be preparing their cart for a transaction but have not started checking out yet.\n\nUser C\n\nWent way deeper into PLP browsing.\nShows signs of being ready to commit to a purchase and even started checking out once. However, they did not complete a transaction.\n\nPossibly, they dropped off in search for coupon codes or better deals elsewhere.\nMaybe, based on the high PLP view count, they were deep into search but did not manage to find the products of interest.\n\n\nUser D\n\nProbably knew what they wanted, which is\n\nIndicated by a relatively high ratio from PLP to PDP views and high PDP views to Add to Cart ratio.\n\nThey viewed their cart multiple times, reviewing it. But somehow, they have not started the checkout.\nThis could be a perfect candidate for the abandoned cart campaign.\n\nUser E\n\nProbably a returning customer who come back shortly after another session.\n\n\n\nSignal Scoring Steps\n\nChoose a timeframe\n\nDepends on the business model and the action you’re expecting to take with it.\nExamples\n\nUsers are taking up to around 1 month to consider a purchase, then update segments on a 30-day rolling basis.\nYou want to communicate with your customers daily, then daily morning updates could be something to consider.\n\n\nIndividual Signals: events that are positively associated with the conversion (e.g. the number of daily PLP views, PDP views, add to carts, etc.)\n\nUnderstanding how a user scores in each one of these signals can help identify which part of the conversion funnel was not covered by a user.\n\nThese scores can inform how the business should interact with that customer\n\nFor each signal in terms of activity (i.e. counts of events), segment customers into 3 quantiles (&lt; Q33, Q33 &lt; Q66, &gt;Q66) with labels below average, average, above average customers\n\nCan also label customers who did not have any events for a signal at all (0-score users)"
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-sopd",
    "href": "qmd/product-development.html#sec-proddev-sopd",
    "title": "Product Development",
    "section": "Stages of Product Development",
    "text": "Stages of Product Development\n\nStage 1: Coming up with initial product ideas.\nStage 2: Selecting ideas:\n\nQuantitative analysis to select a subset of ideas to which to devote resources, often referred to as opportunity sizing.\n\nStage 3: Experiment design:\n\nInvolved with selecting success and guardrail metrics, running sanity checks, choosing randomization units, etc.\nCandidates will need to consider alternatives when it is not possible to run A/B tests.\n\nStage 4: Making a launch decision:\n\nMaking scientific decisions based on experimentation results.\nDiagnosing problems and evaluating tradeoffs.\nThroughout the product development lifecycle, working with the appropriate metrics is of paramount importance"
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-wdplas",
    "href": "qmd/product-development.html#sec-proddev-wdplas",
    "title": "Product Development",
    "section": "Why Do People Leave and Stay?",
    "text": "Why Do People Leave and Stay?\n\nMight need to be answered by user research and experiments instead of user analytics data\nCollaborate with the UX research team to help design the surveys and interviews, and generate insights\nCollaborate with the engineers to design experiments and analyze results\nSome reasons people churn:\n\nThey might not understand the product.\nThe product might be hard to use.\nPeople might not see the value of the product.\nPeople prefer a competitor’s product.\nThe product might have some issues like bugs or being slow.\nThe new user is not the target user. There could be a mismatch between the user acuiqition and core features.\nPeople might only need our product for a short period of time.\n\nSome reasons people become long term users:\n\nPeople love the product.\nPersonalized notification works.\nIt has become a habit for people to use the product."
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-fdbk",
    "href": "qmd/product-development.html#sec-proddev-fdbk",
    "title": "Product Development",
    "section": "Feedback",
    "text": "Feedback\n\nMisc\n\nBefore even touching any of the data, ask yourself some user privacy questions: do we have good reason to be looking at the user data? Would we be violating any user privacy? In modern technology, user privacy is at the forefront so this should always be your first thought.\n\n\n\nTypes\n\nIn-product bug reporting: Many products give users a way to leave feedback if the system crashes or they navigate somewhere unexpected.\nIn-product feedback: Products can ask for more general feedback in specific locations or always give users an option in site menus.\nSocial media: Thanks to the wide penetration of social media, it’s common to find feedback about products online. You can track specific tags on Twitter, monitor your company’s Facebook or Instagram profiles or check your executive’s posts.\nInternal feedback: Many people working internally in a company will have a view of how the product behaves and capturing feedback from these people is also important.\nUser research: Companies can actively seek user feedback through user research which often includes recruiting individuals and asking questions directly.\n\n\n\nPotential Insights\n\nInform product direction: If users are consistently asking for a specific feature or requesting you to add more types of content, then the product may very well benefit from following their advice.\nIdentify bugs: When something goes wrong, users are often happy to complain about it. Monitoring user feedback is a great way to get quick insights into potential problems with your product.\nUnderstand product sentiment: Product teams will often ask themselves, how are we doing? There are many ways to answer this question including benchmarking against similar products and looking at retention metrics, but an equally effective method is to understand product sentiment through user feedback. This can help you know when you may need to invest more in building a better product, or if you can go heavily into marketing spend.\n\n\n\nIssues\n\nUser feedback is typically biased: If people are unhappy with something, then more likely to shout about it, then if they’re not fussed by it. This can lead to the feedback you actually receive being weighted heavily to the extremities, both users complaining about negative experiences and also those strong advocates who loved your product. There’s no silver bullet for dealing with this, so it’s important that all analyses done be caveated with this.\nUser feedback is noisy: Feedback comes from people, and people have very different circumstances when coming into contact with your product. A feature might be loved by one user and hated by the next. To this end, it’s important to try to get as much data as possible, and from different sources. Another important approach is to look deeper than the averages, rather than just the mean, look at the actual distribution of reviews — are there a few people with very negative ratings changing the average?\n\n\n\nDS Outputs\n\nSelf-service dashboards: Users across a company can use the dashboard to get insights on feedback.\nRegular reporting: Having a good understanding of the feedback data can lead to automatic generation of reports which can drastically reduce the time to actually understand feedback and get insights out of it.\nScalable datasets: Datasets which can be used by employees across the company.\n\n\n\nEDA/Processing\n\nFor your product area, identify the keyword/s that users are likely to use, for example if you’re running a clothing e-commerce store, they might be: “fit”, “style”, “expensive”, “ugly”, etc. Try to keep the total number to less than ten.\nSample at least 100 pieces of feedback from the total population and do a simple text match with the terms identified in the step above.\nManually scan through the sampled feedback. Did they all fall into the categories already thought of? Does the category actually match the content? For those missing a category, should there be a new category added to capture this?\nYou can iterate on the three steps above until you’re happy with how the feedback looks.\nPlot the volume of reports matching your keyword/s, do you see a consistent pattern, are there increases around any product launches? Correlation with product launches and known bugs can help confirm the reports correlate with what you’re interested in.\nCheck the volume is high enough to be useful. You should aim to find a few hundred reports per day in order to get meaningful feedback. Of course, you can work with fewer if that’s all you have, but it will be harder to detect quick changes.\n\n\n\nModeling\n\nPerform sentiment analysis to understand if users are positive or negative towards your product.\nRun topic modeling algorithms to have a more flexible understanding of the topics users are giving feedback on.\n\n\n\nTrack Metrics\n\nThis should include taking the analyses completed above and making timeseries to add to dashboards.\nSet up anomaly detection to be alerted if there are significant movements in the metric. This can be simple percentage movements in the key metrics that you identified above in the understand and modelling phases, or you can use more sophisticated algorithms.\nIt’s also good to create automated reports where possible, to summarize longer term trends, such as quarterly reports. You’ll always require some manual work, but setting up the queries and making it as easy as possible to action is important."
  },
  {
    "objectID": "qmd/product-development.html#sec-proddev-metrics",
    "href": "qmd/product-development.html#sec-proddev-metrics",
    "title": "Product Development",
    "section": "Metrics",
    "text": "Metrics\n\nTypes\n\nSuccess\nGuardrail\nFunnel\nInput-Output\nGrowth\nPlatform\n\n\n\nSuccess Metrics\n\nSometimes called a “north star metric” or “primary metric” or “OKR metric”) are used to measure the success or health of a product. Commonly used (daily/monthly) success metrics include:\n\nActive Users\nBookings / Purchases\nRevenue\nClick Through Rate\nConversion Rate\n\n\n\n\nGuardrail Metrics\n\nMeasures core quantities that should not degrade in pursuit of a new product or feature. Success metrics of other core product teams can sometimes also serve as guardrail metrics. Examples can include:\n\nBounce Rate - the percentage of visitors to a particular website who navigate away from the site after viewing only one page. High bounce rate can indicate the landing page needs improvement\nCancellation / Unsubscription Rate\nLatency\n\n\n\n\nFunnel Metrics\n\nAny family of metrics that tracks the “user journey” through various parts of a product.\nAARRR growth metrics (see below for details: A user who is activated must have been acquired (through some channel or organically); a user who generates revenue must have been previously activated.\nConversion Funnels: a standard “conversion path” through the product, captured through basic metrics such as:\n\nTech Company\n\nNumber of visitors to webpage;\nNumber of logged-in users; Number of users who click particular parts of the logged-in pages;\nNumber of users who visit the checkout page;\nNumber of users who purchase.\n\nB2B (Business to Business)\n\nNumber of visitors to webpage (leads);\nNumber of leads who request free trials;\nNumber of leads to which the Sales team proactively reaches out;\nNumber of paid customers (each of which is a company / business) who made recurrent purchases.Number of visitors to webpage (leads);\nNumber of leads who request free trials;\nNumber of leads to which the Sales team proactively reaches out;\nNumber of paid customers (each of which is a company / business) who made recurrent purchases.\n\nWhat makes a good step to monitor in a conversion funnel?\n\nNotes from Step Suggestions: How We Take the “Guess and Check” Out of Building Funnels\nA good funnel step is ubiquitous: almost no users reach the next step without performing this action.\n\n\nExamples:\n\nUbiquitous: “Enter Password” or “Click Add to Cart”\nNot Ubiquitous: “Subscribe to Mailing List” or “View Warranty”\n\nTo qualify as ubiquitus, at least 97% of completers need to have completed the step.\n\nA good funnel step is divisive: some users drop off before performing the step, and others drop off after.\n\n\nX is a good milestone: 25% of users drop off from A to X, and 33% drop off from X to A (divisiveness = min(25%, 33%) = 25%).\nY isn’t a good milestone: 0% of users drop off from A to Y (divisiveness = min(0%, 50%) = 0%).\nZ isn’t a good milestone, 0% of users drop off from Z to B (divisiveness = min(0%, 50%) = 0%)\nNot Divisive Example: “Submit” at the end of a form\n\nIf 100% of people that click that button reach the next page, then it’s not worth adding a step to break that down\n\n\nExamples\n\nform field change events, like entering a password or checking a checkbox.\npageview\n\nPitfalls\n\nAdding a step to the funnel that isn’t required for conversion\n\nIf you add this step, the conversion rate will be biased\nExample:\n\nFunnel for a login form which has a true conversion rate of 36% (login submits/login pageviews)\nEntering a phone number was an optional step. Adding it to the funnel drops the conversion rate to 19.98%\n\n\n\n\n\n\n\n\nInput-Output Metrics\n\nInput / Driver Metrics: Metrics that track the activities / resources used to work towards an outcome.\nOutput metrics: Metrics that demonstrate the outcome of an initiative.\nExample: Search and Recommendation\n\nInput Metrics\n\nClick-Through-Rate (CTR)\nAverage Time Spent on a particular types of content\n\nOutput Metrics\n\nAverage Time Spent on the platform per user\nSessions-per-User\nSuccessful Sessions per User\nAdvertisement Revenue\n\n\nExample: Fraud Detection\n\nAlso see Platform Metrics\nInput metrics\n\nTrue / False Positive Rates of\n\nFraud Rules and Models\n\nFraud scores are calculated based on rules, which add or subtract points. The user action may be a transaction, signup or login. Rules look at data points such as an email address, IP address, or social media presence.\n\nOperations Manual Review Volumes\n\nA human reviews the case to determine whether action is needed. In fraud, an model output may trigger a “manual review” to determine whether an event was indeed fraudulent.\n\nCustomer Transaction Amounts\n\nWhether or not a transaction not fit a customer’s normal buying habits\n\n\n\nOutput metrics\n\nFraud Loss Volumes\nLosses Prevented\nRevenue\n\n\n\n\n\nGrowth Metrics (AARRR)\n\nAre responsible for enlarging a product’s user base and keeping current users engaged\n\n\nAARRR\n\nAcquisition: Getting customers to sign up for a website or product, which requires driving product awareness (often known as “top of funnel”).\nActivation: Getting customers to gain basic familiarity with the product, and appreciating the basic product value.\nRetention: Getting customers to come back to the product on a regular basis; a customer that exhibits no (or minimal) activity over some predetermined time period is known as churned (the precise definition depends on the business context and varies greatly across teams / companies).\nReferral: Getting customers to share the product with others, in turn further growing the user base.\nRevenue: Getting customers to adopt one or more paid features that generate revenue for the company; also known as Monetization.\n\n\n\nConcepts\n\nTradeoffs Between Acquisition and Revenue\n\nHow should a company strike the optimal balance between revenue maximization and acquiring new customers (the latter can always be done via expensive campaigns, such as providing large discounts / gifts for new users, etc.)?\n\nDifferent Levels of Engagement\n\nTypically, a company will have multiple tiers of user engagement metrics, which reflect different levels of product engagement (with “activated user” being the lowest tier).\nIf daily use is what is most valuable (e.g. Facebook, ad revenue), Engagement = Daily Active Users (DAU) / Monthly Active Users (MAU)\n\nEmails and push notifications tend to increase casual numbers (the MAU) but not the daily users (DAU).\nThe magnitude of the engagement of this type is mostly determined by the product category and doesn’t change much over time\n\nsee Misc &gt;&gt; Loyalty measure by app category\n\nhigh-frequency, high-retention apps (e.g. communication)\n\ni.e. an app with 10% DAU/MAU doesn’t usually turn into a 40% DAU/MAU\n\n\nIf daily use is NOT where the company’s value comes from, then find and investigate power users.\n\ne.g. Users of companies like Uber, ecommerce, Airbnb don’t use their services every day but these companies are huge.\n\n\nRetention and Churn Cannot be Easily Estimated Over the Short Term:\n\nDefinitions of retention and churn vary from company to company, but typically require weeks if not months of continuous observation. Furthermore, it is possible for a churned customer to recover (often known as resurrection), so it is important to clarify the precise metric definitions and assumptions before answering any questions (e.g. first time churning, most recent time churning, ever churned, etc.).\n\nDifficulty of Measuring Incrementality for Referral Initiatives:\n\nWhen new referral programs are launched, network effects are typically introduced because any new customer can be simultaneously a referrer and a referee.\n\nDifferences Between B2B and B2C Settings:\n\nIn B2B companies, acquisition/activation/retention etc. are typically defined at the “company / business” level, because a customer is a company / business. There could be analogous granular user-level metrics as well.\n\nChallenges of International Expansion:\n\nThese include additional costs associated with localization (adapting a product appropriately to better fit the customers in a particular market), regulatory / compliance requirements, understanding the competitive landscape, etc.\n\n\n\n\nPower User Analysis\n\nQuestions\n\nWhich customers generate the most value for your company?\nHow are they doing it?\n\nFrequency of use?\n\nWhat % of your users are active (engage in valuable activity) every day last week?\n\nNetwork effects?\nContent they’ve produced or saved?\nWhat characteristics make them different from the casual user?\n\nThe action(s) should correlate to some value metric.\nHow are you going to produce or acquire more of them?\nhow can I understand their needs better and make sure we continue building in a direction that supports their daily workflow (and that we can upsell new features)?\n\nPower User Curve (aka Activity Histogram or the “L30”)\n\nHistogram of users’ engagement by the total number of days they performed which action you’ve deemed valuable in a month\n\nCan be applied to a cohort if you’re a subscription business, track product launches, track results from feature changes, or results from marketing campaiagns, etc. (see below)\nI don’t think this concept has to be limited to time being on the x-axis. It could be any metric value you consider representative of a “power user.”\n\nAlso see Google, Analytics, Analysis &gt;&gt; Power User Analysis (Example 24)\n\nY axis: % users, X axis: visits\n\n\n\nTime Period\n\nDepends on your products natural cycle. May be monthly (“L30”), weekly (“L7”), daily, etc.\nExample: productivity/work-related products that users engage with Monday through Friday. B2B SaaS products will often find it useful to show this version, as they want to drive usage during the work week.\n\n“Smile” Curves are Good\n\nMonthly\n\n\nThe 7% (far right) that engage daily shows that a group of users are returning frequently (daily) so it may be possible that the impressions can support an ad business\nSuccessive Power User Curves should ideally show users shifting over more to the right side of the smile\n\nWeekly\n\n\nThe “smile” occurs between 1 and 5 days which makes sense for productivity type product that is most active during the workweek.\n\n\nCohort Analysis\n\n\nAlso see Algorithms, Product &gt;&gt; Cohort Analysis\nShows a positive shift in user engagement from August to November, where a larger segment of the population is becoming active on a daily basis, and there’s more of a smile curve.\nInflection points where the line starts to bend upward for a particular month can indicate:\n\nWhen a critical product release, unblocking features, or a marketing effort might have started to bend the curve\n\nThis might be a place to double down, to increase engagement.\n\nFor a network effects product, you might expect to see newer cohorts gradually improve as you achieve network density/liquidity.\n\n\n\n\n\n\n\nRARRA\n\n\nAARRR focuses too much on Acquisition even though Retention is more important, especially for many internet products that struggle to retain people.\nReasons why retention is more important:\n\nAcquisition strategies (e.g., ads) are expensive and it is often cheaper to retain a user than get a new user\nRetention is the foundation of growth\nUser retention is more directly connected to revenue than acquisition.\n\n\n\nPlatform Metrics\n\nDepartments include Customer Support, Trust and Safety, Payments, Infrastructure, Operations\nMany of these are indirectly related to the Growth Metrics\nMetrics\n\nCosts of Operations / Infrastructure: A granular understanding of various infrastructure costs allows one to effectively evaluate tradeoffs for expansion.\n\nAverage cost of a help center ticket / escalation - essential quantity to track for many types of product changes in customer-facing companies\n\nSuccess / Failure Rates: These metrics are strongly correlated with conversion, particularly in the Monetization context (since payments are typically the final step of the monetization funnel).\n\nPercentage of transaction attempts that succeed / failure out of all transaction attempts\n\nFalse Positive / False Negative / True Positive Rates (for fraud detection): rules and models for detecting bad user behavio\n\nAlso see input/output metrics\nFalse positives can decrease a good customer’s lifetime value (LTV or CLV)\nFalse negatives increase fraud losses\n\nFraud Loss Metrics: As a company scales, fraud losses typically follow (from a mixture of)\n\n“Born Bad Users” - Users who abuse the product for inappropriate financial gains and\nThird Party Fraud - Users who take over the accounts of good users\n\nVendor Costs: (e.g. Stripes for payments processing)\n\nNeed to be carefully weighed against other core business metrics."
  },
  {
    "objectID": "qmd/real-estate.html#sec-rlest-misc",
    "href": "qmd/real-estate.html#sec-rlest-misc",
    "title": "3  Real Estate",
    "section": "3.1 Misc",
    "text": "3.1 Misc\n\nListing Price\n\nAffects the final selling price, how long the home spends on the market, the volume of interest in the house, and anchors price negotiations with buyers\nAlgorithmic estimates have better performances when they take the list price into account\n\nA home has a value distribution as different potential buyers place different values on the various home features. The eventual selling price is a function of this value distribution and the specific individuals who consider the home.\nPrice/Income ratio\n\nExample: Median house price in middle class suburbia in a very affordable region is $323,000. To maintain an price/income ratio of 3, that requires a household income of $107,000. In 2019, regional median HH income was $62,502. ~30% of HH make over $100K."
  },
  {
    "objectID": "qmd/real-estate.html#sec-rlest-terms",
    "href": "qmd/real-estate.html#sec-rlest-terms",
    "title": "3  Real Estate",
    "section": "3.2 Terms",
    "text": "3.2 Terms\n\nPrice Anchoring - a pricing strategy that plays on buyers’ inherent tendency to rely heavily on a piece of initial information to guide subsequent decisions. In the context of pricing, many businesses will set a visible initial price for a product but make a point of showing that it’s now being sold at a discount.\nPrice Index - a normalized average (typically a weighted average) of price relatives for a given class of goods or services in a given region, during a given interval of time\n\nCase-Shiller U.S. National Home Price Index (also {fredr})\nWiki has formulas\nSee Appraisal Methods &gt;&gt; Prediction Adjustments &gt;&gt; Repeat Sales Model\nIf a price index rises 10%, it means the average level of prices has risen 10%"
  },
  {
    "objectID": "qmd/real-estate.html#sec-rlest-bizq",
    "href": "qmd/real-estate.html#sec-rlest-bizq",
    "title": "3  Real Estate",
    "section": "3.3 Business Questions",
    "text": "3.3 Business Questions\n\nAgent\n\nWhich house should I buy or build to maximize my return?\nWhere or when should I do so?\nWhat is its optimum rent or sale price?\n\nBuyer\n\nMatch them with agent\nfind them a home\nappropriate mortgage"
  },
  {
    "objectID": "qmd/real-estate.html#sec-rlest-usecases",
    "href": "qmd/real-estate.html#sec-rlest-usecases",
    "title": "3  Real Estate",
    "section": "3.4 Use Cases",
    "text": "3.4 Use Cases\n\nUsing computer vision and NLP to enhance searches\n\nIncorporating image search capabilities: extract information from pictures of the property utilizing object detection and image classification techniques, to be used in search matching.\nRecommendation-engine-powered rankings: search results could be ranked according to the likelihood that the specific user will interact with the results, based on previous searches, profile characteristics, and contextual information.\nSearch intent matching: enhance the user experience by adding the ability to write (or dictate) their home preference(s) instead of manually filtering the results. It may be very wise to incorporate such a feature given the rise of home assistants.\nVisual search: perform a search based purely on images of homes. This would enhance the search experience or complement the keyword search and produce more accurate and useful results.\n\nIdentify homeowners who are more likely to sell and estimate their selling price using publicly available data\nFind correlations between past sellers and current homeowners, in order to help predict the likelihood that a given owner is willing to sell.\nEstimate appropriate selling price point and the interest of possible future owners so as to increase the chances of successful outreaches.\nLead classification:\n\nBased on web-based actions executed by a user, understand where they are at in the customer journey and gather accurate information to move them to the next stage in the funnel.\n\nRisk assessment:\n\nMultiple risks should be considered when assessing a real estate transaction. Forecasting models powered by machine learning could complement a traditional risk analysis approach well, especially given their multiple data source analysis capabilities.\n\nHome valuation:\n\nA classic, yet constantly evolving, machine learning task is to set the price of a house based on MLS and alternative data (e.g. satellite imagery). The big iBuyers are all betting heavily on this since it’s key to their business model. An example of the level of detail obtainable is that of how Opendoor analyzed the impact of busy roads in their property valuation model."
  },
  {
    "objectID": "qmd/real-estate.html#sec-rlest-apprais",
    "href": "qmd/real-estate.html#sec-rlest-apprais",
    "title": "3  Real Estate",
    "section": "3.5 Appraisal Methods (Price Estimation)",
    "text": "3.5 Appraisal Methods (Price Estimation)\n\n3.5.1 Traditional\n\nComparative Market Analysis (CMA)\n\na collection of recently sold “comparable” homes (“comps”) that, taken in aggregate, can provide a view on the value distribution of the appraised home (the “subject” home)\nselecting similar properties (in terms of attributes and location) and inferring the target value from the comparables\n\nCost Approach\n\nThe “replacement cost” is determined by totaling values such as the value of raw land (again, using comparison), the cost of rebuilding a new building that could perform the function of the existing property, and then making necessary adjustments (e.g., deprecation of the existing building).\n\nProfits Method (or income approach)\n\nestimates the value of the property based on the income it generates. The value is linked to the business carried out within that property. For example, the market value of a hotel depends on the potential cash flow derived from ownership.\n\nRepeat sales method\n\nanalysis is restricted to just comparing price changes on properties sold more than once\n\n\n\n\n3.5.2 ML Approaches\n\n3.5.2.1 Misc\n\nAutomated Valuation Models (also known as AVMs) were found to have an absolute error below 4% for homes and below 6% for commercial properties, which is much less than the error rates of traditional appraisals.\n\n\n\n3.5.2.2 Features\n\nDifferent predictors may be more valuable for each of the price models.\n\nSale price: Zillow and Redfin use their own algorithms for real estate price estimation.\nRental price: HomeUnion developed a tool called RENTestimate for this.\nTemporary rental price: Airbnb’s pricing tips use a mathematical model that learns how likely a guest is to book a specific listing, on specific dates, at a range of different prices.\n\nAdding nontraditional data sources can improve estimates\n\n\n\n\n3.5.2.3 Hedonic Pricing Method\n\nA typical regression that accounts for property features such as size, number of rooms, property age, home quality characteristics (granite countertops, air conditioning, pool, etc.), and location\n{hpiR} (vignette)\nIt’s just multivariable regression where log sale price (or ppsf, etc.) is the outcome and includes house characteristics and has dummy variables for the years (or days, weeks, etc.) when the house was sold\nThe exponential of the coefficients of the year dummy variables are the index and they represent the average change in the outcome variable from the reference year to the year represented by the dummy variable\n\n1 is the index of the reference year\n\n\n\n\n3.5.2.4 CMA with Comp Price Adjustments\n\n3.5.2.4.1 Process\n\nData Scientist role\n\nSuggests good comparable properties for a CMA\nHighlights the important differences between the comparable and the subject property (the fewer, the better)\nSuggests pricing adjustments for the differences\n\nReal Estate Agent\n\nSelects which properties to use as comps\nSelects which adjustments are relevant and appropriate for their CMA report\n\n\n\n\n3.5.2.4.2 Misc\n\nNotes from\n\nPricing Homes like Agents Do: AI for Real Estate CMA Adjustments\n\nAll models of adjustments seem to use Price per Square Foot (PPSF) for a local area as the outcome variable\nProbably want to go as small an area as you can depending on the amount of training data at that resolution\n\n\n\n3.5.2.4.3 Market Price\n\nUsing the repeat sales model, we estimate how the price-per-square-foot evolved for each locality. Then we can use the model to estimate the difference in average price per square foot (PPSF) in the locality across any two-time points.\nIf a price index rises 10%, it means the average level of prices has risen 10%\n\nExample:\n\nCMA model produces a comp that sold for $1,100,000 almost a year ago in April 2019\nRepeated Sales model’s indexes say that property prices have decreased by 4.6% in area since the sale of this comp.\nThus, to make this comp comparable now, we would need to subtract $51K (1,100,000 * 0.046 = 50,600) from the price it sold for back in April 2019.\n\n\nIn this description and the examples, I’m using sale price, but the same methods can be applied to PPSF.\nThe data is house id, year sold, sale price\n\nThere will be repeated measures because each house will need to have been sold multiple times.\n\nMaybe can this be fudged a bit by using really good comps for the repeated measures\n\nOnly the data from the 1st and last sales are used\nThe first year in the sample data is NOT included as a variable in the regression\n\nThis is the reference or base year. This year automatically gets an index of 1.\n\n\nAssumes homeoskedacity\nSpecification\n\nlog(last_sale_price / first_sale_price) = β1 * I(year_1) + β2 * I(year_2)  + …\nwhere I(year_*) is an indicator variable with values of\n\n1 for year of last sale\n-1 for year of first sale\n0 for years where the house wasn’t sold\n“*” is the year of the sale of the house.\n\nNote that there is no intercept so will need 0 + in the formula\nexp(β*) gives the index for that year Index for the first year of the sample data is automatically set to 1.\n\nExample\ndat &lt;- tibble(\n  y = rep(c(0.182, 0.041, 0.039), 3),\n  year = rep(c(\"2013\", \"2014\", \"2015\"), each = 3),\n  value = c(-1, -1, 0, 0, 1, -1, 1, 0, 1)\n) %&gt;% \n  tidyr::pivot_wider(names_from = year, values_from = value, names_prefix = \"Y\")\n      y Y2013 Y2014 Y2015\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.182    -1    0    1\n2 0.041    -1    1    0\n3 0.039    0    -1    1\n\nmod &lt;- lm(y ~ 0 + Y2014 + Y2015, data = dat)\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)\nY2014  0.07500    0.04808  1.560    0.363\nY2015  0.14800    0.04808  3.078    0.200\n\nexp(coef(mod))\n  Y2014    Y2015 \n1.077884 1.159513\n\n“y” is the log of the sales price ratio\nIndexes:\n\n2013: 1\n2014: 1.08\n2015: 1.16\n\n\nCase-Shiller Method\n\nHandles heteroskedasticity by Weighted Least Squares\nTypically follows the same trend (highly correlated) as the regular repeated sales model but on a lower level\nPackages\n\n{hpiR}(vignette)\n\nMultiple modeling options, including weighted regression, robust regression, random forest\nAlso splits up houses with more than 2 sales into combinations of binary periods\n*Doesn’t sqrt the residuals in the weighted regression like I think the original paper recommends*\n\n\nProcess\n\nFirst fit the repeated sales model:\n\nlog(last_sale_price / first_sale_price) = β1 * I(year_1) + β2 * I(year_2)  + …\n\nRegress the squared residuals on the holding period\n\nε2 = β0 + β1 * holding_period\nHolding Period: the period between the first sale and last sale\n\nCalculate weights using fitted values\n\nweights = 1 / sqrt(^ε2)\n\nFit weighted repeated sales model\n\nModel\nmod &lt;- lm(LogP ~ 0 + ., data = dat %&gt;% select(-HoldingPeriod))\nmod_resids &lt;- resid(mod)\n\nholding_dat &lt;- dat %&gt;% \n  select(HoldingPeriod) %&gt;% \n  mutate(resids_sq = mod_resids^2)\n\nresids_mod &lt;- lm(resids_sq ~ HoldingPeriod, data = holding_dat)\nresid_preds &lt;- fitted(resids_mod)\nwgts &lt;- ifelse(resid_preds &gt; 0, 1 / sqrt(resid_preds), 0)\n\nfinal_mod &lt;- lm(LogP ~ 0 + ., \n                data = dat %&gt;% select(-HoldingPeriod),\n                weights = wgts)\n\n# indexes\nexp(coef(final_mod))\n\nHolding Period: the period between the first sale and last sale\n“LogP”: See previous model’s Specification\n\n\n\n\n\n3.5.2.4.4 Building-Floor\n\nHigher floors in apartment building cost more\nAdjustment varies per building but there isn’t enough training data to have a model per building\n\nSolution: Bin apartment data by building size, then fit a model for each bin.\n\nPerform Market Price adjustment to prices before modeling price differences by floor\n\n\n\n3.5.2.4.5 Location\n\nBuildings are of different ages, have various contractual constraints, and offer vastly different amenities.\n\nExample: A subject propert in a very expensive building for this neighborhood. The comp property is in a building where similarly sized apartments tend to sell for lower prices. So an adjustment to the comp needs to be made.\n\nSpatial nearest neighbor models for ppsf.\n\nTo prepare the training data, we use the Market Price adjustment discussed already\npackages: {nngeo}"
  },
  {
    "objectID": "qmd/real-estate.html#sec-rlest-feats",
    "href": "qmd/real-estate.html#sec-rlest-feats",
    "title": "3  Real Estate",
    "section": "3.6 Features",
    "text": "3.6 Features\n\nScrape traditional variables (from websites?)\n\nNumber of bedrooms\nNumber of bathrooms\nLiving area, square footage\nprice per square foot (PPSF)\nNumber of stories\nYear built\nproperty type (e.g. condo, house)\nFurnished/not furnished\nFireplace/no fireplace\nHeating/no heating\npool/no pool\ngarage space\nZIP code\nLatitude and longitude\nlisting price of comparable homes in the area\nsale price\ndays on the market\n\nOff-market data\n\ntax assessments\nprior sales and other publicly available records\nResident surveys\nmobile phone signal patterns\nYelp reviews of local restaurants help identify “hyperlocal” patterns—granular trends at the city block level rather than at the city level.\n\nMarket trends\n\ninflation\nseasonality in demand\neconomic shocks\n\nGeographic indicators\n\nUber’s H3 grid system instead of zip codes, etc.\nThe quality of local schools\nEmployment opportunities\nProximity to shopping, entertainment, and recreational centers\nSources Google Places, Yelp, or SchoolDigger\n\nImage data\n\nSatellite imagery\n\nMapbox’s Satellite API\n\nallows you to query any location by its latitude and longitude coordinates\nselect from 20 different zoom levels\n50,000 free requests each month!!\n\nnumber of roads and their condition and houses with pools\n\nMethod\n\nConvolutional Neural Networks (CNNs) can extract visual features, revealing underlying information captured in photos\nsatellite\n\nA classification network can distinguish between the top and bottom 15% of houses in a dataset to test this. A model achieved an outstanding 91% accuracy, using only satellite images. After interpreting the model to understand which visual patterns governed its decision process, we saw that it placed high importance on recreational areas such as parks and lakes.\n\nexterior frontal images and interior photos\n\nFeatures\n\nthe activeness of a street frontage\naccessibility of the area (number of roads, condition of the roads)\namount of urbanization\nproximity to parks, lakes and beaches (recreational areas)\namount of greenery\npopulation density\n\n\nNLP\n\nIdentify the most important words in a description and learn which words tend to be used to describe more expensive or affordable houses.\n\n\nSociodemographic\n\nMisc\n\nWhich data resolution is most predictive? (e.g. state, county, neighborhood, etc.)\nOfficial government data sources are best (consistent, unbiased, and up-to-date indicators, with the largest sample sizes)\n\nMedian Household Income\nUnemployment rate\ncrime rate\nEthnicity\nNumber of inhabitants\nAverage age\nPoverty level\nSource: American Community Survey (ACS) at census tract level\n\nsmall subdivisions of a county (around 4,000 inhabitants on average),\ndesigned to be relatively homogeneous units concerning population characteristics, economic status, and living conditions\n\nTotal population\nTotal employed population\npopulation median age\nMedian household income\nPopulation with income below poverty level\nMinority percentage\nVacant housing units for rent\nMedian value for all owner-occupied housing units"
  },
  {
    "objectID": "qmd/retail.html#sec-retail-misc",
    "href": "qmd/retail.html#sec-retail-misc",
    "title": "Retail",
    "section": "Misc",
    "text": "Misc\n\nDS Use Cases (intermediate level - data needs to be centralized and not siloed)\n\nDemand forecasting models for optimizing the stock levels.\nMarket basket analysis models for creating better newsletter offers.\nDynamic pricing models using competitors’ prices for better pricing strategies.\nCustomer segmentation model for better understanding our customers’ shopping preferences and providing them with customized offers and loyalty discounts.\n\nApproach to deciding on a store to close\n\nConsiderations\n\nCatchment Area: How to define the area that matters around my store\nStore Network: What are the relationships between stores\nPredicting Sales Transfer of your Customers (aka Customer Retention)\n\nOverlap in catchment areas and understanding how customers behave in your store network can help in this estimation\n\nPredicting the Impact on Acquiring New Customers (aka Customer Acquistion)\nBringing it together\n\nCalculate 1-year, 3-year impact of removing a particular store\nRank these impacts to choose a store whose removal will have the least impact\n\nMeasurement & Model Improvement\n\ne.g. Including real estate data such as lease terms and market intelligence on competition and anchor stores\n\n\nImpact on Customer Retention\n\nDepends on the convenience and quality of other channels for doing business (such as online or in physical locations), customer loyalty to the company and its products, and market competition.\nIf you have historical data on customer retention after a store closure, you can use catchment and store network features to predict what will happen with customer retention if you decide to close a particular store.\n\nImpact on Customer Acquistion\n\nFor a particular store, take the its catchment area and remove any section that overlaps with another store’s catchment.\n\nThis area will not have any support from a brick-and-mortar perspective to help in your acquisition efforts\n\nLook at trends over time to get a sense of the relative importance of this store’s area to the overall chain and whether you should build a digitally supported strategy to offset the loss of acquiring new customers at a fraction of your cost of operating a retail store at that location"
  },
  {
    "objectID": "qmd/retail.html#sec-retail-lscore",
    "href": "qmd/retail.html#sec-retail-lscore",
    "title": "Retail",
    "section": "Lead Scoring",
    "text": "Lead Scoring\n\nAlso see\n\nAlgorithms, Marketing &gt;&gt; Propensity Model and Uplift Score Model\n\nUse case: identify customer cohorts that are unlikely to become paying customers and eliminate the low efficient outreach\nMap User Journey\n\nAlso see Marketing &gt;&gt; Customer Journey\nExample: B2B\n\n\nQuestions\n\nFind customer’s last touchpoint before conversion\n\ne.g. Proof of concept in B2B\n\nBrainstorm on metrics that may correlate with conversion and form hypotheses\n\ne.g. Metrics that describe user behaviors in proof-of-concept\n\nWhich customer cohorts are likely to schedule sales calls with us and how do they behave without sales guidance\n\ni.e. Which cohorts are more likely to convert if they are given a sales call\nLook at how prospects behave in product trials and interact with marketing emails\n\n\nEDA\n\nPlot metrics by cohort\nCalculate correlation of metrics to conversion\n\nCohorts can be segmented by any grouping factor\n\n\nModel\n\nIf you have enough data, model conversion rate ~ predictors (beta regression?) or convert/no covert ~ predictors (logistic regression, ML, DL)\nOtherwise, weight cohorts by their correlations with conversion"
  },
  {
    "objectID": "qmd/retail.html#sec-retail-catch",
    "href": "qmd/retail.html#sec-retail-catch",
    "title": "Retail",
    "section": "Catchment",
    "text": "Catchment\n\nRefers to the sphere of influence from which a retail location, such as a shopping center, or service, such as a hospital, is likely to draw its customers.\nMajor considerations – supply factors, market demand factors, drive time from customers, transportation access (e.g. interstate) and consumer interactions\nSteps\n\nUse the customers that purchased at the store in the past 12 or 24 months (you need to be the judge on timeframe based on your business model), and map them to where they live, e.g., at census block group level.\nIf the area is massive, applying an optimization function, where you use a polygon and work to minimize the size of the area while keeping the cumulative percent of the total sales as large as possible. (70-80% of cumulative sales is typically optimal)\n\nAssume this optimization function takes into account the considerations mentioned above\n\n\nMeasuring the amount of competition in the given catchment area is useful\nCatchment areas might overlap."
  },
  {
    "objectID": "qmd/retail.html#sec-retail-stnet",
    "href": "qmd/retail.html#sec-retail-stnet",
    "title": "Retail",
    "section": "Store Network",
    "text": "Store Network\n\nUnderstanding how stores are connected beyond a catchment area is essential to make smarter optimization decisions (i.e. closing or opening stores).\nSteps\n\nPick a store and for each customer, note which of your other stores (including ecommerce) that customer has shopped at\n\n(Typically) 3 types of customers:\n\nCustomers who shop at this store exclusively\nCustomers who spend the majority of their $ with you at this store and spend a smaller amount at other stores, including e-commerce\nCustomers who spend a small amount of their $ at this store and spend the most significant amount at other stores, including e-commerce\n\nFor the ones that have shopped at multiple locations, those are generally your more loyal and high-value customers.\n\nThis data forms the basis of the network model\n\nIf the network gets too complicated, then pruning by adding addition rules might be necessary\n\ne.g. Establishing rule for a minimum amount spent to the store network"
  },
  {
    "objectID": "qmd/saas.html#sec-saas-misc",
    "href": "qmd/saas.html#sec-saas-misc",
    "title": "SaaS",
    "section": "Misc",
    "text": "Misc\n\nNotes from\n\nMeaningful metrics: How data sharpened the focus of product teams"
  },
  {
    "objectID": "qmd/saas.html#sec-saas-userseg",
    "href": "qmd/saas.html#sec-saas-userseg",
    "title": "SaaS",
    "section": "User Segmentation",
    "text": "User Segmentation\n\n\nDiagram of users based on their activity on the language learning software, “duolingo.”\n\nSee article for an example of a user journey\n\nActivity States\n\nNew users: learners who are experiencing Duolingo for the first time ever\nCurrent users: learners active today, who were also active in the past week\nReactivated users: learners active today, who were also active in the past month (but not the past week)\nResurrected users: learners active today, who were last active &gt;30 days ago\nAt-risk Weekly Active Users (WAU): learners who have been active within the past week, but not today\nAt-risk Monthly Active Users (MAU): learners who were active within the past month, but not the past week\nDormant Users: learners who have been inactive for at least 30 days\n\nRates\n\nRetention Rates\n\nNew User Retention Rate (NURR): The proportion of day 1 learners who return on day 2. This day 2 transition puts these learners into another “active” state (Current User)\n\nDeactivation Rates: e.g. WAU or MAU loss rate\nActivation Rates: e.g. Reactivation or Resurrection rate\n\nProcess\n\nClassify all users (past or present) into an activity state each day\nCollect data: monitor rates of transition between states.\n\nThese transition probabilities are monitored as retention rates, activation rates, and deactivation rates\n\nUse transition probabilities to create Markov model for simulations\n\nIdentify new metrics, through simulations of the Markov model, that - when optimized - are likely to increase a North Star or Primary metric.\n\nExample\n\n\nSimulation is for 3yrs into the future\n“Lever” is the parameter being increased\n\nIn the sim, it’s increased 2% month-over-month\n\nDaily Active Users (DAU) is the North Star metric that is being opitimized (i.e. increased)\nInterpretation: Increasing Current User Retention Rate (CURR) increases DAU by 75% over the next 3yrs\n\nFormulas at time, t\n\nDAUt = ReactivatedUsert + NewUsert + ResurrectedUsert + CurrentUsert\nWAUt = ReactivatedUsert + NewUsert + ResurrectedUsert + CurrentUsert + AtRiskWAUt\nMAUt = ReactivatedUsert + NewUsert + ResurrectedUsert + CurrentUsert + AtRiskWAUt + AtRiskMAUt\nReactivatedUsert = ReactivationRatet * AtRiskMAUt-1\nResurrectedUsert = ResurrectionRatet * DormantUserst-1\nCurrentUsert = (NewUsert-1 * NURRt) + (ReactivatedUsert-1 * RURRt) + (ResurrectedUsert-1 * SURRt) + (CurrentUsert-1 * CURRt) + (AtRiskWAUt-1 * WAURRt)\nDormantUsert = (DormantUsert-1 * DormantRRt) + (AtRiskMAUt-1 * MAULossRatet)\nAtRiskMAUt = (AtRiskMAUt-1 * ARMAURRt) + (AtRiskWAUt-1 * WAULossRatet)\nAtRiskWAUt = (AtRiskWAUt-1 * ARWAURRt) + (CurrentUsert-1 * (1-CURRt)) + (ReactivatedUsert-1 * (1-RURRt)) + (NewUsert-1 * (1-NURRt)) + (ResurrectedUsert-1 * (1-SURRt))\n\n\nPerform A/B tests to see whether 1) the selected metric can be moved and 2) whether moving it actually moves the north star/primary metric"
  }
]